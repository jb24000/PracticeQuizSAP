window.questionBank = [
    // Domain 1: Design Solutions for Organizational Complexity (26% = ~104 questions)
    {
        id: 'sap_001',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A multinational corporation has 50+ AWS accounts across different business units. Each unit manages their own accounts with varying security standards. The CISO requires centralized security monitoring, compliance enforcement, and cost optimization while allowing business units to maintain operational autonomy.",
        question: "Which combination of services and strategies provides the MOST comprehensive solution?",
        options: [
            "AWS Organizations with SCPs, AWS Control Tower for account provisioning, AWS Security Hub for compliance monitoring, and AWS Cost Explorer with consolidated billing",
            "AWS SSO for access management, individual CloudTrail in each account, AWS Config for compliance, and manual cost allocation tags",
            "AWS Landing Zone, third-party SIEM integration, AWS Trusted Advisor for all accounts, and AWS Budgets per account",
            "Federation with corporate Active Directory, AWS CloudFormation StackSets for governance, Amazon GuardDuty in each account, and AWS Cost and Usage Reports"
        ],
        correct: 0,
        explanation: {
            correct: "This combination provides centralized governance (Organizations + SCPs), standardized account provisioning (Control Tower), unified security monitoring (Security Hub aggregates findings), and consolidated cost management - the most comprehensive solution for enterprise-scale management.",
            whyWrong: {
                1: "Individual CloudTrail and manual tags lack centralization and automation needed at this scale",
                2: "AWS Landing Zone is deprecated in favor of Control Tower, and lacks native compliance monitoring",
                3: "While functional, this approach lacks the unified governance and standardization of Control Tower"
            },
            examStrategy: "For multi-account governance questions, Control Tower + Organizations + Security Hub is the modern AWS best practice. Remember Control Tower replaced Landing Zone."
        }
    },
    {
        id: 'sap_002',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A healthcare organization needs to share patient imaging data between multiple AWS accounts belonging to different hospital networks. The solution must maintain HIPAA compliance, provide audit trails, and allow data providers to revoke access instantly.",
        question: "What is the MOST secure and compliant approach?",
        options: [
            "Use AWS RAM to share encrypted S3 buckets with versioning enabled, AWS CloudTrail for audit logging, and resource-based policies for access control",
            "Implement AWS DataSync between accounts with VPC peering, enable S3 Object Lock for compliance, and use AWS IAM roles for cross-account access",
            "Create a centralized data lake with AWS Lake Formation, implement fine-grained access controls, and use AWS Glue for data cataloging",
            "Deploy AWS Storage Gateway in each account, replicate data using AWS Backup, and implement AWS KMS with customer-managed keys"
        ],
        correct: 2,
        explanation: {
            correct: "Lake Formation provides fine-grained access control at the column and row level, perfect for HIPAA compliance. It offers centralized permissions management with instant revocation capabilities and comprehensive audit trails.",
            whyWrong: {
                0: "RAM is good for sharing but lacks the fine-grained access control needed for HIPAA data",
                1: "DataSync is for migration/transfer, not for controlled sharing with revocation capabilities",
                3: "Storage Gateway doesn't provide the centralized access control and instant revocation required"
            },
            examStrategy: "Lake Formation is the go-to service for secure, compliant data sharing with fine-grained access control across accounts."
        }
    },
    {
        id: 'sap_003',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A global retail company is implementing a disaster recovery strategy across three regions. The RTO is 1 hour and RPO is 15 minutes for critical systems. The company uses a mix of EC2, RDS, DynamoDB Global Tables, and S3. Budget is a significant constraint.",
        question: "Which DR strategy provides the BEST balance of meeting requirements while minimizing costs?",
        options: [
            "Pilot Light: Minimal resources in DR regions, automated AMI copying, RDS automated backups with cross-region read replicas for critical databases, and S3 cross-region replication",
            "Warm Standby: Scaled-down environment running continuously in DR regions, RDS Multi-AZ with read replicas, and Route 53 health checks for failover",
            "Multi-Site Active-Active: Full production capacity in all regions with Route 53 weighted routing, Aurora Global Database, and DynamoDB Global Tables",
            "Backup and Restore: AWS Backup for all resources, S3 lifecycle policies for cost optimization, and AWS Lambda for automated recovery orchestration"
        ],
        correct: 0,
        explanation: {
            correct: "Pilot Light meets the 1-hour RTO (time to scale up minimal resources) and 15-minute RPO (through read replicas and S3 replication) while minimizing costs by not running full infrastructure continuously.",
            whyWrong: {
                1: "Warm Standby exceeds requirements and increases costs with continuously running scaled resources",
                2: "Multi-Site is excessive for these requirements and very expensive with full production capacity everywhere",
                3: "Backup and Restore cannot meet the 1-hour RTO requirement for complex systems"
            },
            examStrategy: "Match DR strategy to specific RTO/RPO requirements. Pilot Light is ideal for RTO of 1-4 hours with tight budget constraints."
        }
    },
    {
        id: 'sap_004',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "An enterprise wants to implement a hub-and-spoke network architecture connecting 30 VPCs across 5 regions. On-premises connectivity is required through multiple data centers. The solution must minimize management overhead and provide transitive routing.",
        question: "What is the MOST scalable and manageable solution?",
        options: [
            "AWS Transit Gateway in each region with Transit Gateway peering, AWS Direct Connect with Transit VIF, and Transit Gateway route tables for routing control",
            "VPC Peering mesh between all VPCs, Virtual Private Gateway in each VPC, and AWS Direct Connect with hosted VIFs",
            "AWS PrivateLink endpoints in each VPC, AWS Site-to-Site VPN for on-premises connectivity, and Route 53 Resolver for DNS",
            "Shared VPC with AWS RAM, VPC Endpoint Services for cross-VPC communication, and AWS Direct Connect Gateway"
        ],
        correct: 0,
        explanation: {
            correct: "Transit Gateway provides true hub-and-spoke with transitive routing, scales to thousands of VPCs, and inter-region peering enables global connectivity. Transit VIF simplifies Direct Connect integration.",
            whyWrong: {
                1: "VPC Peering doesn't support transitive routing and creates an unmanageable mesh at this scale (435 peering connections needed)",
                2: "PrivateLink doesn't provide general network connectivity, only service-specific endpoints",
                3: "Shared VPC doesn't work across regions and VPC Endpoints don't provide general transit routing"
            },
            examStrategy: "Transit Gateway is the answer for any large-scale hub-and-spoke or complex routing scenario. Remember: supports 5000 VPCs per TGW."
        }
    },
    {
        id: 'sap_005',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 120,
        scenario: "A financial services firm needs to implement data residency controls ensuring that sensitive data never leaves specific AWS regions. They also need to prevent developers from accidentally creating resources in non-compliant regions.",
        question: "Which combination of controls provides the STRONGEST enforcement?",
        options: [
            "AWS Organizations SCPs with region restrictions, AWS Config rules for compliance checking, and S3 bucket policies with aws:RequestedRegion condition",
            "IAM permission boundaries with region conditions, AWS CloudFormation with region parameters, and Amazon Macie for data discovery",
            "AWS Control Tower guardrails, VPC Endpoint policies with region restrictions, and AWS KMS key policies",
            "AWS Identity Center permission sets with region limitations, AWS Config conformance packs, and S3 Block Public Access"
        ],
        correct: 0,
        explanation: {
            correct: "SCPs provide preventive controls at the organization level that cannot be overridden, Config rules detect violations, and S3 bucket policies with RequestedRegion ensure data residency at the resource level.",
            whyWrong: {
                1: "Permission boundaries can be bypassed if not consistently applied to all principals",
                2: "Control Tower guardrails are good but less granular than custom SCPs for specific region control",
                3: "Identity Center permissions can be overridden by direct IAM permissions in accounts"
            },
            examStrategy: "SCPs are the strongest preventive control in Organizations - they cannot be overridden even by account root users."
        }
    },
    {
        id: 'sap_006',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A pharmaceutical company with 100+ AWS accounts needs to implement a centralized logging solution for security and compliance. Logs must be tamper-proof, searchable for 7 years, and enable real-time security alerts. The solution must handle 10TB of logs daily.",
        question: "Which architecture provides the MOST secure and scalable logging solution?",
        options: [
            "CloudTrail Organization trail to centralized S3 bucket with MFA delete, S3 Object Lock for immutability, Amazon OpenSearch for search, and EventBridge for real-time alerts",
            "CloudWatch Logs with cross-account subscriptions, Kinesis Data Firehose for aggregation, S3 Glacier for long-term storage, and CloudWatch Alarms for alerting",
            "AWS Systems Manager Session Manager for logging, DynamoDB for log storage with encryption, Lambda for log processing, and SNS for notifications",
            "VPC Flow Logs to S3, AWS Glue for ETL, Redshift for analytics, and QuickSight for visualization"
        ],
        correct: 0,
        explanation: {
            correct: "Organization trail centralizes all CloudTrail logs, Object Lock ensures immutability for compliance, OpenSearch handles 10TB daily with powerful search capabilities, and EventBridge enables real-time response to security events.",
            whyWrong: {
                1: "CloudWatch Logs becomes expensive for 7-year retention at this scale",
                2: "Session Manager only logs interactive sessions, not comprehensive audit trails",
                3: "VPC Flow Logs only capture network traffic, missing application and API audit trails"
            },
            examStrategy: "For centralized logging: CloudTrail Organization trail + S3 Object Lock for compliance + OpenSearch for analysis at scale."
        }
    },
    {
        id: 'sap_007',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A media conglomerate needs to implement cost allocation and chargeback across 75 AWS accounts belonging to different production studios. Each studio has multiple projects with varying budgets and resource requirements.",
        question: "What provides the MOST accurate cost tracking and allocation?",
        options: [
            "AWS Cost Categories with rule-based allocation, cost allocation tags enforced via SCPs, AWS Budgets with alerts, and Cost Explorer with custom reports",
            "Manual tagging strategy, AWS Cost and Usage Reports exported to S3, Athena for analysis, and QuickSight dashboards",
            "AWS Billing Conductor for custom billing, consolidated billing in Organizations, and third-party cost management tools",
            "Resource Groups for organization, Trusted Advisor for optimization, CloudWatch billing alarms, and monthly AWS bill analysis"
        ],
        correct: 0,
        explanation: {
            correct: "Cost Categories enable flexible hierarchical allocation, SCP-enforced tags ensure compliance, Budgets provide proactive control, and Cost Explorer offers detailed analysis - providing comprehensive cost management.",
            whyWrong: {
                1: "Manual tagging is error-prone and lacks enforcement mechanisms",
                2: "Billing Conductor is for AWS Partners/resellers, not internal chargeback",
                3: "This approach lacks granular allocation and automated enforcement"
            },
            examStrategy: "Cost Categories + enforced tagging via SCPs is the enterprise pattern for accurate cost allocation and chargeback."
        }
    },
    {
        id: 'sap_008',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A global bank requires all data to be encrypted with customer-managed keys that automatically rotate every 90 days. Keys must be stored in hardware security modules, and key usage must be audited across 200+ AWS accounts. The solution must support cross-region replication of encrypted data.",
        question: "Which key management architecture provides the MOST secure and scalable solution?",
        options: [
            "AWS KMS with customer-managed CMKs in each region, automatic key rotation enabled, CloudHSM as custom key store, cross-account key policies, and CloudTrail for key usage auditing",
            "AWS CloudHSM clusters in each region with client-side encryption, Parameter Store for key distribution, Lambda functions for rotation, and CloudWatch Events for auditing",
            "AWS Secrets Manager for key storage with automatic rotation, KMS for envelope encryption, AWS Config for compliance checking, and S3 bucket policies for encryption enforcement",
            "Third-party HSM on EC2 instances, Systems Manager for key distribution, AWS Backup for key backups, and Amazon Macie for encryption validation"
        ],
        correct: 0,
        explanation: {
            correct: "KMS with CloudHSM as custom key store provides FIPS 140-2 Level 3 HSM protection, automatic rotation handles the 90-day requirement, multi-region CMKs enable encrypted replication, and CloudTrail provides comprehensive key usage auditing across all accounts.",
            whyWrong: {
                1: "Direct CloudHSM usage requires complex client-side implementation and manual cross-region coordination",
                2: "Secrets Manager is for application secrets, not for data encryption key management at scale",
                3: "Third-party HSM lacks native AWS integration and creates operational overhead"
            },
            examStrategy: "KMS with CloudHSM custom key store is the premium solution for regulated industries requiring HSM-backed keys with AWS integration."
        }
    },
    {
        id: 'sap_009',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "An insurance company needs to implement a data governance framework across multiple AWS accounts. They must track data lineage, ensure PII is properly classified and protected, and maintain compliance with GDPR requirements.",
        question: "Which combination of services provides comprehensive data governance?",
        options: [
            "AWS Glue DataBrew for data profiling, Amazon Macie for PII discovery, AWS Glue Data Catalog for metadata, and Lake Formation for access control",
            "AWS DataSync for data movement tracking, S3 Inventory for data classification, CloudWatch Logs for audit trails, and IAM policies for access control",
            "Amazon Comprehend for PII detection, DynamoDB for metadata storage, Lambda for data processing, and CloudTrail for compliance",
            "AWS Data Pipeline for lineage tracking, S3 Object Tags for classification, Config Rules for compliance, and KMS for encryption"
        ],
        correct: 0,
        explanation: {
            correct: "This combination provides complete data governance: DataBrew profiles data quality, Macie automatically discovers and classifies PII, Glue Catalog maintains metadata and lineage, and Lake Formation enforces fine-grained access controls for GDPR compliance.",
            whyWrong: {
                1: "DataSync is for migration, not lineage tracking, and lacks PII discovery capabilities",
                2: "Comprehend is for text analysis, not comprehensive PII discovery across data types",
                3: "Data Pipeline is legacy and doesn't provide modern data governance capabilities"
            },
            examStrategy: "For data governance: Macie for PII discovery + Glue for cataloging + Lake Formation for access control."
        }
    },
    {
        id: 'sap_010',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A multinational corporation needs to implement a zero-trust network architecture across 15 AWS regions. Remote employees must securely access resources in any region without traditional VPN. The solution must provide microsegmentation and east-west traffic inspection.",
        question: "Which architecture BEST implements zero-trust principles at scale?",
        options: [
            "AWS Verified Access for identity-based access, AWS Network Firewall for traffic inspection, VPC Lattice for service-to-service communication, and CloudWatch for centralized logging",
            "AWS Client VPN with SAML authentication, Transit Gateway with security domains, GuardDuty for threat detection, and VPC Flow Logs for monitoring",
            "AWS SSO with MFA, PrivateLink endpoints for all services, Security Groups with least privilege, and AWS WAF for application protection",
            "AWS AppStream for remote access, Service Mesh with Istio on EKS, Network ACLs for segmentation, and Amazon Detective for investigation"
        ],
        correct: 0,
        explanation: {
            correct: "Verified Access provides true zero-trust without VPN, Network Firewall enables deep packet inspection for east-west traffic, VPC Lattice handles secure service mesh without complexity, and centralized logging enables security monitoring.",
            whyWrong: {
                1: "Client VPN maintains traditional perimeter security, not zero-trust architecture",
                2: "This lacks proper traffic inspection and service-to-service zero-trust controls",
                3: "AppStream is for application streaming, not general resource access"
            },
            examStrategy: "AWS Verified Access is the zero-trust access solution. VPC Lattice is the AWS-native service mesh for service-to-service communication."
        }
    },
    {
        id: 'sap_011',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A government agency requires complete isolation between different classification levels of data (Unclassified, Confidential, Secret). Each level has its own set of AWS accounts. No data can flow from higher to lower classifications.",
        question: "Which network architecture ensures the STRONGEST isolation?",
        options: [
            "Separate AWS Organizations for each classification level, no network connectivity between Organizations, and dedicated Direct Connect virtual interfaces per level",
            "Single Organization with OUs per classification, Transit Gateway with isolated route tables, and Security Groups with deny rules",
            "VPC peering with asymmetric routing, Network ACLs for isolation, and AWS PrivateLink for controlled service access",
            "Shared Services VPC with multiple private subnets, AWS Firewall Manager for centralized rules, and VPC endpoints for service access"
        ],
        correct: 0,
        explanation: {
            correct: "Separate Organizations provide complete administrative and network isolation. No connectivity between them ensures data cannot flow between classification levels, and dedicated Direct Connect VIFs maintain isolation to on-premises.",
            whyWrong: {
                1: "Transit Gateway still allows potential for misconfiguration and data leakage between levels",
                2: "VPC peering creates network paths that could be exploited despite security controls",
                3: "Shared VPC inherently connects different classification levels, violating isolation requirements"
            },
            examStrategy: "For complete isolation requirements (government, military), separate Organizations provide the strongest boundary."
        }
    },
    {
        id: 'sap_012',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A healthcare network with 50 hospitals needs to implement a centralized PACS (Picture Archiving and Communication System) on AWS. Each hospital generates 5TB of medical images daily. Images must be accessible within 1 second for 30 days, within 1 minute for 1 year, and archived for 7 years.",
        question: "Which storage architecture optimizes both performance and cost?",
        options: [
            "S3 Standard for 30 days with CloudFront caching, S3 Intelligent-Tiering with archive tiers configured, and S3 Batch Operations for lifecycle transitions",
            "EFS with provisioned throughput for hot data, S3 Standard-IA for warm data, and Glacier Deep Archive for long-term storage with DataSync for transitions",
            "FSx for Lustre for high-performance access, S3 Standard for 1-year storage, and Glacier Flexible Retrieval with vault lock for compliance",
            "EBS volumes with snapshots to S3, Storage Gateway for hybrid access, and AWS Backup for long-term retention management"
        ],
        correct: 0,
        explanation: {
            correct: "S3 Standard provides millisecond access for 30 days, Intelligent-Tiering automatically moves data through Instant/Frequent/Infrequent/Archive tiers based on access patterns, perfectly matching the requirements while optimizing costs. CloudFront ensures sub-second access globally.",
            whyWrong: {
                1: "EFS is expensive for 5TB daily and DataSync transitions are manual, not policy-based",
                2: "FSx for Lustre is overkill for image viewing and expensive for long-term storage",
                3: "EBS doesn't scale well for multi-hospital access and snapshot management is complex"
            },
            examStrategy: "S3 Intelligent-Tiering is ideal for unknown or changing access patterns. It now includes Archive tiers for complete lifecycle management."
        }
    },
    {
        id: 'sap_013',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A financial institution needs to implement strong workload isolation for different business units while maintaining centralized billing and security oversight. Each unit needs autonomy to manage their own resources and budgets.",
        question: "Which AWS Organizations structure provides the BEST balance of autonomy and control?",
        options: [
            "Nested OUs with delegated administrator accounts per business unit, SCPs for guardrails, and AWS Control Tower Account Factory for standardized provisioning",
            "Flat OU structure with all accounts at root level, IAM roles for cross-account access, and CloudFormation StackSets for governance",
            "Single account with VPC per business unit, Resource Groups for isolation, and IAM policies for access control",
            "Separate AWS Organizations per business unit with consolidated billing agreement and cross-organization trusts"
        ],
        correct: 0,
        explanation: {
            correct: "Nested OUs allow hierarchical policy inheritance, delegated administrators provide autonomy while maintaining central oversight, SCPs enforce security guardrails, and Account Factory ensures consistent account provisioning.",
            whyWrong: {
                1: "Flat structure lacks hierarchical policy management and becomes unwieldy at scale",
                2: "Single account violates workload isolation requirements and creates blast radius risks",
                3: "Separate Organizations lose centralized control and complicate billing/security management"
            },
            examStrategy: "Nested OUs with delegated administration is the pattern for balancing autonomy with centralized governance."
        }
    },
    {
        id: 'sap_014',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A global logistics company needs to track shipments across 100+ countries with real-time updates. The system must handle 1 million concurrent tracking requests, provide 99.99% availability, and maintain data sovereignty per country regulations.",
        question: "Which architecture ensures global scale with data sovereignty?",
        options: [
            "DynamoDB Global Tables with region-per-country deployment, API Gateway with edge-optimized endpoints, Lambda@Edge for routing based on origin country, and Route 53 geolocation routing",
            "Aurora Global Database with write forwarding, CloudFront with multiple origins, ECS Fargate for processing, and AWS Global Accelerator for routing",
            "Single-region RDS with read replicas, S3 with Cross-Region Replication, EC2 Auto Scaling, and CloudFront for caching",
            "Amazon Timestream in each region, Kinesis Data Streams for real-time updates, AppSync for GraphQL API, and Transfer Family for data ingestion"
        ],
        correct: 0,
        explanation: {
            correct: "DynamoDB Global Tables provides multi-region active-active with local reads/writes for sovereignty, API Gateway handles massive concurrent requests, Lambda@Edge routes requests to correct region for compliance, and Route 53 ensures users connect to their local region.",
            whyWrong: {
                1: "Aurora Global Database has single write region, violating data sovereignty requirements",
                2: "Single-region RDS cannot meet data sovereignty requirements for 100+ countries",
                3: "Timestream is for time-series data, not general shipment tracking, and lacks global tables feature"
            },
            examStrategy: "DynamoDB Global Tables is the solution for multi-region active-active with data sovereignty requirements."
        }
    },
    {
        id: 'sap_015',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "An enterprise needs to implement privileged access management for administrative access to 200+ AWS accounts. Access must require approval, be time-limited, and maintain detailed audit logs for compliance.",
        question: "Which solution provides the MOST secure privileged access management?",
        options: [
            "AWS Systems Manager Session Manager with IAM Identity Center, temporary elevated access via AWS Service Catalog, and CloudTrail for audit logging",
            "AWS Secrets Manager for credential rotation, Lambda functions for approval workflow, and CloudWatch Logs for auditing",
            "HashiCorp Vault on EC2 for credential management, Step Functions for approval workflow, and S3 for audit log storage",
            "IAM users with MFA, manual approval process via email, and AWS Config for compliance tracking"
        ],
        correct: 0,
        explanation: {
            correct: "Session Manager provides secure shell access without SSH keys, Identity Center enables temporary elevation with approval workflows, Service Catalog can provision time-limited access, and CloudTrail provides comprehensive audit logging.",
            whyWrong: {
                1: "Secrets Manager doesn't provide session management or approval workflows for interactive access",
                2: "Third-party solutions add complexity and don't integrate natively with AWS services",
                3: "Manual processes don't scale and lack automated time-limiting capabilities"
            },
            examStrategy: "Session Manager + Identity Center is AWS's solution for privileged access management without managing SSH keys or passwords."
        }
    },
    {
        id: 'sap_016',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A multinational bank needs to implement a fraud detection system processing 500,000 transactions per second across 50 countries. The system must detect anomalies in real-time, maintain 5-year historical analysis, and comply with regional data privacy laws.",
        question: "Which architecture provides the required scale and compliance?",
        options: [
            "Amazon Kinesis Data Streams with sharding per region, Kinesis Analytics with ML models, DynamoDB for hot data, S3 with Athena for historical analysis, and Lambda for regional compliance rules",
            "Amazon MSK with cluster per region, SageMaker for fraud detection, ElastiCache for real-time scoring, Redshift for historical data, and Glue for ETL",
            "API Gateway with WAF, Lambda with reserved concurrency, Aurora Serverless for transactions, QuickSight for analytics, and Macie for compliance",
            "EventBridge for event routing, Step Functions for orchestration, RDS PostgreSQL with read replicas, OpenSearch for analysis, and Config for compliance"
        ],
        correct: 0,
        explanation: {
            correct: "Kinesis handles 500K TPS with automatic sharding, Analytics provides real-time ML processing, DynamoDB offers microsecond latency for hot data, S3+Athena cost-effectively handles 5-year analysis, and Lambda enforces regional compliance rules.",
            whyWrong: {
                1: "MSK requires more management overhead and SageMaker adds latency for real-time detection",
                2: "Aurora Serverless cannot handle 500K TPS and Lambda has concurrency limits at this scale",
                3: "EventBridge has throughput limits and RDS cannot scale to 500K TPS"
            },
            examStrategy: "For real-time analytics at massive scale: Kinesis Data Streams + Analytics + DynamoDB for hot path, S3 + Athena for cold path."
        }
    },
    {
        id: 'sap_017',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A research institution needs to share genomic datasets (100TB+) with partner organizations while maintaining strict access controls, tracking usage for billing, and ensuring HIPAA compliance.",
        question: "Which data sharing architecture provides the necessary controls and compliance?",
        options: [
            "AWS Data Exchange for data publishing, Lake Formation for access control, CloudTrail Lake for usage analytics, and AWS Marketplace for billing",
            "S3 with presigned URLs, Cognito for authentication, CloudWatch for usage tracking, and manual invoicing",
            "Transfer Family with SFTP, IAM roles for partners, Cost and Usage Reports for billing, and Macie for compliance",
            "Direct Connect with partners, EFS for shared storage, Resource Access Manager for sharing, and AWS Billing Conductor"
        ],
        correct: 0,
        explanation: {
            correct: "Data Exchange provides secure data sharing with built-in billing, Lake Formation ensures fine-grained HIPAA-compliant access control, CloudTrail Lake enables usage analytics, and Marketplace handles automated billing.",
            whyWrong: {
                1: "Presigned URLs don't provide fine-grained access control needed for HIPAA",
                2: "Transfer Family lacks advanced access controls and usage tracking capabilities",
                3: "Direct Connect is expensive and EFS doesn't provide data governance features"
            },
            examStrategy: "AWS Data Exchange is the service for monetizing and sharing data products with external organizations."
        }
    },
    {
        id: 'sap_018',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "An automotive manufacturer needs to collect telemetry from 10 million vehicles globally, process data for predictive maintenance, and provide real-time alerts to drivers. The system must handle spotty cellular connectivity and minimize data transfer costs.",
        question: "Which IoT architecture optimizes for reliability and cost?",
        options: [
            "AWS IoT Core with MQTT, IoT Analytics for processing, IoT Events for alerting, IoT Device Management for OTA updates, and IoT SiteWise for edge processing",
            "API Gateway with REST APIs, Kinesis for streaming, Lambda for processing, SNS for notifications, and DynamoDB for device state",
            "AppSync with GraphQL subscriptions, EventBridge for routing, Batch for processing, SES for alerts, and S3 for telemetry storage",
            "Direct Connect with VPN backup, EC2 for MQTT brokers, EMR for analytics, CloudWatch for alerting, and RDS for device registry"
        ],
        correct: 0,
        explanation: {
            correct: "IoT Core handles intermittent connectivity with MQTT QoS, IoT Analytics provides managed analytics, IoT Events enables complex event detection, Device Management handles fleet-wide updates, and SiteWise processes data at edge reducing transfer costs.",
            whyWrong: {
                1: "API Gateway doesn't handle intermittent connectivity well and lacks IoT-specific features",
                2: "AppSync isn't optimized for IoT telemetry and lacks device management capabilities",
                3: "Self-managed MQTT brokers don't scale to 10 million devices efficiently"
            },
            examStrategy: "AWS IoT suite is purpose-built for large-scale IoT deployments. IoT SiteWise for edge processing reduces data transfer costs."
        }
    },
    {
        id: 'sap_019',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A media company needs to implement content rights management across multiple streaming platforms. Different content has varying geographic restrictions, time-based availability, and device-specific access rules.",
        question: "Which architecture provides flexible rights management?",
        options: [
            "CloudFront with signed URLs/cookies, Lambda@Edge for access validation, DynamoDB for rights metadata, and AWS Elemental MediaPackage for DRM",
            "S3 with bucket policies, IAM roles for platform access, Config Rules for compliance, and CloudWatch Events for expiration",
            "API Gateway with API keys, Cognito for user pools, RDS for rights database, and WAF for geo-blocking",
            "Application Load Balancer with path-based routing, ECS for validation services, ElastiCache for rules, and Route 53 for geo-routing"
        ],
        correct: 0,
        explanation: {
            correct: "CloudFront signed URLs/cookies provide secure, time-limited access, Lambda@Edge enables complex validation at edge locations, DynamoDB offers fast global rights lookup, and MediaPackage provides industry-standard DRM.",
            whyWrong: {
                1: "S3 bucket policies aren't granular enough for complex rights management",
                2: "API Gateway doesn't handle video streaming efficiently and lacks DRM",
                3: "ALB-based solution lacks edge capabilities and built-in DRM support"
            },
            examStrategy: "CloudFront + Lambda@Edge is the pattern for complex access control at edge. MediaPackage provides DRM for streaming."
        }
    },
    {
        id: 'sap_020',
        domain: "Domain 1: Design Solutions for Organizational Complexity",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A fintech startup building a cryptocurrency exchange needs to process 1 million transactions per second, maintain an immutable audit trail, detect fraud in real-time, and comply with regulations in 30 countries.",
        question: "Which architecture meets these demanding requirements?",
        options: [
            "Amazon QLDB for immutable ledger, Kinesis Data Streams for transaction processing, SageMaker for fraud detection, DynamoDB for order book, and Lambda for compliance rules per country",
            "Managed Blockchain for immutability, API Gateway for transactions, Fraud Detector for anomaly detection, Aurora for order matching, and Config for compliance",
            "DynamoDB Streams with KMS encryption, MSK for event streaming, Rekognition for identity verification, RDS with audit plugins, and Systems Manager for compliance",
            "EventBridge for transaction routing, Step Functions for processing, S3 Object Lock for audit trail, Comprehend for fraud detection, and Control Tower for multi-region compliance"
        ],
        correct: 0,
        explanation: {
            correct: "QLDB provides cryptographically verifiable immutable ledger perfect for audit trails, Kinesis handles million TPS with sharding, SageMaker enables custom real-time fraud models, DynamoDB provides microsecond latency for order book, and Lambda enforces country-specific rules.",
            whyWrong: {
                1: "Managed Blockchain has lower throughput and higher latency than required",
                2: "DynamoDB Streams alone doesn't provide immutability guarantees required for audit",
                3: "EventBridge cannot handle 1 million TPS and Object Lock doesn't provide ledger capabilities"
            },
            examStrategy: "QLDB is the service for immutable ledger requirements. Kinesis is the only service that scales to millions of TPS."
        }
    },

    // Domain 2: Design for New Solutions (29% = ~116 questions)
    {
        id: 'sap_021',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A video streaming platform needs to deliver 4K content globally to millions of users. Content is stored in S3, with new releases causing massive traffic spikes. Users complain about buffering in Asia-Pacific regions. The solution must optimize for both performance and cost.",
        question: "Which architecture provides the BEST global performance while managing costs?",
        options: [
            "CloudFront with multiple origins, S3 Transfer Acceleration for uploads, Lambda@Edge for request routing, and CloudFront origin shield in strategic regions",
            "Multiple S3 buckets with Cross-Region Replication, Application Load Balancers in each region, and Route 53 geolocation routing",
            "AWS Global Accelerator with regional S3 endpoints, EC2 Auto Scaling groups for caching, and ElastiCache Redis clusters",
            "Amazon S3 Multi-Region Access Points, CloudFront with S3 origins, and AWS Elemental MediaConvert for adaptive bitrate streaming"
        ],
        correct: 0,
        explanation: {
            correct: "CloudFront provides global edge caching, Origin Shield reduces origin load and costs during spikes, Lambda@Edge enables intelligent request routing, and Transfer Acceleration optimizes uploads. This combination offers the best performance and cost optimization.",
            whyWrong: {
                1: "ALBs and EC2 for static content delivery is expensive and less performant than CloudFront",
                2: "Global Accelerator is for dynamic content and doesn't provide caching benefits needed here",
                3: "While good, this lacks Origin Shield for spike protection and Lambda@Edge for intelligent routing"
            },
            examStrategy: "For global content delivery, CloudFront + Origin Shield + Lambda@Edge is the premium solution. Origin Shield is key for spike protection."
        }
    },
    {
        id: 'sap_022',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "An IoT company collects telemetry from 10 million devices sending data every 30 seconds. The data needs real-time processing for anomaly detection, storage for 7 years for compliance, and must support ad-hoc analytics. Cost optimization is critical.",
        question: "Which architecture provides the MOST cost-effective solution while meeting all requirements?",
        options: [
            "AWS IoT Core for ingestion, Kinesis Data Streams with Kinesis Analytics for real-time processing, Kinesis Firehose to S3 with Parquet conversion, and Athena for analytics",
            "API Gateway with Lambda for ingestion, DynamoDB Streams for real-time processing, DynamoDB with TTL for hot data, and S3 for cold storage",
            "AWS IoT Core, Amazon Timestream for time-series storage, AWS Lambda for anomaly detection, and QuickSight for analytics",
            "Amazon MQ for ingestion, Amazon MSK for streaming, EMR for processing, S3 with Glacier for storage, and Redshift for analytics"
        ],
        correct: 0,
        explanation: {
            correct: "This serverless architecture minimizes costs: IoT Core handles device connectivity, Kinesis provides reliable streaming, Parquet conversion reduces storage costs by 80%, S3 intelligent tiering optimizes long-term storage, and Athena enables serverless analytics.",
            whyWrong: {
                1: "DynamoDB for 7 years of IoT data would be extremely expensive, even with TTL",
                2: "Timestream is expensive for 7-year retention at this scale",
                3: "MQ, MSK, EMR, and Redshift involve significant fixed costs and management overhead"
            },
            examStrategy: "For IoT at scale: IoT Core → Kinesis → S3 (with format conversion) → Athena is the cost-optimized pattern."
        }
    },
    {
        id: 'sap_023',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A genomics research platform processes TB-scale datasets using complex ML pipelines. Researchers need to run various frameworks (TensorFlow, PyTorch, custom C++ code) with GPU requirements. Jobs can run for days, but are fault-tolerant and can use spot instances. The platform must support 100+ concurrent researchers.",
        question: "Which compute architecture BEST balances performance, cost, and researcher flexibility?",
        options: [
            "AWS Batch with EC2 Spot Fleet, FSx for Lustre for high-performance storage, Step Functions for workflow orchestration, and ECR for container management",
            "Amazon SageMaker with custom algorithms, SageMaker Processing for ETL, S3 for storage, and SageMaker Pipelines for orchestration",
            "EKS with Cluster Autoscaler and Spot instances, EBS GP3 volumes, Argo Workflows for orchestration, and Kubernetes Jobs for processing",
            "AWS ParallelCluster with Slurm scheduler, EFS for shared storage, EC2 Spot Fleet, and AWS Systems Manager for job submission"
        ],
        correct: 0,
        explanation: {
            correct: "AWS Batch is purpose-built for large-scale batch computing, natively supports spot instances with graceful interruption handling, FSx for Lustre provides the HPC-grade performance needed for genomics, and Step Functions handles complex multi-day workflows with built-in error handling.",
            whyWrong: {
                1: "SageMaker is optimized for ML training/inference, not general-purpose HPC workloads with custom C++ code",
                2: "EKS requires significant Kubernetes expertise and doesn't provide native HPC job scheduling features",
                3: "ParallelCluster is good for HPC but requires more management and lacks native AWS service integration"
            },
            examStrategy: "AWS Batch is the go-to for large-scale batch processing with spot instances. FSx for Lustre is the choice for HPC workloads."
        }
    },
    {
        id: 'sap_024',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A social media platform needs to implement a content moderation system that checks images and text for inappropriate content before publishing. The system must handle 50,000 posts per minute during peak times with sub-second response times.",
        question: "Which architecture provides the BEST performance and scalability?",
        options: [
            "API Gateway with Lambda reserved concurrency, Amazon Rekognition for image moderation, Amazon Comprehend for text analysis, and DynamoDB for results caching",
            "Application Load Balancer with ECS Fargate, Amazon Textract for text extraction, custom ML models on SageMaker endpoints, and ElastiCache for caching",
            "CloudFront with Lambda@Edge for initial filtering, Step Functions Express Workflows for orchestration, and Amazon Augmented AI for human review",
            "Kinesis Data Streams for ingestion, Kinesis Analytics with custom ML models, S3 for temporary storage, and SNS for notifications"
        ],
        correct: 0,
        explanation: {
            correct: "This serverless architecture auto-scales to handle traffic spikes, Rekognition and Comprehend provide managed AI services with sub-second latency, Lambda reserved concurrency ensures consistent performance, and DynamoDB caching reduces repeat processing.",
            whyWrong: {
                1: "Textract is for document text extraction, not content moderation, and Fargate has cold start delays",
                2: "Lambda@Edge has size and timeout limitations not suitable for complex moderation tasks",
                3: "Kinesis Analytics isn't designed for synchronous request-response patterns needed here"
            },
            examStrategy: "For real-time AI/ML processing at scale, combine API Gateway + Lambda + managed AI services (Rekognition, Comprehend, etc.)."
        }
    },
    {
        id: 'sap_025',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 120,
        scenario: "A fintech startup is building a payment processing system requiring exactly-once processing, audit trails for 10 years, PCI compliance, and the ability to handle 100,000 transactions per second during Black Friday.",
        question: "Which combination of services ensures reliability and compliance?",
        options: [
            "SQS FIFO queues with deduplication, DynamoDB with point-in-time recovery, X-Ray for tracing, and S3 with Object Lock for audit trails",
            "Amazon MQ with message deduplication, Aurora PostgreSQL with encryption, AWS CloudTrail with S3 lifecycle policies, and AWS WAF for PCI compliance",
            "Kinesis Data Streams with Lambda, RDS Multi-AZ with automated backups, CloudWatch Logs with S3 export, and AWS Shield for DDoS protection",
            "EventBridge with replay capability, Aurora Serverless v2, AWS Audit Manager for compliance tracking, and Glacier for long-term audit storage"
        ],
        correct: 0,
        explanation: {
            correct: "SQS FIFO guarantees exactly-once processing with built-in deduplication, DynamoDB handles 100K TPS easily, X-Ray provides complete transaction tracing, and S3 Object Lock ensures immutable audit trails for compliance.",
            whyWrong: {
                1: "Amazon MQ doesn't scale to 100K TPS as easily as SQS, and Aurora might struggle at peak load",
                2: "Kinesis doesn't guarantee exactly-once processing without additional complexity",
                3: "EventBridge doesn't provide exactly-once guarantees and Aurora Serverless v2 may have scaling delays"
            },
            examStrategy: "For exactly-once processing: SQS FIFO or Kinesis Data Streams with DynamoDB for idempotency. For audit trails: S3 with Object Lock."
        }
    },
    {
        id: 'sap_026',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A gaming company is launching a mobile game expecting 50 million players. The game requires real-time leaderboards, player matching based on skill level, and in-game chat. The solution must handle 1 million concurrent players during launch week.",
        question: "Which architecture BEST supports the real-time gaming requirements at scale?",
        options: [
            "Amazon GameLift for game servers, ElastiCache for Redis sorted sets for leaderboards, DynamoDB with GSI for player matching, and AWS AppSync for real-time chat via GraphQL subscriptions",
            "ECS Fargate for game servers, Aurora PostgreSQL for game state, SQS for matchmaking queue, and IoT Core for WebSocket connections",
            "EC2 Auto Scaling with custom game servers, RDS MySQL with read replicas for leaderboards, Lambda for matchmaking logic, and API Gateway WebSocket APIs for chat",
            "AWS Wavelength for edge computing, DynamoDB Streams for leaderboards, Kinesis Data Streams for matchmaking, and Amazon MQ for messaging"
        ],
        correct: 0,
        explanation: {
            correct: "GameLift is purpose-built for game server management with automatic scaling, Redis sorted sets are perfect for real-time leaderboards, DynamoDB GSI enables fast skill-based queries, and AppSync provides managed real-time subscriptions for chat.",
            whyWrong: {
                1: "Aurora and SQS aren't optimized for real-time gaming latency requirements",
                2: "Custom game servers require significant management overhead at this scale",
                3: "Wavelength is for mobile edge computing, not general game hosting"
            },
            examStrategy: "GameLift is AWS's managed game server solution. For leaderboards, Redis sorted sets are the industry standard pattern."
        }
    },
    {
        id: 'sap_027',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A real estate platform needs to process and analyze millions of property images for feature detection (pools, solar panels, roof condition). Results must be searchable and integrated with property listings in near real-time.",
        question: "Which ML architecture provides the most efficient image analysis pipeline?",
        options: [
            "S3 event triggers Lambda, Rekognition Custom Labels for feature detection, OpenSearch for searchable metadata, and DynamoDB for property listings integration",
            "Kinesis Video Streams for image ingestion, SageMaker endpoints for inference, RDS PostgreSQL for metadata, and ElastiCache for search",
            "ECS Batch for processing, Textract for metadata extraction, Comprehend for description analysis, and CloudSearch for search functionality",
            "Step Functions for orchestration, EC2 with GPU for TensorFlow models, Elasticsearch on EC2, and Aurora for data storage"
        ],
        correct: 0,
        explanation: {
            correct: "S3 events provide serverless triggers, Rekognition Custom Labels enables training specific feature detection models without ML expertise, OpenSearch offers powerful search capabilities, and DynamoDB provides fast lookups for integration.",
            whyWrong: {
                1: "Kinesis Video Streams is for video, not image batches, adding unnecessary complexity",
                2: "Textract is for text extraction from documents, not image feature detection",
                3: "Self-managed TensorFlow on EC2 requires significant ML expertise and operational overhead"
            },
            examStrategy: "Rekognition Custom Labels is ideal for domain-specific image classification without deep ML knowledge."
        }
    },
    {
        id: 'sap_028',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "An autonomous vehicle company needs to process 100TB of sensor data daily from test vehicles. Data includes LIDAR, camera feeds, and telemetry. The platform must support real-time monitoring, ML model training, and compliance with safety regulations requiring 10-year retention.",
        question: "Which data platform architecture handles this scale efficiently?",
        options: [
            "AWS IoT FleetWise for vehicle data collection, S3 data lake with Glue for ETL, SageMaker for model training, Athena for analysis, and S3 Intelligent-Tiering for lifecycle management",
            "Kinesis Data Streams for ingestion, EMR for processing, HDFS for storage, Deep Learning AMIs for training, and Redshift for analytics",
            "Direct Connect for data upload, EFS for shared storage, Batch for processing, EC2 P4d instances for training, and RDS for metadata",
            "Transfer Family for uploads, FSx for Lustre for high-performance storage, ParallelCluster for processing, and Glacier for archival"
        ],
        correct: 0,
        explanation: {
            correct: "IoT FleetWise is purpose-built for vehicle data, S3 provides scalable data lake storage, Glue handles ETL without managing infrastructure, SageMaker offers managed ML training, and Intelligent-Tiering automatically optimizes storage costs over 10 years.",
            whyWrong: {
                1: "HDFS on EMR is expensive for 100TB daily and requires significant management",
                2: "EFS is expensive for 100TB daily storage and not optimized for data lake patterns",
                3: "This approach lacks real-time monitoring capabilities and requires heavy infrastructure management"
            },
            examStrategy: "IoT FleetWise is the specialized service for automotive data collection. S3-based data lakes are most cost-effective for massive scale."
        }
    },
    {
        id: 'sap_029',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A news organization needs to automatically generate subtitles for live broadcasts in 15 languages, create searchable transcripts, and enable content discovery across their video archive of 1 million hours.",
        question: "Which media processing architecture provides comprehensive capabilities?",
        options: [
            "AWS Elemental MediaLive for live streaming, Amazon Transcribe with real-time streaming, Amazon Translate for multi-language, and OpenSearch for transcript search",
            "Kinesis Video Streams for ingestion, Lambda for processing, Comprehend for language detection, and DynamoDB for transcript storage",
            "MediaConnect for transport, SageMaker for custom speech models, S3 for storage, and Athena for search",
            "IVS for live streaming, Textract for subtitle extraction, Polly for text-to-speech, and RDS for metadata"
        ],
        correct: 0,
        explanation: {
            correct: "MediaLive handles professional live streaming, Transcribe provides real-time streaming transcription with high accuracy, Translate handles 15+ languages, and OpenSearch enables powerful full-text search across transcripts.",
            whyWrong: {
                1: "Kinesis Video Streams lacks built-in transcription and Lambda isn't suitable for real-time video processing",
                2: "Building custom speech models is unnecessary when Transcribe provides this functionality",
                3: "IVS is for interactive streaming and Textract is for document text extraction, not speech"
            },
            examStrategy: "AWS Elemental services are for professional media workflows. Transcribe now supports real-time streaming for live captioning."
        }
    },
    {
        id: 'sap_030',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A pharmaceutical company needs a drug discovery platform processing molecular simulations requiring 10,000 vCPUs for burst workloads. Jobs are containerized, may run for weeks, and must maintain GxP compliance with validated environments.",
        question: "Which HPC architecture meets compliance and scale requirements?",
        options: [
            "AWS Batch with validated AMIs, FSx for Lustre for scratch space, Step Functions for workflow orchestration, CloudFormation for infrastructure as code, and CloudTrail for audit",
            "EKS with Karpenter for autoscaling, EBS volumes for storage, Argo Workflows for orchestration, and Open Policy Agent for compliance",
            "ParallelCluster with custom AMIs, EFS for shared storage, Slurm for scheduling, and Systems Manager for patch management",
            "EC2 Spot Fleet with dedicated hosts, instance store for scratch, SWF for workflows, and Config for compliance"
        ],
        correct: 0,
        explanation: {
            correct: "Batch provides managed compute with validation support, FSx for Lustre offers HPC-grade performance, Step Functions enables complex long-running workflows, CloudFormation ensures reproducible validated environments, and CloudTrail provides GxP-required audit trails.",
            whyWrong: {
                1: "EKS requires additional validation effort and Kubernetes adds complexity for HPC workloads",
                2: "ParallelCluster requires more manual validation and management for GxP compliance",
                3: "SWF is deprecated and Spot Fleet alone lacks orchestration for complex workflows"
            },
            examStrategy: "AWS Batch + FSx for Lustre is the go-to for large-scale HPC. For compliance, emphasize validated AMIs and audit trails."
        }
    },
    {
        id: 'sap_031',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A logistics company needs to optimize delivery routes for 10,000 drivers daily, considering real-time traffic, weather, and package priorities. The solution must provide updates every 5 minutes and integrate with mobile apps.",
        question: "Which architecture provides real-time optimization capabilities?",
        options: [
            "Amazon Location Service for routing, Lambda for optimization logic, DynamoDB for driver state, AppSync for real-time updates to mobile apps, and EventBridge for scheduled recalculation",
            "HERE Maps API on EC2, SageMaker for route optimization, RDS for data storage, API Gateway for mobile integration, and CloudWatch Events for scheduling",
            "Route 53 for geo-routing, Batch for calculations, ElastiCache for caching, SNS for push notifications, and SQS for job queuing",
            "Google Maps API via Lambda, EMR for big data processing, S3 for results, CloudFront for API delivery, and Cognito for authentication"
        ],
        correct: 0,
        explanation: {
            correct: "Location Service provides managed routing with traffic data, Lambda enables serverless optimization, DynamoDB handles high-frequency state updates, AppSync delivers real-time updates via GraphQL subscriptions, and EventBridge triggers regular recalculations.",
            whyWrong: {
                1: "Self-managed HERE Maps adds operational overhead and costs",
                2: "Route 53 is for DNS, not route optimization, and Batch isn't suitable for 5-minute updates",
                3: "Using Google Maps API adds external dependencies and EMR is overkill for this use case"
            },
            examStrategy: "Amazon Location Service is AWS's answer to mapping/routing needs. AppSync excels at real-time mobile updates."
        }
    },
    {
        id: 'sap_032',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A stock trading platform requires ultra-low latency (<1ms) for order execution, must handle 5 million orders per second during market open, maintain ACID compliance, and provide real-time risk analysis.",
        question: "Which architecture achieves ultra-low latency at scale?",
        options: [
            "DynamoDB with DAX for microsecond latency, Kinesis Data Streams for order flow, Lambda with provisioned concurrency for risk checks, and TimeStream for time-series analytics",
            "ElastiCache for Redis with cluster mode, ECS on Graviton3 for processing, Aurora PostgreSQL for transactions, and QuickSight for analytics",
            "EC2 with SR-IOV and placement groups, custom in-memory database, Kafka on EC2 for messaging, and Spark for risk analysis",
            "MemoryDB for Redis as primary database, Fargate for compute, EventBridge for event routing, and Managed Streaming for Kafka"
        ],
        correct: 0,
        explanation: {
            correct: "DAX provides consistent microsecond latency, DynamoDB scales to millions of TPS, Kinesis handles high-throughput streaming, provisioned concurrency eliminates Lambda cold starts, and TimeStream is optimized for financial time-series data.",
            whyWrong: {
                1: "ElastiCache isn't a primary database and Aurora adds network latency",
                2: "Custom solutions require extensive development and don't guarantee <1ms latency",
                3: "MemoryDB has higher latency than DAX and EventBridge isn't suitable for millions of TPS"
            },
            examStrategy: "DAX is the only AWS service providing microsecond latency. DynamoDB + Kinesis handles millions of TPS."
        }
    },
    {
        id: 'sap_033',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "An e-learning platform needs to deliver personalized content to 10 million students, track progress in real-time, provide AI tutoring, and generate performance analytics for educators.",
        question: "Which architecture best supports personalized learning at scale?",
        options: [
            "Amazon Personalize for recommendations, DynamoDB for progress tracking, Amazon Lex for AI tutoring, QuickSight with embedded analytics for educators",
            "SageMaker for recommendation models, RDS PostgreSQL for data, Lambda for tutoring logic, and Athena for analytics",
            "ElasticSearch for content search, Aurora for user data, Comprehend for text analysis, and Redshift for analytics",
            "CloudSearch for discovery, DocumentDB for progress, Polly for audio tutoring, and EMR for analytics"
        ],
        correct: 0,
        explanation: {
            correct: "Personalize provides managed recommendation engine perfect for content personalization, DynamoDB handles millions of concurrent progress updates, Lex offers conversational AI for tutoring, and QuickSight embedded analytics integrates seamlessly for educators.",
            whyWrong: {
                1: "Building custom recommendation models requires ML expertise and longer development",
                2: "ElasticSearch alone doesn't provide personalization and Aurora may struggle with write scale",
                3: "CloudSearch is deprecated and DocumentDB isn't optimal for this use case"
            },
            examStrategy: "Amazon Personalize is the managed service for recommendation systems. Lex provides conversational AI capabilities."
        }
    },
    {
        id: 'sap_034',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A smart city initiative needs to process data from 100,000 IoT sensors (traffic, air quality, utilities), provide real-time dashboards for city operations, enable predictive maintenance, and open data APIs for citizens.",
        question: "Which IoT platform architecture scales to city-wide deployment?",
        options: [
            "AWS IoT Core for device management, IoT Analytics for processing, IoT SiteWise for asset modeling, QuickSight for dashboards, and API Gateway for public APIs",
            "Kinesis Data Streams for ingestion, EMR for processing, S3 data lake, Grafana on EC2, and Lambda for APIs",
            "MQTT on EC2, Spark Streaming for processing, Cassandra for storage, Kibana for visualization, and Express.js for APIs",
            "IoT Greengrass for edge processing, MSK for streaming, Redshift for analytics, Tableau for dashboards, and AppSync for APIs"
        ],
        correct: 0,
        explanation: {
            correct: "IoT Core handles massive device fleets, IoT Analytics provides managed analytics, SiteWise models industrial assets for cities, QuickSight offers embeddable dashboards, and API Gateway provides scalable public APIs.",
            whyWrong: {
                1: "Building with Kinesis/EMR requires more management and lacks IoT-specific features",
                2: "Self-managed MQTT doesn't scale to 100,000 devices efficiently",
                3: "Greengrass is for edge but the question focuses on cloud platform"
            },
            examStrategy: "AWS IoT suite provides purpose-built services for IoT scenarios. IoT SiteWise is key for industrial/city asset modeling."
        }
    },
    {
        id: 'sap_035',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "medium",
        timeRecommendation: 150,
        scenario: "A retail chain needs to implement computer vision for self-checkout, detecting unscanned items, age verification for restricted products, and loss prevention. The solution must work across 5,000 stores with varying internet connectivity.",
        question: "Which edge AI architecture provides reliable in-store processing?",
        options: [
            "AWS Panorama for edge computer vision, IoT Greengrass for device management, S3 for model updates, and IoT Analytics for aggregated insights",
            "Rekognition API calls from stores, Lambda for processing, DynamoDB for alerts, and CloudWatch for monitoring",
            "SageMaker Neo for model optimization, EC2 instances in stores, VPN connections, and Kinesis for data streaming",
            "DeepLens cameras, IoT Core for connectivity, Batch for processing, and QuickSight for analytics"
        ],
        correct: 0,
        explanation: {
            correct: "Panorama provides purpose-built edge computer vision appliances perfect for retail, Greengrass manages edge deployments and works offline, S3 enables centralized model updates, and IoT Analytics aggregates insights from all stores.",
            whyWrong: {
                1: "Cloud API calls fail with poor connectivity and add latency for real-time detection",
                2: "Managing EC2 instances in 5,000 stores is operationally complex",
                3: "DeepLens is discontinued and was for development, not production deployment"
            },
            examStrategy: "AWS Panorama is the service for computer vision at the edge. Greengrass enables offline operation for edge scenarios."
        }
    },
    {
        id: 'sap_036',
        domain: "Domain 2: Design for New Solutions",
        difficulty: "hard",
        timeRecommendation: 180,
        scenario: "A cryptocurrency exchange needs to process 1 million transactions per second, maintain an immutable audit trail, detect fraud in real-time, and comply with regulations in 30 countries.",
        question: "Which architecture meets these demanding requirements?",
        options: [
            "Amazon QLDB for immutable ledger, Kinesis Data Streams for transaction processing, SageMaker for fraud detection, DynamoDB for order book, and Lambda for compliance rules per country",
            "Managed Blockchain for immutability, API Gateway for transactions, Fraud Detector for anomaly detection, Aurora for order matching, and Config for compliance",
            "DynamoDB Streams with KMS encryption, MSK for event streaming, Rekognition for identity verification, RDS with audit plugins, and Systems Manager for compliance",
            "EventBridge for transaction routing, Step Functions for processing, S3 Object Lock for audit trail, Comprehend for fraud detection, and Control Tower for multi-region compliance"
        ],
        correct: 0,
        explanation: {
            correct: "QLDB provides cryptographically verifiable immutable ledger perfect for audit trails, Kinesis handles million TPS with sharding, SageMaker enables custom real-time fraud models, DynamoDB provides microsecond latency for order book, and Lambda enforces country-specific rules.",
            whyWrong: {
                1: "Managed Blockchain has lower throughput and higher latency than required",
                2: "DynamoDB Streams alone doesn't provide immutable audit trail guarantees. Rekognition is for image/video analysis, not transaction fraud detection.",
                3: "S3 Object Lock provides immutability but not the performance for real-time audit trails. Comprehend is for NLP, not fraud detection. Step Functions adds too much latency for million TPS."
   },
		 examStrategy: "For cryptocurrency/fintech at scale: QLDB provides immutable audit trails with cryptographic verification. Kinesis handles millions of TPS. DynamoDB offers microsecond latency for order books. SageMaker enables real-time fraud detection."	
 }
	},		
{
  id: 'sap_037',
  domain: "Domain 1: Design Solutions for Organizational Complexity",
  difficulty: "hard",
  timeRecommendation: 180,
  scenario: "A multinational corporation with 50 AWS accounts across 5 business units needs to implement a zero-trust security model. They require centralized identity management, network isolation between business units, encrypted data sharing capabilities, and compliance with SOC 2, HIPAA, and GDPR across different regions.",
  question: "Which architecture provides the most comprehensive zero-trust implementation while maintaining operational efficiency?",
  options: [
    "AWS SSO with external IdP integration, Transit Gateway with route table segregation, AWS RAM for cross-account sharing, Macie for data classification, and Security Hub with custom compliance frameworks per region",
    "Cognito for centralized authentication, PrivateLink endpoints in each account, S3 bucket policies for data sharing, GuardDuty for threat detection, and Config conformance packs",
    "IAM Identity Center with permission sets, AWS Network Firewall with domain filtering, Lake Formation for data governance, Detective for investigation, and Audit Manager for compliance",
    "Directory Service with MFA, VPC peering with security groups, DataSync for data transfer, CloudTrail Lake for analysis, and Control Tower with guardrails"
  ],
  correct: 0,
  explanation: {
    correct: "AWS SSO (Identity Center) with external IdP provides true centralized identity with SAML/OIDC. Transit Gateway with route segregation enables network isolation and inspection. RAM enables secure resource sharing with encryption. Macie classifies sensitive data for compliance. Security Hub aggregates compliance across accounts with regional requirements.",
    whyWrong: {
      1: "Cognito is for application users, not enterprise workforce. PrivateLink alone doesn't provide network segmentation between business units.",
      2: "Network Firewall is good but doesn't address the identity federation requirement. Lake Formation is for data lakes specifically, not general data sharing.",
      3: "Directory Service doesn't provide modern zero-trust capabilities. VPC peering creates a mesh that's hard to manage at scale and doesn't provide inspection."
    },
	  examStrategy: "For zero-trust architecture: AWS SSO (Identity Center) for identity, Transit Gateway for network segmentation, RAM for secure sharing, Security Hub for compliance aggregation."
    }
  },
   {
    id: 'sap_038',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A gaming company is launching a real-time multiplayer game expecting 10 million concurrent players. The game requires <50ms latency globally, player session state management, real-time leaderboards, and the ability to handle 100,000 new connections per second during launch events.",
    question: "What architecture best supports these real-time gaming requirements?",
    options: [
      "GameLift FlexMatch for matchmaking, DynamoDB global tables for session state, ElastiCache for leaderboards, API Gateway WebSocket APIs for connections",
      "Global Accelerator for anycast routing, AppSync with subscriptions for real-time updates, DAX for session caching, Kinesis Data Analytics for leaderboards",
      "CloudFront with Lambda@Edge for routing, MemoryDB for session state, Timestream for leaderboards, IoT Core for WebSocket connections",
      "Route 53 geoproximity routing, ECS with Service Discovery, Redis cluster for all caching needs, ALB with sticky sessions"
    ],
    correct: 0,
    explanation: {
      correct: "GameLift FlexMatch is purpose-built for game matchmaking with low latency. DynamoDB global tables provide multi-region session state with single-digit millisecond performance. ElastiCache offers sorted sets perfect for real-time leaderboards. API Gateway WebSocket handles massive concurrent connections with automatic scaling.",
      whyWrong: {
        1: "AppSync is for GraphQL and not optimized for gaming latency. Kinesis Analytics adds unnecessary complexity for simple leaderboards.",
        2: "IoT Core is not designed for gaming connections. Timestream is for time-series data, overkill for leaderboards.",
        3: "ALB doesn't support WebSocket at the scale needed. ECS adds operational overhead compared to managed services."
      },
		examStrategy: "For real-time gaming at scale: GameLift FlexMatch for matchmaking, DynamoDB global tables for session state, ElastiCache for leaderboards. API Gateway WebSocket handles massive concurrent connections."
	}
	},

  {
    id: 'sap_039',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "An e-commerce platform running on EC2 with RDS MySQL is experiencing 40% increased costs and performance degradation. Current architecture: 50 m5.xlarge instances, Multi-AZ RDS db.r5.4xlarge, CloudFront, and S3. Monthly bill is $45,000 with 60% idle capacity during off-peak hours.",
    question: "Which optimization strategy provides the best cost-performance improvement?",
    options: [
      "Migrate to ECS Fargate with Spot capacity, Aurora Serverless v2, implement S3 Intelligent-Tiering, use Lambda@Edge for dynamic content, and Compute Savings Plans",
      "Switch to EC2 Spot Fleet with mixed instances, Aurora MySQL with read replicas, enable RDS Proxy, implement CloudFront caching policies, and Reserved Instances",
      "Containerize on EKS with Karpenter autoscaling, Aurora Global Database, use ElastiCache for session management, CloudFront Origin Shield, and Savings Plans",
      "Implement Auto Scaling with predictive scaling, convert to Aurora PostgreSQL, add DAX caching layer, optimize CloudFront behaviors, and Spot Instances for batch processing"
    ],
    correct: 0,
    explanation: {
      correct: "Fargate eliminates idle EC2 capacity with per-second billing. Aurora Serverless v2 auto-scales database capacity reducing costs by 50%+. S3 Intelligent-Tiering optimizes storage costs. Lambda@Edge reduces backend load. Compute Savings Plans provide 66% discount on Fargate.",
      whyWrong: {
        1: "Spot Fleet risks availability issues for production web servers. RDS Proxy adds cost without solving the core scaling problem.",
        2: "EKS with Karpenter is complex and adds management overhead. Aurora Global is expensive overkill for a single-region e-commerce site.",
        3: "Predictive scaling helps but doesn't eliminate idle capacity like serverless. DAX is unnecessary cost if not using DynamoDB."
      },
		examStrategy: "For e-commerce optimization: Fargate eliminates idle capacity, Aurora Serverless v2 auto-scales, S3 Intelligent-Tiering optimizes storage. Lambda@Edge reduces backend load."
	}
	
	},

  {
    id: 'sap_040',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A financial services company needs to migrate 500TB of on-premises Oracle databases to AWS within 3 months. They require minimal downtime (<1 hour), must maintain ACID compliance, need to preserve stored procedures, and want to reduce Oracle licensing costs by 70%.",
    question: "What migration strategy best meets these requirements?",
    options: [
      "Use DMS with CDC for continuous replication to Aurora PostgreSQL, SCT for schema conversion, Lambda for stored procedure migration, and perform cutover during maintenance window",
      "AWS DataSync for initial load to RDS Oracle, then Golden Gate for replication, gradually migrate stored procedures, and switch to Aurora PostgreSQL post-migration",
      "Database Migration Accelerator to RDS Custom for Oracle, maintain procedures as-is, then re-platform to Babelfish for Aurora PostgreSQL to reduce licenses",
      "Snowball Edge for initial transfer, DMS for ongoing sync to RDS Oracle, manually convert procedures, then use Blue/Green deployment to Aurora"
    ],
    correct: 2,
    explanation: {
      correct: "RDS Custom for Oracle allows immediate migration with full Oracle compatibility. Babelfish for Aurora PostgreSQL enables running Oracle workloads on PostgreSQL, eliminating 70% of Oracle licenses while maintaining stored procedures and ACID compliance with minimal code changes.",
      whyWrong: {
        0: "SCT and Lambda conversion of stored procedures would take longer than 3 months for 500TB of databases with complex procedures.",
        1: "DataSync is for file storage, not databases. Golden Gate adds complexity and licensing costs.",
        3: "Snowball Edge adds unnecessary time. Blue/Green deployment doesn't address the stored procedure compatibility issue."
      },
		examStrategy: "For Oracle migrations with minimal downtime: RDS Custom maintains Oracle compatibility, Babelfish enables PostgreSQL to run Oracle workloads. DMS with CDC for continuous replication."
	}
	
	},

  {
    id: 'sap_041',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A biotech company needs to process genomic sequencing data: 100TB daily input, HIPAA compliant, 99.999% durability, support for 10,000 concurrent analysis jobs, real-time collaboration between 5 global research centers, and 7-year retention with instant retrieval for the first year.",
    question: "Which architecture best handles these genomic data processing requirements?",
    options: [
      "S3 with Object Lock for compliance, Batch for job orchestration, FSx for Lustre for high-performance computing, Transfer Family for data ingestion, and Intelligent-Tiering for lifecycle management",
      "S3 Glacier Instant Retrieval after 1 year, EMR for processing, EFS for shared storage, DataSync for transfer, and Macie for compliance scanning",
      "S3 Standard with versioning, ParallelCluster for HPC, EBS Multi-Attach for shared data, Direct Connect for transfers, and Backup for long-term retention",
      "S3 Outposts for edge processing, Glue for ETL, FSx for Windows for collaboration, Storage Gateway for ingestion, and Glacier Deep Archive for retention"
    ],
    correct: 0,
    explanation: {
      correct: "S3 Object Lock ensures HIPAA compliance with immutability. Batch efficiently orchestrates 10,000 concurrent jobs. FSx for Lustre provides the massive parallel processing needed for genomics. Transfer Family handles secure multi-site uploads. Intelligent-Tiering automatically moves data to Glacier after 1 year while maintaining instant retrieval for recent data.",
      whyWrong: {
        1: "EMR is for big data analytics, not optimized for genomic HPC workloads. EFS lacks the performance for massive parallel genomic processing.",
        2: "EBS Multi-Attach is limited to 16 instances, insufficient for 10,000 jobs. ParallelCluster adds management overhead.",
        3: "Outposts unnecessary for this use case. FSx for Windows not optimized for genomic workloads. Glacier Deep Archive has 12-hour retrieval time."
	  },
		examStrategy: "For genomic/biotech workloads: S3 Object Lock for HIPAA compliance, Batch for job orchestration at scale, FSx for Lustre for HPC performance."
      }
	 },
	 
	// Continuing the question bank...
{
    id: 'sap_042',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global media company is building a new video streaming platform expecting 100 million users. The platform needs to deliver live sports events with <3 second latency, support 4K/8K video, handle 10 million concurrent viewers during major events, and provide personalized content recommendations. The architecture must support multiple regions with local content regulations.",
    question: "Which architecture provides the MOST scalable and performant solution?",
    options: [
        "Amazon CloudFront with AWS Elemental MediaStore as origin, AWS Elemental MediaLive for encoding, Amazon DynamoDB Global Tables for user profiles, Amazon Personalize for recommendations, and Amazon Kinesis Data Analytics for real-time analytics",
        "Amazon CloudFront with multiple origins, AWS Elemental MediaPackage for just-in-time packaging, Amazon ElastiCache for session data, Amazon SageMaker for ML recommendations, and Amazon Redshift for analytics",
        "AWS Global Accelerator with Amazon EC2 instances running Wowza, Amazon Aurora Global Database for user data, AWS Lambda@Edge for personalization, and Amazon EMR for analytics processing",
        "Amazon CloudFront with AWS Elemental MediaConnect for transport, Amazon ECS with custom streaming servers, Amazon Neptune for recommendation graph, and Amazon Timestream for time-series analytics"
    ],
    correct: 0,
    explanation: {
        correct: "This combination provides ultra-low latency streaming (MediaStore + CloudFront), professional live encoding (MediaLive), globally distributed user data (DynamoDB Global Tables), managed ML recommendations (Personalize), and real-time analytics (Kinesis Data Analytics) - optimal for massive scale live streaming.",
        whyWrong: {
            1: "MediaPackage adds packaging latency not suitable for <3 second requirement, and Redshift isn't ideal for real-time analytics",
            2: "EC2-based streaming lacks the managed scaling and low latency of AWS Elemental services, and Lambda@Edge has limitations for complex personalization",
            3: "MediaConnect is for contribution not distribution, and ECS-based custom servers won't match managed service performance at this scale"
        },
        examStrategy: "For ultra-low latency live streaming at scale, AWS Elemental MediaStore + MediaLive + CloudFront is the go-to combination. Remember MediaStore is optimized for live video origination with consistent low latency."
    }
},
{
    id: 'sap_043',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial institution runs a monolithic Java application on 20 m5.24xlarge EC2 instances with Oracle Database on RDS. The application handles loan processing with strict SLA requirements (99.99% availability). Monthly AWS costs exceed $500,000. Performance analysis shows CPU utilization averages 30%, memory at 40%, but certain batch jobs require full resources for 4 hours nightly. The database shows hot partitions during month-end processing.",
    question: "Which optimization strategy provides the BEST cost reduction while maintaining SLAs?",
    options: [
        "Implement EC2 Auto Scaling with scheduled actions for batch processing, migrate to Aurora PostgreSQL with read replicas, use EC2 Instance Savings Plans, and implement application caching with ElastiCache",
        "Containerize with ECS on EC2 Spot Fleet with Spot Fleet management, implement database sharding on current RDS Oracle, use Reserved Instances, and add CloudFront for static content",
        "Migrate to EKS with Karpenter for intelligent scaling, implement Aurora Serverless v2 for variable workloads, use Compute Savings Plans, and implement database connection pooling with RDS Proxy",
        "Decompose into microservices on Lambda for transactional workloads, keep EC2 for batch jobs with Spot instances, migrate to DynamoDB with proper partition key design, and implement API caching with API Gateway"
    ],
    correct: 2,
    explanation: {
        correct: "EKS with Karpenter provides intelligent right-sizing and scaling, Aurora Serverless v2 handles variable database loads cost-effectively, Compute Savings Plans offer flexibility across services, and RDS Proxy reduces connection overhead - maintaining high availability while optimizing costs.",
        whyWrong: {
            0: "Auto Scaling helps but doesn't address oversized instances, and PostgreSQL migration might break Oracle-specific features required for financial applications",
            1: "Spot Fleet for critical financial workloads risks SLA violations, and Oracle sharding is complex without addressing the core cost issue",
            3: "Full Lambda migration is too aggressive for monolithic app, DynamoDB may not suit complex financial queries, and mixing architectures increases complexity"
        },
        examStrategy: "For monolithic optimization: containerization (not full microservices) provides better resource utilization. Aurora Serverless v2 excels at variable workloads. Karpenter is the modern solution for Kubernetes autoscaling."
    }
},
{
    id: 'sap_044',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A healthcare company needs to migrate 500TB of medical imaging data from on-premises NAS to AWS. The data must remain accessible during migration with <100ms latency for recent data (last 30 days, ~50TB). Compliance requires encryption in transit and at rest, with audit logging. Network bandwidth is 1Gbps dedicated for migration. Post-migration, the solution must support AI/ML workloads for image analysis.",
    question: "Which migration strategy ensures continuous access while optimizing for future ML workloads?",
    options: [
        "AWS DataSync for initial transfer, AWS Storage Gateway File Gateway for hybrid access during migration, Amazon S3 Intelligent-Tiering for storage, and AWS HealthLake for FHIR compliance with Amazon SageMaker for ML",
        "AWS Direct Connect with AWS Storage Gateway Volume Gateway in cached mode, ongoing replication to Amazon S3, post-migration transition to Amazon FSx for Lustre with S3 integration for ML processing",
        "AWS Snowball Edge for bulk transfer, AWS Storage Gateway File Gateway for recent data access, migration to Amazon EFS with lifecycle management, and Amazon Rekognition for image analysis",
        "AWS Database Migration Service with Large Object support, Amazon S3 with Transfer Acceleration, AWS Lambda for access patterns monitoring, and Amazon Textract with custom ML models"
    ],
    correct: 1,
    explanation: {
        correct: "Direct Connect ensures reliable transfer, Volume Gateway cached mode provides low-latency access to hot data while migrating, S3 serves as durable storage, and FSx for Lustre offers high-performance computing ideal for ML workloads with native S3 integration.",
        whyWrong: {
            0: "DataSync alone can't maintain <100ms latency during migration, and HealthLake is for FHIR data not imaging files",
            2: "Snowball Edge creates accessibility gaps during shipping, and EFS isn't optimized for ML workloads compared to FSx for Lustre",
            3: "DMS isn't designed for file migration, and Textract is for document processing not medical imaging"
        },
        examStrategy: "For large-scale migrations with continuous access requirements, Storage Gateway in cached mode is key. For ML/HPC workloads on file data, FSx for Lustre with S3 integration is the optimal choice."
    }
},
{
    id: 'sap_045',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global retail company with 200 stores operates separate AWS accounts per region (15 accounts total). Each region has different compliance requirements, payment providers, and inventory systems. The company needs to implement a unified data lake for global analytics, centralized security monitoring, and cross-region inventory visibility while respecting data residency laws that prohibit certain data from leaving specific countries.",
    question: "Which architecture BEST addresses these multi-region compliance and integration requirements?",
    options: [
        "AWS Lake Formation with cross-account permissions, AWS Glue DataBrew for PII detection and masking, Amazon Macie for data classification, federated queries with Amazon Athena, and AWS DataSync with data residency rules",
        "Amazon DataZone for data mesh architecture, AWS Clean Rooms for privacy-preserving analytics, Amazon S3 with Object Lock for compliance, AWS Glue ETL with data lineage, and Amazon Redshift data sharing",
        "Centralized S3 bucket with bucket policies per region, AWS Glue crawlers for cataloging, Amazon EMR for processing, AWS Lambda for data transformation, and Amazon QuickSight with row-level security",
        "Hub-and-spoke architecture with AWS PrivateLink, Amazon MSK for event streaming, Amazon Kinesis Data Firehose for ingestion, Amazon OpenSearch for analytics, and AWS Config for compliance tracking"
    ],
    correct: 1,
    explanation: {
        correct: "DataZone enables federated data mesh architecture respecting boundaries, Clean Rooms allows analytics without moving sensitive data, S3 Object Lock ensures compliance, Glue provides lineage for audit, and Redshift data sharing enables cross-region analytics without violating residency laws.",
        whyWrong: {
            0: "Lake Formation doesn't inherently solve data residency issues, and DataSync would violate data movement restrictions",
            2: "Centralized bucket architecture directly violates data residency requirements for certain countries",
            3: "MSK and streaming architecture doesn't address the core requirement of maintaining data residency while enabling analytics"
        },
        examStrategy: "For data residency compliance with analytics needs, look for solutions that process data in-place (Clean Rooms, data sharing) rather than centralizing data movement. DataZone is AWS's answer to data mesh architecture."
    }
},
{
    id: 'sap_046',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous vehicle company is designing a platform to process sensor data from 50,000 vehicles. Each vehicle generates 4TB of data daily including video, LIDAR, and telemetry. The platform must support real-time safety alerts (<500ms), batch processing for model training, handle 100Gbps peak ingestion, provide 11 9's durability for crash investigation data, and support replay of any time window for simulation.",
    question: "Which architecture meets these extreme scale and performance requirements?",
    options: [
        "AWS IoT Core for device management, Amazon Kinesis Data Streams with 10,000 shards for ingestion, Amazon S3 Express One Zone for hot data, S3 Glacier Instant Retrieval for cold data, Amazon EMR for batch processing, and Amazon Managed Service for Apache Flink for real-time processing",
        "AWS IoT FleetWise for vehicle data collection, Amazon Managed Streaming for Apache Kafka (MSK) with tiered storage, Amazon S3 with Intelligent-Tiering, AWS Batch for processing, Amazon SageMaker for model training, and Amazon Timestream for time-series queries",
        "AWS Direct Connect with multiple VIFs, Amazon Kinesis Data Firehose with record format conversion, Amazon S3 with cross-region replication, AWS Lambda for real-time alerts, Amazon Redshift for analytics, and AWS Glue for ETL",
        "AWS IoT Greengrass for edge processing, Amazon MSK Serverless for streaming, Amazon EFS for temporary storage, Amazon S3 Standard-IA for archival, Amazon EKS with GPU nodes for processing, and Amazon Neptune for relationship analysis"
    ],
    correct: 1,
    explanation: {
        correct: "IoT FleetWise is purpose-built for vehicle data, MSK with tiered storage handles massive throughput with replay capability, S3 Intelligent-Tiering optimizes storage costs, Batch handles large-scale processing, SageMaker supports model training, and Timestream enables efficient time-based queries for any window.",
        whyWrong: {
            0: "S3 Express One Zone doesn't provide 11 9's durability (only 99.999999999% requires multiple zones), and 10,000 Kinesis shards would be extremely expensive",
            2: "Kinesis Data Firehose has latency (minimum 60 seconds) incompatible with <500ms alerts, and Lambda has payload limitations for video processing",
            3: "EFS isn't suitable for 100Gbps ingestion rates, and MSK Serverless has throughput limitations for this scale"
        },
        examStrategy: "For automotive/IoT scenarios, IoT FleetWise is the specialized service. MSK with tiered storage provides Kafka compatibility with replay capabilities. For 11 9's durability, ensure multi-AZ or multi-region storage."
    }
},
{
    id: 'sap_047',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "An e-commerce platform on AWS experiences 70% lower traffic during nights and weekends. The current architecture uses 40 c5.2xlarge EC2 instances in an Auto Scaling group, RDS MySQL Multi-AZ (db.r5.8xlarge), and ElastiCache Redis cluster with 6 cache.m5.xlarge nodes. Monthly costs are $125,000. The platform maintains session state in ElastiCache and serves both web and API traffic.",
    question: "Which optimization approach would provide the MOST cost savings while maintaining performance?",
    options: [
        "Convert EC2 instances to Spot Fleet with mixed instance types, implement Aurora Serverless v1, reduce ElastiCache nodes with auto-discovery, and separate web/API into different target groups",
        "Implement predictive scaling for EC2, migrate to Aurora MySQL with auto-scaling replicas, use ElastiCache for Redis with auto-scaling, and implement Savings Plans for compute",
        "Containerize on Fargate Spot for stateless workloads, keep RDS but downsize with scheduled scaling, implement DynamoDB for session state, and use CloudFront to reduce backend load",
        "Move to EC2 t3.xlarge with unlimited mode, implement Aurora Global Database, replace ElastiCache with DynamoDB Accelerator (DAX), and implement request throttling"
    ],
    correct: 1,
    explanation: {
        correct: "Predictive scaling anticipates traffic patterns for optimal cost, Aurora with auto-scaling replicas provides better price-performance than RDS, ElastiCache auto-scaling adjusts to demand, and Savings Plans provide 72% discounts while maintaining flexibility for the predictable baseline load.",
        whyWrong: {
            0: "Spot Fleet risks availability issues for customer-facing e-commerce, and Aurora Serverless v1 has cold start issues",
            2: "Fargate Spot isn't suitable for customer-facing workloads, and migrating session state to DynamoDB requires application changes",
            3: "T3 instances may not handle peak loads efficiently, Aurora Global is unnecessary overhead for single-region, and DAX is for DynamoDB not a Redis replacement"
        },
        examStrategy: "For predictable traffic patterns, predictive scaling + Savings Plans is optimal. Aurora MySQL with replicas offers better scaling than RDS Multi-AZ. Maintain architectural stability while optimizing costs."
    }
},
{
    id: 'sap_048',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A government agency must migrate a 15-year-old mainframe COBOL system processing citizen benefits to AWS. The system handles 5 million transactions daily, integrates with 50 other systems via MQ and file transfers, has 400TB of DB2 data, and must maintain bit-for-bit compatibility for regulatory audit. Zero downtime is required, and the migration must complete within 6 months.",
    question: "Which migration strategy ensures compatibility and zero downtime?",
    options: [
        "AWS Mainframe Modernization Service with Micro Focus for COBOL runtime, AWS SCT for DB2 to Aurora PostgreSQL migration, Amazon MQ for message queue compatibility, AWS DataSync for file transfer migration, and parallel run validation",
        "AWS Mainframe Modernization Service with automated refactoring to Java, DMS with CDC for DB2 to RDS migration, Amazon SQS to replace MQ, AWS Transfer Family for file protocols, and blue-green deployment",
        "Containerize COBOL applications using OpenShift on EKS, migrate DB2 to Amazon RDS for Db2, implement Amazon MSK for messaging, AWS Direct Connect for hybrid connectivity, and phased cutover approach",
        "EC2 instances with COBOL compilers, lift-and-shift DB2 to EC2, Amazon MQ for JMS compatibility, AWS Storage Gateway for file systems, and database replication with GoldenGate"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Mainframe Modernization with Micro Focus maintains COBOL compatibility ensuring bit-for-bit accuracy, SCT handles complex DB2 migration, Amazon MQ provides native MQ protocol support, DataSync manages file migrations, and parallel run enables zero-downtime validation.",
        whyWrong: {
            1: "Automated refactoring to Java cannot guarantee bit-for-bit compatibility required for regulatory audit",
            2: "OpenShift on EKS adds unnecessary complexity, and RDS for Db2 has limitations for mainframe DB2 features",
            3: "Manual EC2 approach lacks mainframe-specific features and GoldenGate licensing adds complexity and cost"
        },
        examStrategy: "For mainframe migrations requiring compatibility, AWS Mainframe Modernization Service is purpose-built. Micro Focus runtime maintains COBOL execution compatibility. Amazon MQ preserves message protocol compatibility."
    }
},
{
    id: 'sap_049',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational bank operates in 30 countries with strict data sovereignty laws. Each country has its own AWS account with local applications and databases. The bank needs to implement global fraud detection that analyzes patterns across all regions in real-time, while ensuring no personally identifiable information (PII) leaves its country of origin. The solution must also provide unified reporting for regulators.",
    question: "Which architecture BEST balances global fraud detection with data sovereignty requirements?",
    options: [
        "Implement federated learning with Amazon SageMaker where models are trained locally and only model parameters are shared globally, use AWS Clean Rooms for cross-border analytics without data movement, and Amazon DataZone for governance",
        "Deploy Amazon Fraud Detector in each region with shared model artifacts, use AWS PrivateLink for secure connectivity, aggregate anonymized metrics in a central account, and implement AWS CloudFormation StackSets for consistent deployment",
        "Use Amazon Macie to identify and tokenize PII, replicate tokenized data to central region with DMS, implement Amazon Neptune for fraud graph analysis, and use AWS Lake Formation for access control",
        "Implement event sourcing with Amazon EventBridge global endpoints, process with Lambda@Edge for data localization, analyze with Amazon OpenSearch cross-cluster replication, and report with Amazon QuickSight SPICE"
    ],
    correct: 0,
    explanation: {
        correct: "Federated learning with SageMaker allows model training on local data without moving it, sharing only parameters preserves privacy, Clean Rooms enables privacy-preserving analytics across borders without raw data movement, and DataZone provides unified governance while respecting boundaries.",
        whyWrong: {
            1: "Sharing model artifacts might leak information about local data patterns, and anonymized metrics may not be sufficient for sophisticated fraud detection",
            2: "Tokenization and central replication still involves moving derived PII data, which may violate strict sovereignty laws",
            3: "EventBridge and cross-cluster replication would require data movement across borders, violating sovereignty requirements"
        },
        examStrategy: "For data sovereignty with global analytics, federated learning and Clean Rooms are key AWS services. They enable insights without data movement. DataZone provides governance for distributed data architectures."
    }
},
{
    id: 'sap_050',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A genomics research company is building a platform to analyze DNA sequences. Each analysis job processes 100GB files, requires 128 vCPUs and 1TB RAM for 4-6 hours, runs complex bioinformatics pipelines with 50+ steps, needs access to a 10TB reference genome database, and must support 1000 concurrent analyses. Results must be available for 7 years with sub-second query response for the first year.",
    question: "Which architecture provides the MOST cost-effective and scalable solution?",
    options: [
        "AWS Batch with EC2 Spot instances, Amazon FSx for Lustre for scratch space, Amazon S3 for input/output, AWS Step Functions for pipeline orchestration, Amazon RDS for metadata, and S3 Intelligent-Tiering for long-term storage",
        "Amazon ECS with Fargate for compute, Amazon EFS for shared storage, AWS Glue for ETL pipelines, Amazon DynamoDB for job tracking, Amazon Athena for queries, and S3 Glacier Deep Archive for archival",
        "Amazon EMR with Spot instances for processing, HDFS for temporary storage, AWS Data Pipeline for orchestration, Amazon Redshift for analytics, Amazon ElastiCache for reference data, and S3 lifecycle policies",
        "AWS ParallelCluster with Slurm scheduler, Amazon FSx for Lustre linked to S3, AWS Batch for job submission, Amazon Aurora for results database, and S3 Standard-IA with Glacier transition"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Batch with Spot handles large-scale compute cost-effectively, FSx for Lustre provides high-performance scratch space essential for genomics, Step Functions orchestrates complex pipelines, S3 offers durable storage, and Intelligent-Tiering automatically optimizes storage costs over 7 years.",
        whyWrong: {
            1: "Fargate has 200GB storage limit insufficient for genomics workloads, and EFS performance doesn't match FSx for Lustre for this use case",
            2: "EMR is optimized for data analytics not bioinformatics pipelines, and Data Pipeline is being deprecated in favor of Step Functions",
            3: "ParallelCluster adds management overhead, and mixing Batch with ParallelCluster creates unnecessary complexity"
        },
        examStrategy: "For HPC/genomics workloads: AWS Batch + FSx for Lustre is the standard pattern. Step Functions excels at complex pipeline orchestration. Spot instances provide massive cost savings for batch workloads."
    }
},
// Continuing questions 51-60...
{
   id: 'sap_051',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A SaaS company's multi-tenant application uses a single RDS PostgreSQL instance (db.r6g.16xlarge) with 15,000 schemas (one per customer). Database performance has degraded with connection pool exhaustion, slow schema migrations taking 48 hours, backup times exceeding 6 hours, and cross-tenant query latency affecting large customers. The company needs to improve performance while preparing for 10x growth.",
   question: "Which modernization strategy BEST addresses the current issues and enables future scaling?",
   options: [
       "Migrate to Aurora PostgreSQL with write-through caching in ElastiCache, implement RDS Proxy for connection pooling, use Blue/Green deployments for schema changes, and partition large tenant data",
       "Implement database-per-tenant with Aurora Serverless v2, use AWS Lambda for schema migrations, implement Amazon DynamoDB for hot data, and consolidate with Amazon Redshift for analytics",
       "Adopt a pooled model with tenant isolation via Row Level Security (RLS), migrate to Aurora PostgreSQL with global database, implement read replicas per region, and use Amazon OpenSearch for complex queries",
       "Transition to a hybrid model with large tenants on dedicated Aurora instances and small tenants on shared clusters, implement AWS Database Migration Service for tenant movement, and use Amazon MemoryDB for caching"
   ],
   correct: 3,
   explanation: {
       correct: "Hybrid model addresses the noisy neighbor problem by isolating large tenants, dedicated instances eliminate cross-tenant performance impact, DMS enables seamless tenant migration between tiers, and MemoryDB provides Redis-compatible caching with durability. This approach balances cost and performance while enabling granular scaling.",
       whyWrong: {
           0: "Single database with 15,000 schemas will still face fundamental PostgreSQL limitations even with Aurora, and schema migration issues persist",
           1: "Database-per-tenant with 15,000 databases becomes extremely expensive and complex to manage, even with Serverless v2",
           2: "RLS with 15,000 tenants in a pooled model creates significant performance overhead and doesn't solve schema migration challenges"
       },
       examStrategy: "For multi-tenant architecture problems at scale, hybrid models (combining pooled and dedicated) often provide the best balance. Consider tenant segmentation based on size/requirements rather than one-size-fits-all approaches."
   }
},
{
   id: 'sap_052',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A broadcasting company needs to migrate 5PB of video archives from LTO tape libraries to AWS. The tapes are stored in an offline vault requiring 24-hour retrieval notice. The migration must maintain the existing Media Asset Management (MAM) system's file paths, support partial file restore for editing workflows, complete within 12 months, and reduce retrieval time from 24 hours to under 1 hour for any asset.",
   question: "Which migration and storage strategy BEST meets these requirements?",
   options: [
       "AWS Snowball Edge for bulk transfer, store in S3 Glacier Flexible Retrieval with S3 Inventory, implement AWS DataSync for ongoing updates, and use S3 Batch Operations for bulk restores",
       "AWS Direct Connect with Storage Gateway Tape Gateway for virtual tape library, migrate to S3 Glacier Instant Retrieval, maintain file interface with Storage Gateway File Gateway, and implement S3 Intelligent-Tiering",
       "Multiple AWS Snowmobile for massive transfer, store in S3 Glacier Deep Archive with metadata in DynamoDB, implement Lambda-based retrieval workflow, and use AWS Backup for management",
       "AWS Snowball Edge devices in parallel, store in S3 Standard with immediate transition to Glacier Instant Retrieval, implement Amazon FSx for Windows File Server for MAM compatibility, and use S3 Object Lambda for path translation"
   ],
   correct: 3,
   explanation: {
       correct: "Parallel Snowball Edge devices accelerate transfer within 12 months, Glacier Instant Retrieval provides <1 hour access at lower cost, FSx for Windows maintains file system semantics required by MAM systems, and S3 Object Lambda transparently handles path translation without MAM changes.",
       whyWrong: {
           0: "Glacier Flexible Retrieval has 1-12 hour retrieval time not meeting <1 hour requirement, and doesn't maintain file system interface for MAM",
           1: "Tape Gateway is for ongoing tape workflows not bulk migration, and wouldn't complete 5PB transfer in 12 months over network",
           2: "Snowmobile is overkill for 5PB and has long logistics time, Deep Archive has 12+ hour retrieval time not meeting requirements"
       },
       examStrategy: "For large-scale tape migrations, consider parallel Snowball devices for speed. Glacier Instant Retrieval is key for <1 hour access at archive prices. FSx provides file system compatibility for legacy applications."
   }
},
{
   id: 'sap_053',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global investment firm has 500 AWS accounts across trading desks, research teams, and support functions. Regulations require complete audit trails, 7-year data retention, real-time anomaly detection for insider trading, and geographic restrictions on certain trading data. The firm needs to implement a data lake for compliance reporting while allowing teams to maintain independent development velocity.",
   question: "Which solution provides comprehensive compliance while preserving team autonomy?",
   options: [
       "AWS Control Tower with custom guardrails, AWS CloudTrail Lake for centralized audit, Amazon Security Lake for threat detection, AWS Organizations SCPs for geographic restrictions, and Amazon DataZone for federated data governance",
       "AWS Landing Zone with account vending, centralized logging to S3, Amazon GuardDuty with custom threat intelligence, AWS Config conformance packs, and AWS Lake Formation with cross-account permissions",
       "AWS Organizations with consolidated billing, Amazon Macie for data discovery, AWS CloudWatch cross-account observability, IAM permission boundaries, and Amazon Athena federated query",
       "AWS SSO with permission sets, AWS CloudTrail organization trail, Amazon Detective for investigation, VPC endpoint policies for data residency, and AWS Glue Data Catalog with Lake Formation"
   ],
   correct: 0,
   explanation: {
       correct: "Control Tower provides governed account provisioning with custom guardrails for regulations, CloudTrail Lake offers managed 7-year retention with SQL querying, Security Lake centralizes security findings with anomaly detection, SCPs enforce geographic restrictions at the organizational level, and DataZone enables federated governance preserving team autonomy.",
       whyWrong: {
           1: "Landing Zone is deprecated in favor of Control Tower, and custom S3 logging requires more management for 7-year compliance retention",
           2: "Lacks centralized security data lake capabilities for insider trading detection, and CloudWatch isn't designed for long-term audit retention",
           3: "Missing automated account governance and Security Lake's purpose-built compliance features for financial services requirements"
       },
       examStrategy: "For enterprise compliance with team autonomy, Control Tower + Security Lake + DataZone is the modern pattern. CloudTrail Lake provides managed long-term audit storage. Remember Landing Zone is deprecated."
   }
},
{
   id: 'sap_054',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A smart city initiative needs to process data from 1 million IoT sensors including traffic cameras, air quality monitors, and utility meters. The platform must handle 10 million messages per second, provide real-time alerts for emergencies (<1 second), support 5-year historical analysis, enable citizen mobile app with 50ms response time, and integrate with 20 different city department systems using various protocols.",
   question: "Which architecture BEST handles this scale and diversity of requirements?",
   options: [
       "AWS IoT Core with Basic Ingest for cost optimization, Amazon Kinesis Data Streams for real-time processing, Amazon Timestream for time-series storage, AWS AppSync for mobile API, and Amazon EventBridge for system integration",
       "AWS IoT SiteWise for sensor management, Amazon MSK for streaming, Amazon S3 with Athena for historical data, Amazon API Gateway with Lambda for mobile backend, and AWS Step Functions for orchestration",
       "AWS IoT Core Fleet Provisioning for device management, Amazon Kinesis Data Analytics for stream processing, Amazon OpenSearch for real-time analytics, Amazon CloudFront with Lambda@Edge for mobile, and Amazon MQ for legacy integration",
       "AWS IoT Device Management for fleet operations, Amazon Managed Service for Apache Flink for complex event processing, hybrid storage with Timestream (hot) and S3 (cold), Amazon Neptune for relationship analysis, and AWS Transfer Family for file-based integration"
   ],
   correct: 0,
   explanation: {
       correct: "IoT Core with Basic Ingest reduces costs by bypassing message broker for high-volume telemetry, Kinesis handles 10M messages/second with multiple shards, Timestream is purpose-built for IoT time-series with automatic tiering, AppSync provides real-time subscriptions with 50ms response via caching, and EventBridge offers native integration with 20+ protocols.",
       whyWrong: {
           1: "IoT SiteWise is for industrial IoT not smart city scale, and API Gateway + Lambda adds latency vs AppSync's managed GraphQL subscriptions",
           2: "Lambda@Edge isn't suitable for complex mobile backend logic, and MQ doesn't scale to 10M messages/second",
           3: "Neptune adds unnecessary complexity for time-series data, and Transfer Family is for file protocols not real-time streaming"
       },
       examStrategy: "For massive IoT scale, IoT Core Basic Ingest is crucial for cost. Timestream is the go-to for IoT time-series data. AppSync excels at real-time mobile/web APIs with subscriptions."
   }
},
{
   id: 'sap_055',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "medium",
   timeRecommendation: 150,
   scenario: "An online education platform experiences severe performance degradation during exam periods. The current architecture uses Application Load Balancer → 30 EC2 instances → RDS MySQL read replicas (1 writer, 5 readers). During exams, database CPU hits 100%, read replica lag exceeds 30 seconds, and students experience timeout errors. The platform serves 500,000 concurrent students during peak exam times with 50,000 exam submissions per minute.",
   question: "Which optimization strategy would MOST effectively resolve the performance issues?",
   options: [
       "Implement Amazon ElastiCache for Redis with write-through caching for exam questions and student responses, add RDS Proxy for connection pooling, implement Aurora Auto Scaling for read replicas, and use SQS for asynchronous exam submission processing",
       "Migrate to Aurora Serverless v2 for automatic scaling, implement DynamoDB for exam session state, add CloudFront for static content, and use Step Functions for exam workflow orchestration",
       "Scale to 60 EC2 instances, add more RDS read replicas, implement database sharding by exam ID, and use Application Load Balancer request routing based on exam type",
       "Containerize on EKS with Horizontal Pod Autoscaler, implement Amazon MemoryDB for session management, migrate to Aurora PostgreSQL for better concurrency, and add API Gateway with throttling"
   ],
   correct: 0,
   explanation: {
       correct: "ElastiCache write-through caching dramatically reduces database load for frequently accessed exam questions, RDS Proxy prevents connection exhaustion during peaks, Aurora Auto Scaling dynamically adjusts read capacity, and SQS decouples submission processing preventing timeout errors during peak submission periods.",
       whyWrong: {
           1: "Serverless v2 might struggle with consistent high load during exams, and mixing DynamoDB for state adds complexity without addressing core database bottleneck",
           2: "Simply adding more resources doesn't solve the fundamental architecture issues, and manual sharding is complex to implement and maintain",
           3: "Containerization doesn't address the database bottleneck, and PostgreSQL migration requires significant application changes"
       },
       examStrategy: "For read-heavy workloads with predictable content (exam questions), caching is crucial. RDS Proxy solves connection pool exhaustion. Asynchronous processing (SQS) prevents cascade failures during peaks."
   }
},
{
   id: 'sap_056',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A pharmaceutical company must migrate a validated GxP system from on-premises VMware to AWS. The system includes 200 VMs, Oracle RAC databases, NFS storage for research data, and integrations with lab equipment. FDA regulations require validation documentation, 21 CFR Part 11 compliance for electronic signatures, no data modification during migration, and ability to demonstrate the migrated system is identical to source.",
   question: "Which migration approach ensures regulatory compliance and system validation?",
   options: [
       "AWS Application Migration Service (MGN) for block-level replication, Amazon FSx for NetApp ONTAP for NFS compatibility, Amazon RDS Custom for Oracle RAC support, AWS Artifact for compliance documentation, and parallel validation environment",
       "VMware Cloud on AWS for lift-and-shift, AWS DataSync for NFS migration, native Oracle RAC on EC2, AWS Config for compliance monitoring, and automated testing with AWS Device Farm",
       "AWS Server Migration Service for VM replication, Amazon EFS for shared storage, Oracle RAC alternatives on Aurora, AWS Audit Manager for compliance, and manual validation procedures",
       "CloudEndure Migration for continuous replication, Amazon FSx for OpenZFS, containerized Oracle on EKS, AWS CloudTrail for audit logs, and infrastructure as code validation"
   ],
   correct: 0,
   explanation: {
       correct: "MGN provides block-level replication ensuring exact copies for validation, FSx for NetApp ONTAP maintains NFS semantics and features critical for no-modification requirement, RDS Custom supports Oracle RAC in managed environment, Artifact provides GxP compliance documentation, and parallel validation enables FDA-required testing without disrupting production.",
       whyWrong: {
           1: "VMware Cloud on AWS is expensive and doesn't provide the compliance artifacts needed for GxP validation",
           2: "SMS is deprecated in favor of MGN, and Aurora doesn't support Oracle RAC features required for validated systems",
           3: "Containerizing Oracle RAC breaks validation as it's not identical to source system, violating FDA requirements"
       },
       examStrategy: "For GxP/regulated migrations, maintaining identical functionality is crucial. MGN is the modern replacement for SMS. RDS Custom enables managed databases with OS access required for Oracle RAC."
   }
},
{
   id: 'sap_057',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A multi-national retail corporation has acquired 5 companies in the last year, each with their own AWS accounts, technology stacks, and DevOps practices. The CTO mandates unified CI/CD pipelines, centralized security scanning, consistent tagging for cost allocation, and shared services (authentication, monitoring, logging) while allowing acquired companies to maintain their deployment autonomy for 18 months during integration.",
   question: "Which approach BEST balances standardization with autonomy during the integration period?",
   options: [
       "AWS Control Tower for account governance, AWS Service Catalog for approved services, AWS CodePipeline with shared templates, Amazon ECR with image scanning, AWS Organizations tag policies, and AWS SSO for centralized authentication",
       "AWS Organizations for account structure, AWS CodeCommit with branch protections, Jenkins on ECS for CI/CD, Amazon Inspector for security, manual tagging enforcement, and Active Directory federation",
       "AWS Landing Zone for multi-account, GitLab CI/CD with runners in each account, AWS Security Hub for scanning, Cost and Usage Reports for allocation, and Okta for identity management",
       "GitHub Actions with self-hosted runners, AWS CloudFormation StackSets for governance, Amazon GuardDuty for security, AWS Resource Groups for tagging, and AWS Cognito for authentication"
   ],
   correct: 0,
   explanation: {
       correct: "Control Tower provides governed account structure with guardrails allowing autonomy within boundaries, Service Catalog enables self-service while maintaining compliance, CodePipeline templates standardize CI/CD while allowing customization, ECR scanning centralizes security, tag policies enforce standards automatically, and SSO provides unified authentication without disrupting existing workflows.",
       whyWrong: {
           1: "CodeCommit forces repository migration disrupting existing workflows, and manual tagging won't ensure compliance across acquired companies",
           2: "Landing Zone is deprecated, and GitLab requires additional infrastructure management vs native AWS services",
           3: "GitHub Actions with self-hosted runners adds complexity, and StackSets alone doesn't provide the governance framework needed"
       },
       examStrategy: "For M&A scenarios, Control Tower + Service Catalog enables governed self-service. Tag policies ensure compliance without manual intervention. Native AWS services reduce integration complexity during transitions."
   }
},
{
   id: 'sap_058',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A social media platform is building a content moderation system that must process 100 million posts daily containing text, images, and videos. The system needs to detect harmful content in <2 seconds, support 50 languages, adapt to emerging threat patterns, maintain 99.99% accuracy for CSAM detection, provide explainable decisions for appeals, and comply with different regional content policies.",
   question: "Which architecture provides the MOST comprehensive and scalable content moderation solution?",
   options: [
       "Amazon Rekognition Content Moderation for images/video, Amazon Comprehend for text analysis with custom classifiers, Amazon Augmented AI (A2I) for human review workflows, Amazon SageMaker for custom models, DynamoDB for decision storage, and Lambda@Edge for regional policy enforcement",
       "Amazon Textract for text extraction, Amazon Translate for multi-language support, Amazon Personalize for threat pattern learning, Amazon Fraud Detector for risk scoring, S3 Intelligent-Tiering for content storage, and Step Functions for orchestration",
       "Amazon Transcribe for video-to-text, Amazon Lex for intent detection, Amazon Forecast for threat prediction, Amazon Neptune for relationship analysis, ElastiCache for real-time scoring, and API Gateway for content submission",
       "Amazon Polly for audio analysis, Amazon Kendra for content indexing, Amazon CodeGuru for pattern detection, Amazon QuickSight for moderation dashboards, Kinesis Data Streams for ingestion, and CloudFront for content delivery"
   ],
   correct: 0,
   explanation: {
       correct: "Rekognition Content Moderation is purpose-built for harmful content detection in images/video, Comprehend with custom classifiers handles multi-language text analysis and can be trained on emerging threats, A2I enables human-in-the-loop for high-stakes decisions and appeals, SageMaker allows custom models for CSAM with required accuracy, DynamoDB provides fast decision storage and retrieval, and Lambda@Edge enables regional policy variations at the edge.",
       whyWrong: {
           1: "Textract is for document text extraction not content moderation, Personalize is for recommendations not threat detection, and Fraud Detector isn't designed for content moderation",
           2: "Transcribe is for speech-to-text not content analysis, Lex is for chatbots not content moderation, and Forecast is for time-series not threat detection",
           3: "Polly is for text-to-speech not analysis, CodeGuru is for code review not content patterns, and this lacks core content moderation capabilities"
       },
       examStrategy: "For content moderation, Rekognition Content Moderation + Comprehend is the core combination. A2I is crucial for human review requirements. SageMaker enables custom models for specific compliance needs."
   }
},
{
   id: 'sap_059',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A logistics company's route optimization system processes 50,000 delivery routes daily using a monolithic .NET application on Windows EC2 instances. The system takes 6 hours for daily batch processing, costs $200,000/month, cannot handle real-time route updates, and fails to scale during peak seasons. The company wants to enable real-time optimization while reducing processing time to under 1 hour.",
   question: "Which modernization approach BEST achieves real-time capabilities and performance targets?",
   options: [
       "Decompose into microservices on AWS Lambda with .NET 6 runtime, use Step Functions Express Workflows for orchestration, implement Amazon Location Service for routing, DynamoDB for route storage with DAX caching, and EventBridge for real-time triggers",
       "Containerize on ECS with AWS Fargate for Windows, implement Amazon SQS for job queuing, use Amazon ElastiCache for optimization caching, Aurora PostgreSQL for data, and Application Load Balancer for traffic distribution",
       "Migrate to AWS Batch with Spot Fleet for batch processing, implement Kinesis Data Streams for real-time updates, use Amazon SageMaker for route optimization ML, Redshift for analytics, and API Gateway for real-time endpoints",
       "Refactor to .NET Core on Linux with EKS and Karpenter, implement Apache Kafka on MSK for event streaming, use Amazon Neptune for route graph analysis, OpenSearch for real-time queries, and CloudFront for API caching"
   ],
   correct: 0,
   explanation: {
       correct: "Lambda with .NET 6 enables massive parallelization reducing 6 hours to <1 hour, Step Functions Express Workflows orchestrate high-volume, short-duration route calculations efficiently, Location Service provides managed routing without custom algorithms, DynamoDB with DAX delivers microsecond latency for real-time updates, and EventBridge enables event-driven real-time optimization triggers.",
       whyWrong: {
           1: "Fargate for Windows has higher costs and startup latency, and doesn't provide the parallelization needed to meet 1-hour target",
           2: "Batch is for scheduled jobs not real-time processing, and mixing batch with streaming adds unnecessary complexity",
           3: "Full Kubernetes adds operational overhead, and Neptune is overkill for route optimization vs purpose-built Location Service"
       },
       examStrategy: "For batch-to-real-time modernization, serverless (Lambda) provides best parallelization. Location Service is purpose-built for routing. Step Functions Express Workflows are designed for high-volume, short-duration workflows."
   }
},
{
   id: 'sap_060',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A national hospital network needs to migrate 200 applications from 5 data centers to AWS. Applications include Epic EHR, PACS imaging systems storing 2PB of DICOM files, legacy MUMPS databases, Windows-based clinical apps, and Linux research workloads. Migration must maintain HIPAA compliance, support hybrid operation for 2 years, complete critical systems within 6 months, and achieve 30% cost reduction.",
   question: "Which migration strategy and tooling combination BEST meets these complex requirements?",
   options: [
       "AWS Migration Hub for orchestration, Application Migration Service for servers, DataSync for PACS data, AWS HealthLake for DICOM storage, Database Migration Service for MUMPS to DocumentDB, AWS Outposts for hybrid connectivity, and Reserved Instances for cost optimization",
       "Migration Evaluator for assessment, AWS Application Discovery Service for dependency mapping, Snowball Edge for PACS transfer, S3 with Gateway Endpoints for DICOM, mainframe modernization for MUMPS, Direct Connect for hybrid, and Savings Plans for cost reduction",
       "AWS MAP (Migration Acceleration Program) for funding and expertise, MGN for critical systems, AWS Storage Gateway for PACS caching, FSx for Windows for clinical apps, MUMPS containers on EKS, Transit Gateway for hybrid network, and Spot Instances where applicable",
       "CloudEndure for continuous replication, AWS Backup for data protection, Snowmobile for bulk transfer, Glacier for DICOM archival, RDS Custom for MUMPS migration, VPN for connectivity, and Cost Explorer for optimization"
   ],
   correct: 0,
   explanation: {
       correct: "Migration Hub provides centralized tracking across 200 applications, MGN enables rapid migration of critical systems within 6 months, DataSync handles continuous PACS synchronization, HealthLake is HIPAA-compliant and purpose-built for medical imaging with DICOM support, DMS can migrate MUMPS to DocumentDB maintaining hierarchical structure, Outposts enables true hybrid infrastructure for 2-year transition, and RIs provide predictable 30% cost savings.",
       whyWrong: {
           1: "Lacks specific healthcare capabilities of HealthLake, and mainframe modernization for MUMPS is unnecessarily complex for hospital systems",
           2: "MAP is a program not a tool, FSx for Windows doesn't handle DICOM natively, and containerizing MUMPS adds risk for critical healthcare systems",
           3: "CloudEndure is superseded by MGN, Glacier doesn't support DICOM querying needed for PACS, and RDS Custom doesn't support MUMPS"
       },
       examStrategy: "For healthcare migrations, HealthLake is purpose-built for medical imaging. MGN (successor to CloudEndure) is the modern server migration service. Outposts provides true hybrid infrastructure for extended transitions."
   }
},

// Continuing questions 61-70...
{
   id: 'sap_061',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global manufacturing conglomerate operates 100 factories across 40 countries, each with its own AWS account for factory automation and IoT data. Corporate requires unified predictive maintenance analytics, real-time production monitoring, centralized quality control ML models, and supply chain visibility. Each factory must maintain operational independence due to regulations and network reliability concerns. The solution must handle 50TB daily data ingestion and support offline operation for 72 hours.",
   question: "Which architecture BEST balances central analytics with factory autonomy and offline resilience?",
   options: [
       "AWS IoT SiteWise Edge for local data collection with buffering, AWS IoT Greengrass for edge ML inference, AWS DataSync for resilient data transfer to central data lake, Amazon SageMaker multi-model endpoints for centralized training with edge deployment, and AWS IoT TwinMaker for unified digital twins",
       "AWS Outposts in each factory for local processing, Amazon MSK for event streaming, AWS Direct Connect with backup VPN, centralized Amazon Redshift for analytics, and AWS Lambda for real-time processing",
       "AWS Snow Family for edge computing, Amazon Kinesis Data Firehose for ingestion, S3 Cross-Region Replication for data distribution, Amazon EMR for big data processing, and QuickSight for dashboards",
       "AWS Local Zones for factory proximity, Amazon RDS with read replicas, AWS Batch for processing, centralized Elasticsearch for analytics, and AWS AppSync for offline synchronization"
   ],
   correct: 0,
   explanation: {
       correct: "IoT SiteWise Edge provides industrial data collection with 72-hour buffering for offline resilience, Greengrass enables edge ML inference maintaining operations during disconnection, DataSync handles intermittent connectivity gracefully, SageMaker multi-model endpoints allow centralized training with optimized edge deployment, and IoT TwinMaker provides unified visibility across all factories while respecting autonomy.",
       whyWrong: {
           1: "Outposts requires reliable connectivity and is expensive for 100 factories, MSK doesn't handle offline scenarios well",
           2: "Snow Family is for migration not permanent edge computing, and Kinesis Firehose requires consistent connectivity",
           3: "Local Zones aren't available in all 40 countries, and RDS replicas don't support offline operation for 72 hours"
       },
       examStrategy: "For industrial IoT with offline requirements, IoT SiteWise Edge + Greengrass is the standard pattern. IoT TwinMaker provides digital twin capabilities for manufacturing. DataSync handles unreliable networks better than streaming services."
   }
},
{
   id: 'sap_062',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A cryptocurrency exchange is building a new trading platform requiring: processing 1 million transactions per second with <10ms latency, maintaining full ACID compliance for financial transactions, supporting complex queries for regulatory reporting, storing all transaction history immutably for 10 years, providing real-time fraud detection, and ensuring 99.999% availability with zero data loss.",
   question: "Which architecture BEST meets these extreme performance and compliance requirements?",
   options: [
       "Amazon MemoryDB for ultra-low latency with persistence, Amazon QLDB for immutable transaction ledger, Amazon Kinesis Data Analytics for real-time fraud detection, Amazon S3 with Object Lock for long-term storage, and Amazon DocumentDB for complex queries",
       "Amazon DynamoDB with Global Tables for performance, Amazon Timestream for time-series transactions, AWS Lambda for fraud detection, Amazon Macie for compliance monitoring, and Amazon Neptune for transaction relationships",
       "Amazon ElastiCache for Redis with cluster mode, Amazon Aurora with write forwarding, Amazon Fraud Detector for risk scoring, AWS Backup with vault lock, and Amazon Athena for regulatory queries",
       "Amazon Keyspaces for Cassandra compatibility, Amazon EventBridge for event processing, Amazon SageMaker for fraud models, Amazon Glacier for archival, and Amazon Redshift for reporting"
   ],
   correct: 0,
   explanation: {
       correct: "MemoryDB provides Redis-compatible performance with <10ms latency plus Multi-AZ persistence ensuring zero data loss, QLDB offers immutable ledger with cryptographic verification required for financial compliance, Kinesis Analytics enables real-time fraud detection on streaming transactions, S3 Object Lock ensures immutable 10-year retention, and DocumentDB handles complex regulatory queries with MongoDB compatibility.",
       whyWrong: {
           1: "DynamoDB lacks ACID transactions for complex financial operations, and Timestream isn't designed for transactional workloads requiring immediate consistency",
           2: "ElastiCache for Redis lacks the durability guarantees of MemoryDB, and Aurora can't achieve 1M TPS with <10ms latency",
           3: "Keyspaces is eventually consistent not suitable for financial transactions, and Glacier retrieval time conflicts with reporting requirements"
       },
       examStrategy: "For ultra-high performance with durability, MemoryDB > ElastiCache. QLDB is purpose-built for immutable financial ledgers. Always verify ACID compliance for financial workloads."
   }
},
{
   id: 'sap_063',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A video game company's multiplayer platform supports 10 million daily active users. Current architecture uses Network Load Balancer → 100 c5.4xlarge EC2 instances → Amazon DynamoDB. Problems include: $300,000 monthly costs, 15% of instances idle during off-peak, connection drops during auto-scaling events, DynamoDB hot partition keys during viral events, and 5-second matchmaking latency. The platform needs to reduce costs by 40% while improving matchmaking to <1 second.",
   question: "Which optimization strategy BEST achieves both cost reduction and performance improvement?",
   options: [
       "Implement AWS GameLift FleetIQ for intelligent Spot instance management, Amazon ElastiCache for matchmaking cache, DynamoDB Contributor Insights to identify and fix hot keys, Application Load Balancer with connection draining, and Compute Savings Plans for remaining On-Demand instances",
       "Migrate to Amazon EKS with Cluster Autoscaler, implement Redis on Amazon MemoryDB for session state, partition DynamoDB tables by region, use AWS Global Accelerator for anycast, and Reserved Instances for baseline capacity",
       "Containerize on ECS with capacity providers mixing Spot and On-Demand, implement Amazon GameLift FlexMatch for matchmaking, use DynamoDB Global Tables with auto-scaling, implement AWS App Mesh for service mesh, and Spot Fleet for batch workloads",
       "Transition to Lambda for connection handling, Amazon Gamelift Realtime Servers for game sessions, Amazon Neptune for player graphs, DynamoDB Accelerator for caching, and CloudFront for static assets"
   ],
   correct: 0,
   explanation: {
       correct: "GameLift FleetIQ optimizes Spot instances specifically for gaming workloads achieving 40%+ savings, ElastiCache dramatically reduces matchmaking queries to DynamoDB improving speed to <1 second, Contributor Insights identifies hot keys for targeted fixes, ALB with connection draining prevents drops during scaling, and Savings Plans provide additional 20% savings on remaining On-Demand usage.",
       whyWrong: {
           1: "Generic EKS doesn't provide gaming-specific optimizations of GameLift, and regional partitioning doesn't solve hot key issues",
           2: "FlexMatch without FleetIQ misses Spot optimization opportunity, and Global Tables add cost without solving the hot partition problem",
           3: "Lambda isn't suitable for stateful game connections, and Neptune is overkill for matchmaking compared to purpose-built ElastiCache"
       },
       examStrategy: "For gaming workloads, GameLift services provide purpose-built optimizations. Contributor Insights is key for diagnosing DynamoDB hot partitions. Combine multiple cost optimization strategies (Spot + Savings Plans)."
   }
},
{
   id: 'sap_064',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A European bank must migrate from IBM mainframe z/OS to AWS within 18 months due to data center closure. The system includes: 50,000 COBOL programs, 100TB of DB2 data with referential integrity, CICS transactions processing 2 million daily operations, JCL batch jobs with complex dependencies, VSAM files, and MQ Series integrations. EU banking regulations require parallel run validation and zero data loss.",
   question: "Which migration approach ensures regulatory compliance and timeline achievement?",
   options: [
       "AWS Mainframe Modernization Service with Blu Age for automated refactoring to Java, Amazon Aurora PostgreSQL for DB2 migration, Amazon MQ for MQ Series compatibility, AWS Batch for JCL replacement, Step Functions for job orchestration, and 6-month parallel run with data reconciliation",
       "AWS Mainframe Modernization Service with Micro Focus for rehosting, RDS for Db2 to maintain compatibility, AWS Transfer Family for file transfer, Amazon Managed Workflows for Apache Airflow for batch, EventBridge for CICS events, and incremental cutover with rollback capability",
       "Manual COBOL to Java conversion with contractors, DynamoDB for modernized data model, Lambda for CICS replacement, ECS for batch processing, Amazon MSK for messaging, and big-bang cutover with extensive testing",
       "Containerize COBOL with OpenCOBOL on EKS, migrate DB2 to Amazon RDS Custom, implement Kafka for event streaming, AWS Glue for ETL, API Gateway for service exposure, and phased migration by business function"
   ],
   correct: 0,
   explanation: {
       correct: "Blu Age automated refactoring reduces risk and time vs manual conversion, Aurora PostgreSQL provides DB2 compatibility with better AWS integration, Amazon MQ maintains MQ Series protocols for seamless integration, AWS Batch naturally replaces JCL batch processing, Step Functions handles complex job dependencies, and 6-month parallel run satisfies regulatory validation requirements.",
       whyWrong: {
           1: "Rehosting with Micro Focus doesn't modernize for cloud benefits, and MWAA isn't designed for mainframe batch job complexity",
           2: "Manual conversion of 50,000 programs in 18 months is unrealistic, and big-bang cutover violates regulatory parallel run requirements",
           3: "OpenCOBOL containerization lacks enterprise support needed for banking, and phased migration is complex with tightly coupled mainframe systems"
       },
       examStrategy: "For mainframe modernization under time pressure, automated refactoring (Blu Age) > manual conversion. AWS Mainframe Modernization Service is purpose-built. Parallel run is mandatory for regulated industries."
   }
},
{
   id: 'sap_065',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global pharmaceutical company has 300 AWS accounts across R&D, manufacturing, and commercial divisions. New regulations require: segregation of duties with approval workflows, encryption of all data with customer-managed keys, 90-day maximum access for external researchers, quarterly access reviews with evidence, immutable audit logs for 7 years, and automated remediation of non-compliant resources within 1 hour.",
   question: "Which solution MOST comprehensively addresses these regulatory requirements?",
   options: [
       "AWS Control Tower with custom controls, AWS IAM Identity Center with temporary elevated access management, AWS CloudTrail Lake for immutable audit storage, AWS Config with auto-remediation rules, AWS KMS with key policies enforcing encryption, and AWS Audit Manager for compliance evidence collection",
       "AWS Organizations with SCPs, AWS SSO with session policies, S3 Object Lock for audit logs, AWS Systems Manager for remediation, CloudHSM for key management, and AWS Security Hub for compliance monitoring",
       "AWS Landing Zone with guardrails, IAM roles with external ID, CloudWatch Logs with encryption, AWS Lambda for auto-remediation, AWS Secrets Manager for key rotation, and Amazon Macie for data discovery",
       "Federated access with SAML, IAM permission boundaries, Amazon QLDB for audit trail, AWS Config conformance packs, customer-managed CMKs, and Amazon Detective for investigation"
   ],
   correct: 0,
   explanation: {
       correct: "Control Tower enforces preventive controls for segregation of duties, IAM Identity Center's temporary elevated access management provides approval workflows with automatic expiration for 90-day requirement, CloudTrail Lake offers 7-year immutable storage with SQL querying, Config auto-remediation ensures 1-hour compliance, KMS key policies enforce encryption at scale, and Audit Manager automates evidence collection for quarterly reviews.",
       whyWrong: {
           1: "SSO lacks temporary elevated access workflows, S3 Object Lock requires manual configuration per bucket, and CloudHSM is overkill for this use case",
           2: "Landing Zone is deprecated, CloudWatch Logs isn't designed for 7-year immutable audit storage, and lacks automated compliance evidence collection",
           3: "Missing centralized governance framework, QLDB is for application data not audit logs, and lacks automated access review capabilities"
       },
       examStrategy: "For comprehensive compliance: Control Tower + IAM Identity Center (formerly SSO) + Audit Manager is the modern pattern. CloudTrail Lake provides managed long-term audit storage. Temporary elevated access is key for external users."
   }
},
{
   id: 'sap_066',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A metaverse platform needs to support 1 million concurrent users in shared virtual spaces. Requirements include: <50ms latency for user interactions globally, real-time 3D asset streaming (100GB+ per space), spatial audio processing for proximity chat, physics simulation at 60 FPS, blockchain integration for NFT ownership, AI-driven NPCs with natural language, and 8K resolution support for VR headsets.",
   question: "Which architecture BEST delivers this immersive metaverse experience at scale?",
   options: [
       "Amazon CloudFront with multiple origins for asset delivery, AWS Wavelength for ultra-low latency compute, Amazon GameLift Realtime Servers for world hosting, Amazon IVS for audio/video streaming, Amazon Managed Blockchain for NFTs, Amazon Polly and Lex for NPC interactions, and AWS Local Zones for regional presence",
       "AWS Global Accelerator for anycast routing, Amazon EC2 G5 instances for GPU rendering, Amazon FSx for Lustre for asset storage, Amazon Chime SDK for spatial audio, Amazon Managed Blockchain for NFTs, Amazon Bedrock for AI NPCs, and CloudFront for content delivery",
       "Amazon Lightsail for simple deployment, Amazon S3 for asset storage, AWS AppSync for real-time synchronization, Amazon Kinesis Video Streams for streaming, Amazon QLDB for NFT ledger, Amazon Comprehend for chat moderation, and Route 53 for geo-routing",
       "AWS Outposts for on-premise deployment, Amazon ECS with GPU support, Amazon EFS for shared storage, Amazon Connect for voice chat, DynamoDB for NFT tracking, Amazon Rekognition for avatar creation, and Direct Connect for dedicated bandwidth"
   ],
   correct: 0,
   explanation: {
       correct: "CloudFront with multiple origins efficiently delivers massive 3D assets globally, Wavelength provides edge computing with <50ms latency for mobile/5G users, GameLift Realtime Servers are purpose-built for multiplayer synchronization at 60 FPS, IVS handles scalable audio/video streaming for proximity chat, Managed Blockchain provides true NFT ownership, Polly and Lex enable natural NPC interactions, and Local Zones extend presence to metro areas.",
       whyWrong: {
           1: "G5 instances for 1M users would be extremely expensive, and Chime SDK isn't designed for spatial audio in 3D environments",
           2: "Lightsail lacks the scale for 1M users, and Kinesis Video isn't suitable for 3D asset streaming or interactive content",
           3: "Outposts doesn't provide global distribution, and Connect is for call centers not spatial audio in virtual worlds"
       },
       examStrategy: "For metaverse/gaming at scale: GameLift for multiplayer, Wavelength for mobile edge computing, IVS for streaming. Managed Blockchain (not QLDB) for true blockchain/NFT requirements."
   }
},
{
   id: 'sap_067',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "medium",
   timeRecommendation: 150,
   scenario: "A news website runs on AWS with CloudFront → ALB → 20 m5.large EC2 instances → RDS MySQL. During breaking news, traffic increases 50x causing: CloudFront cache misses due to dynamic content, ALB target group unhealthy hosts, RDS connection exhaustion, and $50,000 unexpected data transfer costs. The site needs to handle viral traffic while controlling costs.",
   question: "Which optimization strategy MOST effectively handles viral traffic while minimizing costs?",
   options: [
       "Implement CloudFront origin request policies to increase cache hit ratio, configure ALB with slow start and connection draining, add RDS Proxy for connection pooling, enable Auto Scaling with predictive scaling, and use CloudFront Origin Shield to reduce origin requests",
       "Add more EC2 instances preemptively, upgrade to RDS instance with more connections, implement CloudWatch alarms for scaling, increase CloudFront TTLs, and purchase data transfer credits",
       "Migrate to Lambda@Edge for dynamic content, implement API Gateway caching, use DynamoDB for session state, add ElastiCache for database caching, and enable CloudFront compression",
       "Switch to Lightsail for predictable pricing, implement Varnish cache on EC2, use read replicas for RDS, add CloudFlare in front of CloudFront, and implement rate limiting"
   ],
   correct: 0,
   explanation: {
       correct: "Origin request policies optimize caching for dynamic content reducing backend load and transfer costs, slow start prevents overwhelming new instances during scaling, RDS Proxy eliminates connection exhaustion, predictive scaling anticipates viral traffic patterns, and Origin Shield acts as an additional caching layer reducing origin requests by up to 88% and associated data transfer costs.",
       whyWrong: {
           1: "Preemptively adding instances wastes money during normal traffic, and simply increasing TTLs doesn't work for dynamic news content",
           2: "Lambda@Edge adds complexity and cold starts during traffic spikes, and full migration during an incident is risky",
           3: "Lightsail has scaling limitations for viral traffic, and adding CloudFlare to CloudFront is redundant and adds latency"
       },
       examStrategy: "For viral traffic optimization: Origin Shield is key for reducing origin load. RDS Proxy solves connection pooling. Origin request policies optimize dynamic content caching without stale data."
   }
},
{
   id: 'sap_068',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A government agency needs to migrate 1,000 applications from on-premises data centers to AWS GovCloud. Applications include classified systems (IL5), citizen-facing services requiring 24/7 availability, legacy Oracle forms applications, mainframe COBOL systems, and modern microservices. Migration must complete in 24 months, maintain FedRAMP High compliance, and achieve 25% cost reduction.",
   question: "Which migration strategy and approach ensures compliance while meeting aggressive timelines?",
   options: [
       "Portfolio assessment with Migration Evaluator, wave-based migration using AWS Application Migration Service for lift-and-shift, AWS Mainframe Modernization for COBOL systems, Oracle APEX on RDS for forms migration, AWS Control Tower for GovCloud account setup, and Reserved Instances for cost optimization",
       "AWS Professional Services engagement, containerization with EKS for microservices, rehost mainframes on EC2, Oracle Cloud@Customer for Oracle workloads, AWS Landing Zone for governance, and Spot Instances for development environments",
       "Partner-led migration with AWS MAP funding, CloudEndure for replication, serverless refactoring for citizen services, mainframe replacement with COTS solutions, Direct Connect to GovCloud, and Savings Plans for commitment discounts",
       "In-house migration team, manual application refactoring, AWS Outposts for classified systems, hybrid cloud with VMware on AWS, VPN connectivity, and on-demand pricing for flexibility"
   ],
   correct: 0,
   explanation: {
       correct: "Migration Evaluator provides data-driven portfolio analysis for 1,000 apps, wave-based approach enables systematic migration within 24 months, MGN allows rapid lift-and-shift for quick wins, Mainframe Modernization handles COBOL systems with compliance, Oracle APEX provides compatible forms migration path, Control Tower ensures FedRAMP compliance from day one, and RIs deliver 25% cost savings for government's predictable workloads.",
       whyWrong: {
           1: "Oracle Cloud@Customer isn't available in GovCloud, and rehosting mainframes on EC2 doesn't modernize or reduce costs",
           2: "CloudEndure is deprecated for MGN, COTS replacement for mainframe requires longer than 24 months, and partner-led may lack security clearances",
           3: "Outposts in GovCloud adds complexity, VMware on AWS is expensive, and on-demand pricing won't achieve 25% cost reduction"
       },
       examStrategy: "For government migrations: GovCloud requires specific services, Control Tower ensures compliance, wave-based migration manages complexity. MGN replaced CloudEndure. Consider security clearance requirements."
   }
},
{
   id: 'sap_069',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A financial services firm has 200 development teams across 50 AWS accounts. Each team deploys 20+ times daily to production. The firm needs to implement: security scanning for all deployments, prevention of cryptocurrency mining, secrets rotation every 30 days, cost allocation per feature/deployment, compliance with SOC2 and PCI-DSS, and deployment rollback within 2 minutes. Teams must maintain deployment velocity.",
   question: "Which solution provides comprehensive governance without impacting deployment speed?",
   options: [
       "AWS CodePipeline with native security stages, Amazon ECR image scanning with Security Hub integration, AWS Secrets Manager with automatic rotation, AWS Organizations cost allocation tags with automated enforcement, AWS Config conformance packs for compliance, and AWS CodeDeploy with automatic rollback on CloudWatch alarms",
       "GitLab CI/CD with security scanning, Amazon Inspector for vulnerability assessment, HashiCorp Vault for secrets, manual tagging policies, AWS Audit Manager for compliance, and blue-green deployments with Route 53",
       "Jenkins with security plugins, AWS Systems Manager Parameter Store, CloudFormation drift detection, Cost Explorer with custom reports, third-party compliance tools, and canary deployments with Lambda",
       "GitHub Actions with security workflows, Amazon GuardDuty for threat detection, AWS KMS for encryption, AWS Budgets for cost control, AWS Security Hub for compliance, and feature flags for rollback"
   ],
   correct: 0,
   explanation: {
       correct: "CodePipeline native security stages maintain velocity without external dependencies, ECR scanning with Security Hub provides centralized vulnerability management preventing crypto mining, Secrets Manager automatic rotation ensures 30-day compliance without manual intervention, Organizations tag enforcement ensures accurate cost allocation, Config conformance packs provide continuous SOC2/PCI compliance checking, and CodeDeploy automatic rollback achieves 2-minute recovery using CloudWatch metrics.",
       whyWrong: {
           1: "GitLab adds external dependency and complexity, manual tagging won't ensure compliance across 200 teams, and blue-green doubles infrastructure costs",
           2: "Jenkins requires significant maintenance overhead, Parameter Store lacks automatic rotation, and drift detection is reactive not preventive",
           3: "GitHub Actions lacks native AWS security integration, GuardDuty is detective not preventive for crypto mining, and feature flags require application changes"
       },
       examStrategy: "For high-velocity governance: native AWS services reduce complexity. Automatic enforcement > manual policies. ECR scanning prevents malicious images. CodeDeploy provides fastest rollback with CloudWatch integration."
   }
},
{
   id: 'sap_070',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "An AI startup is building a real-time video analysis platform for 10,000 security cameras. The platform must: detect threats in <2 seconds, support 4K video streams, identify 500 different object types, track people across multiple cameras, store 30 days of video with instant replay, provide custom AI model training per customer, and maintain privacy compliance with automatic face blurring.",
   question: "Which architecture BEST balances real-time performance with scalability and privacy?",
   options: [
       "Amazon Kinesis Video Streams for ingestion, Amazon Rekognition Custom Labels for object detection, AWS Panorama for edge inference, Amazon SageMaker for model training, Amazon S3 Intelligent-Tiering for video storage, Amazon OpenSearch for metadata indexing, and Lambda functions for face blurring with Rekognition",
       "AWS IoT Core for camera management, Amazon IVS for video ingestion, Rekognition Video for analysis, SageMaker Neo for edge deployment, DynamoDB for metadata, S3 Glacier for storage, and EC2 with GPU for custom processing",
       "Direct camera integration to EC2 with NVIDIA GPUs, custom YOLO models for detection, Amazon Neptune for person tracking graphs, EFS for video storage, ElastiCache for metadata, and Batch for model training",
       "CloudFront for video ingestion, Lambda for initial processing, Textract for text detection, Comprehend for analysis, S3 for storage, Athena for queries, and Mechanical Turk for labeling"
   ],
   correct: 0,
   explanation: {
       correct: "Kinesis Video Streams handles massive concurrent 4K streams with built-in durability, Rekognition Custom Labels enables training models for 500 specific object types, Panorama provides edge inference for <2 second detection, SageMaker offers scalable custom model training per customer, S3 Intelligent-Tiering optimizes 30-day storage costs, OpenSearch enables fast metadata searches for instant replay, and Lambda with Rekognition provides serverless real-time face blurring for privacy.",
       whyWrong: {
           1: "IVS is for live streaming to viewers not security analysis, and SageMaker Neo alone doesn't provide edge infrastructure like Panorama",
           2: "Direct EC2 integration lacks scalability for 10,000 cameras, and EFS isn't cost-effective for 30 days of 4K video storage",
           3: "CloudFront isn't designed for camera ingestion, and Textract/Comprehend are for documents not video analysis"
       },
       examStrategy: "For video analysis at scale: Kinesis Video Streams for ingestion, Rekognition for AI/ML, Panorama for edge ML. S3 Intelligent-Tiering optimizes storage costs. Always address privacy requirements explicitly."
   }
},

// Continuing questions 71-80...
{
   id: 'sap_071',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A streaming service operates globally with 50 million subscribers. Current architecture uses CloudFront → 500 EC2 instances across 6 regions → Multi-region Aurora MySQL. Issues include: $2M monthly AWS bill, 40% of compute capacity unused during off-peak, 15-second video start time, regional failover takes 10 minutes causing customer churn, and personalization algorithm takes 24 hours to update. Target: 50% cost reduction and 3-second video start time.",
   question: "Which optimization strategy BEST achieves both aggressive cost and performance targets?",
   options: [
       "Implement CloudFront Origin Shield with cache optimization, replace EC2 with ECS Fargate Spot for stateless transcoding, Aurora Global Database with write forwarding for <1 second failover, S3 Intelligent-Tiering for video assets, Amazon Personalize for real-time recommendations, and Compute Savings Plans for remaining infrastructure",
       "Add more CloudFront edge locations, implement EC2 Auto Scaling with predictive scaling, Aurora read replicas in all regions, ElastiCache for session caching, SageMaker for personalization, and Reserved Instances for baseline capacity",
       "Migrate to Lambda@Edge for dynamic content, implement Kubernetes with Karpenter, DynamoDB Global Tables for metadata, S3 Transfer Acceleration, custom ML on EC2, and Spot Instances for everything",
       "Use CloudFront Functions for personalization, containerize on App Runner, migrate to Timestream for analytics, implement Kinesis for streaming, Neptune for recommendations, and on-demand pricing for flexibility"
   ],
   correct: 0,
   explanation: {
       correct: "Origin Shield reduces origin requests by 88% cutting transfer costs significantly, Fargate Spot for stateless transcoding saves 70% over EC2, Aurora Global with write forwarding enables <1 second failover preventing churn, S3 Intelligent-Tiering automatically optimizes storage costs for massive video libraries, Personalize provides real-time recommendations vs 24-hour batch, and Compute Savings Plans add 27% additional savings - combining to exceed 50% cost reduction while improving performance.",
       whyWrong: {
           1: "Adding edge locations increases costs, predictive scaling helps but doesn't address 40% waste, and 24-hour SageMaker batch doesn't improve personalization speed",
           2: "Lambda@Edge has payload limits for video, Karpenter adds complexity, and Spot for everything risks availability issues for customer-facing services",
           3: "CloudFront Functions are too limited for complex personalization, App Runner doesn't suit video workloads, and Timestream is for IoT not video streaming"
       },
       examStrategy: "For video streaming optimization: Origin Shield is crucial for CDN costs. Fargate Spot works well for stateless media processing. Aurora Global write forwarding enables true active-active multi-region."
   }
},
{
   id: 'sap_072',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A 150-year-old insurance company must migrate from multiple data centers to AWS. Systems include: AS/400 with RPG programs for policy management, 30-year-old actuarial models in FORTRAN, Windows-based call center applications, 500TB of scanned documents dating to 1920s, modern web applications, and real-time integration with 1000+ insurance agencies. Migration must maintain ISO 27001 certification and complete core systems within 12 months.",
   question: "Which migration approach handles this diversity of legacy systems while maintaining compliance?",
   options: [
       "AWS Migration Hub Orchestra for coordination, Mainframe Modernization Service for AS/400 migration to cloud-native, High Performance Computing (HPC) on AWS Batch for FORTRAN models, Amazon Connect for call center modernization, S3 Intelligent-Tiering with Amazon Textract for document digitization, and AWS App2Container for modern apps",
       "Lift-and-shift AS/400 to EC2, containerize FORTRAN with Singularity on ECS, Amazon WorkSpaces for call center, S3 Glacier for documents, replatform web apps to Elastic Beanstalk, and Direct Connect for agency connectivity",
       "Rewrite everything in microservices on Lambda, replace actuarial models with SageMaker, implement Amazon Chime for call center, scan documents to DynamoDB, containerize on EKS, and API Gateway for partner integration",
       "AWS Outposts for AS/400 hosting, keep FORTRAN on-premises with hybrid connectivity, Citrix on AWS for call center, Storage Gateway for documents, lift-and-shift web apps, and Site-to-Site VPN for agencies"
   ],
   correct: 0,
   explanation: {
       correct: "Migration Hub Orchestra provides centralized coordination across diverse workloads, Mainframe Modernization handles AS/400 with RPG properly, AWS Batch with HPC optimizes FORTRAN performance while maintaining algorithms, Connect modernizes call center with built-in compliance, S3 Intelligent-Tiering manages 500TB cost-effectively while Textract extracts searchable text, and App2Container accelerates modern app migration - all maintaining ISO 27001.",
       whyWrong: {
           1: "Lift-and-shift AS/400 to EC2 lacks proper RPG runtime support, and Glacier makes documents inaccessible for daily operations",
           2: "Complete rewrite in 12 months is impossible for 150 years of systems, and loses critical business logic in legacy code",
           3: "Outposts for AS/400 doesn't reduce data center footprint, and keeping FORTRAN on-premises doesn't achieve cloud benefits"
       },
       examStrategy: "For complex legacy migrations: Migration Hub Orchestra coordinates diverse workloads. Mainframe Modernization handles AS/400. Preserve critical algorithms (FORTRAN) while modernizing infrastructure."
   }
},
{
   id: 'sap_073',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A multinational energy company operates wind farms, solar plants, and traditional power stations across 25 countries. Each facility has its own AWS account for operational technology (OT) systems. The company needs unified energy production monitoring, predictive maintenance across all assets, compliance with different national grid regulations, real-time energy trading capabilities, and carbon footprint tracking. The solution must handle 10 million IoT sensors and integrate with legacy SCADA systems.",
   question: "Which architecture BEST provides unified operations while respecting OT/IT segregation and regulations?",
   options: [
       "AWS IoT SiteWise for industrial data aggregation with edge gateways, AWS IoT TwinMaker for unified asset visualization, Amazon Timestream for time-series storage, SageMaker for predictive maintenance models, AWS Data Exchange for energy market integration, and AWS Organizations with SCPs for regulatory compliance per country",
       "AWS IoT Core for sensor management, Amazon MSK for data streaming, S3 Data Lake with Glue, Redshift for analytics, QuickSight for dashboards, and Control Tower for governance",
       "AWS IoT Greengrass for edge computing, Kinesis Data Streams for ingestion, DynamoDB for real-time data, EMR for big data processing, Forecast for predictions, and CloudFormation StackSets for multi-account deployment",
       "AWS Outposts at each facility, Direct Connect network mesh, centralized EKS cluster, Prometheus for monitoring, Grafana for visualization, and Lambda for trading automation"
   ],
   correct: 0,
   explanation: {
       correct: "IoT SiteWise is purpose-built for industrial OT data with SCADA integration via OPC-UA, TwinMaker provides digital twins for unified visualization across facilities, Timestream efficiently handles 10M sensors' time-series data with automatic tiering, SageMaker enables predictive maintenance models, Data Exchange provides real-time market data for energy trading, and Organizations SCPs enforce country-specific grid regulations while maintaining OT/IT segregation.",
       whyWrong: {
           1: "Generic IoT Core lacks industrial protocols for SCADA, and MSK doesn't provide OT-specific features like asset modeling",
           2: "Greengrass alone doesn't provide asset hierarchies needed for energy facilities, and Forecast is for business metrics not industrial predictive maintenance",
           3: "Outposts at 25 countries' facilities is extremely expensive, and Prometheus/Grafana lacks industrial data modeling capabilities"
       },
       examStrategy: "For industrial IoT/OT: IoT SiteWise is the key service for SCADA integration. IoT TwinMaker provides digital twin visualization. Data Exchange enables real-time market data integration."
   }
},
{
   id: 'sap_074',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A quantum computing research consortium is building a hybrid classical-quantum computing platform. Requirements: manage quantum circuit execution across multiple quantum hardware providers, classical pre/post-processing requiring 1000+ GPU nodes, store and analyze 100TB of quantum experiment results daily, provide researcher collaboration tools for 10,000 scientists globally, maintain quantum algorithm IP protection, and achieve <100ms latency between classical and quantum processing.",
   question: "Which architecture BEST supports this cutting-edge hybrid quantum-classical platform?",
   options: [
       "Amazon Braket for quantum computing with multiple QPU backends, AWS ParallelCluster with P4d instances for GPU computing, Amazon FSx for Lustre for high-performance storage, AWS CodeCommit with fine-grained permissions for IP protection, SageMaker notebooks for collaboration, and AWS Direct Connect to quantum facilities",
       "EC2 with NVIDIA GPUs for classical compute, API Gateway to quantum providers, S3 for data storage, GitHub Enterprise for code, Jupyter Hub on EKS for collaboration, and VPN connections to quantum centers",
       "AWS Batch with GPU compute environments, Lambda for quantum job orchestration, DynamoDB for experiment metadata, EFS for shared storage, Cloud9 for development, and Transit Gateway for network connectivity",
       "ECS with GPU tasks, Step Functions for workflow orchestration, Redshift for analytics, CodeArtifact for algorithm sharing, WorkDocs for collaboration, and PrivateLink to quantum services"
   ],
   correct: 0,
   explanation: {
       correct: "Braket provides native integration with multiple quantum hardware providers (IBM, Rigetti, IonQ) with optimized classical-quantum interaction, ParallelCluster with P4d delivers massive GPU scale needed for pre/post-processing, FSx for Lustre handles 100TB daily with sub-millisecond latency required for <100ms processing, CodeCommit with IAM provides fine-grained IP protection, SageMaker notebooks enable secure collaborative research, and Direct Connect ensures reliable low-latency connectivity to quantum facilities.",
       whyWrong: {
           1: "Building custom API integration to quantum providers adds complexity and latency vs native Braket service, and GitHub Enterprise lacks fine-grained AWS IAM integration",
           2: "Batch doesn't provide the HPC cluster features needed for tightly-coupled GPU workloads, and DynamoDB isn't suitable for 100TB daily scientific data",
           3: "ECS doesn't provide HPC scheduling needed for quantum workflows, and WorkDocs isn't designed for computational research collaboration"
       },
       examStrategy: "Amazon Braket is AWS's quantum computing service - know it exists. ParallelCluster is for HPC workloads. FSx for Lustre is the go-to for scientific computing storage."
   }
},
{
   id: 'sap_075',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "An e-learning platform with 20 million students runs on: 200 EC2 instances, RDS PostgreSQL Multi-AZ, 50TB of video content in S3, and CloudFront. Problems: PostgreSQL vacuum operations cause 30-minute outages during exams, S3 costs $40,000/month for rarely-accessed old courses, CloudFront invalidation takes 20 minutes affecting content updates, EC2 scaling during semester start causes 5-minute delays, and real-time collaboration features have 500ms latency.",
   question: "Which optimization strategy MOST comprehensively addresses all performance and cost issues?",
   options: [
       "Migrate to Aurora PostgreSQL with vacuum processing offload, implement S3 Intelligent-Tiering for automatic storage optimization, use CloudFront versioned URLs instead of invalidations, implement EC2 warm pools for faster scaling, and add AWS AppSync for real-time collaboration with WebSocket subscriptions",
       "Add read replicas for vacuum isolation, manually move old content to Glacier, increase CloudFront cache TTLs, use larger EC2 instances to avoid scaling, and implement WebSockets on ALB",
       "Switch to DynamoDB for zero-maintenance, use S3 lifecycle policies with fixed rules, implement Lambda@Edge for dynamic content, add more EC2 instances preemptively, and use ElastiCache for real-time features",
       "Implement Amazon RDS Proxy, use S3 Storage Lens for optimization recommendations, add CloudFront behaviors for different content types, use Spot instances for cost savings, and implement Server-Sent Events for real-time"
   ],
   correct: 0,
   explanation: {
       correct: "Aurora PostgreSQL handles vacuum operations without downtime through storage layer optimization, S3 Intelligent-Tiering automatically moves objects between tiers saving 60%+ on storage, versioned URLs eliminate invalidation delays enabling instant updates, warm pools maintain pre-initialized instances reducing scaling time to seconds, and AppSync provides managed GraphQL with WebSocket subscriptions achieving <100ms latency for collaboration.",
       whyWrong: {
           1: "Read replicas don't solve vacuum locking on primary, manual Glacier movement is error-prone, and WebSockets on ALB requires complex session affinity",
           2: "DynamoDB migration requires complete application rewrite, fixed lifecycle rules don't adapt to access patterns, and preemptive scaling wastes money",
           3: "RDS Proxy doesn't solve vacuum issues, Storage Lens only provides recommendations not automation, and Server-Sent Events are one-way only"
       },
       examStrategy: "Aurora PostgreSQL solves many RDS operational issues including vacuum. S3 Intelligent-Tiering is superior to manual lifecycle rules. Warm pools are key for predictable scaling events."
   }
},
{
   id: 'sap_076',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A 200-hospital healthcare network is migrating to AWS with: 500 clinical applications, 10PB of medical imaging, Epic EHR system, real-time HL7 interfaces to 5000 medical devices, on-premises Active Directory with 100,000 users, and legacy Visual Basic applications. Requirements: HIPAA compliance, 99.99% uptime for critical systems, maintain all integrations during migration, complete in 18 months, and enable AI-driven diagnostics post-migration.",
   question: "Which migration strategy ensures healthcare continuity while enabling innovation?",
   options: [
       "AWS HealthLake for clinical data aggregation with FHIR APIs, Application Migration Service for bulk application migration, DataSync with ongoing sync for PACS imaging, AWS HealthImaging for DICOM storage and AI/ML, AD Connector for authentication continuity, App2Container for VB apps, and AWS Batch for HL7 processing",
       "Lift-and-shift everything to EC2, implement Direct Connect for reliability, use Storage Gateway for imaging, keep Active Directory on-premises, manually modernize VB apps, and build custom HL7 interfaces",
       "Complete cloud-native rebuild with microservices, replace Epic with custom solution, implement Cognito for authentication, store images in S3, use Lambda for all processing, and API Gateway for interfaces",
       "VMware Cloud on AWS for minimal change, AWS Backup for data protection, FSx for Windows for file shares, migrate AD to AWS Managed AD, keep legacy apps unchanged, and use Kinesis for HL7 streaming"
   ],
   correct: 0,
   explanation: {
       correct: "HealthLake aggregates clinical data with FHIR standards enabling AI while maintaining Epic, MGN rapidly migrates 500 applications meeting 18-month timeline, DataSync maintains imaging availability during migration, HealthImaging provides DICOM-native storage with ML integration, AD Connector maintains authentication without disruption, App2Container modernizes VB apps without rewriting, and Batch processes HL7 messages reliably at scale.",
       whyWrong: {
           1: "Pure lift-and-shift doesn't enable AI diagnostics goal, and custom HL7 interfaces are risky for critical medical device integrations",
           2: "Complete rebuild of 500 apps in 18 months is impossible, and replacing Epic would face massive clinical resistance and risk",
           3: "VMware Cloud is expensive at this scale, and unchanged legacy apps prevent innovation goals"
       },
       examStrategy: "For healthcare migrations: HealthLake and HealthImaging are purpose-built services. Maintain critical systems (Epic, AD) during migration. App2Container modernizes legacy apps without rewriting."
   }
},
{
   id: 'sap_077',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global automotive manufacturer has 200 AWS accounts across design, manufacturing, supply chain, and dealers. New requirements: implement zero-trust network architecture, enable secure data sharing with 10,000 suppliers, comply with TISAX Level 3 for automotive security, provide isolated development environments for each of 500 engineering teams, centralize threat detection across OT and IT systems, and maintain less than 1-hour recovery time for ransomware attacks.",
   question: "Which security architecture MOST comprehensively addresses these automotive industry requirements?",
   options: [
       "AWS Verified Access for zero-trust application access, AWS Clean Rooms for secure supplier data collaboration, AWS Audit Manager with TISAX compliance framework, AWS Service Catalog with account vending for team isolation, Amazon Security Lake for unified OT/IT threat detection, and AWS Backup with vault lock and cross-region replication for ransomware recovery",
       "AWS PrivateLink for all connections, AWS Data Exchange for supplier sharing, AWS Config for compliance, AWS Organizations for account management, Amazon GuardDuty for threat detection, and snapshots for backup",
       "Transit Gateway with security domains, S3 with bucket policies for sharing, AWS Security Hub for compliance, Control Tower for accounts, Amazon Detective for investigation, and AWS Elastic Disaster Recovery",
       "AWS Network Firewall everywhere, Direct Connect with partners, Macie for data protection, SSO for access management, CloudWatch for monitoring, and AMI backups for recovery"
   ],
   correct: 0,
   explanation: {
       correct: "Verified Access provides true zero-trust without VPN complexity, Clean Rooms enables secure multi-party computation without raw data exposure perfect for supplier collaboration, Audit Manager has pre-built TISAX framework for automotive compliance, Service Catalog with account vending provides automated secure isolation for 500 teams, Security Lake aggregates both OT and IT security data for unified threat detection, and Backup with vault lock provides immutable backups with <1 hour recovery from ransomware.",
       whyWrong: {
           1: "PrivateLink alone doesn't provide zero-trust architecture, and Data Exchange doesn't provide the privacy-preserving compute of Clean Rooms needed for sensitive automotive data",
           2: "Transit Gateway is network-level not application-level zero-trust, and Elastic Disaster Recovery is for full DR not rapid ransomware recovery",
           3: "Network Firewall is perimeter-based not zero-trust, and Direct Connect with 10,000 suppliers is impractical and expensive"
       },
       examStrategy: "AWS Verified Access is the zero-trust service. Clean Rooms enables secure multi-party analytics. Security Lake unifies security data. Know industry-specific compliance (TISAX for automotive)."
   }
},
{
   id: 'sap_078',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A space technology company is building a satellite constellation management platform. Requirements: track 500 satellites in real-time with orbital mechanics calculations, process 1TB of telemetry per satellite daily, command uplink with <500ms latency globally, collision avoidance predictions 7 days ahead, integration with ground station networks, store 10 years of mission data for analysis, and support mission planning simulations using quantum computing.",
   question: "Which architecture BEST supports these aerospace mission-critical requirements?",
   options: [
       "AWS Ground Station for satellite communication, Amazon Timestream for telemetry ingestion with 10-year retention, AWS Batch with C6i instances for orbital calculations, Amazon Forecast with custom algorithms for collision prediction, AWS IoT Core for command distribution, Amazon Braket for quantum simulations, and S3 Intelligent-Tiering for mission archives",
       "Direct ground station integration, Kinesis Data Streams for telemetry, Lambda for calculations, SageMaker for predictions, API Gateway for commands, EC2 for simulations, and Glacier for archives",
       "Satellite modems on EC2, MSK for data streaming, EMR for processing, Neptune for orbital graphs, AppSync for command interface, HPC cluster for simulations, and EFS for storage",
       "Custom ground software, DynamoDB for telemetry, Fargate for calculations, Comprehend for log analysis, Step Functions for commands, Quantum Ledger Database for simulations, and Redshift for analytics"
   ],
   correct: 0,
   explanation: {
       correct: "Ground Station provides managed satellite communication with global coverage and <500ms latency, Timestream efficiently handles massive telemetry with automatic hot/cold tiering for 10-year retention, Batch with C6i provides the compute power needed for complex orbital mechanics, Forecast with custom algorithms enables 7-day collision predictions, IoT Core reliably distributes commands globally, Braket enables quantum computing for advanced mission planning, and S3 Intelligent-Tiering optimizes long-term storage costs.",
       whyWrong: {
           1: "Building ground station integration is complex and lacks global coverage, and Lambda has timeout limitations for complex orbital calculations",
           2: "EC2-based ground stations lack redundancy and global reach, and Neptune is overkill for orbital calculations vs purpose-built physics simulations",
           3: "QLDB is a ledger database not for quantum computing, and Comprehend is for NLP not telemetry analysis"
       },
       examStrategy: "AWS Ground Station is the satellite communication service. Timestream excels at high-volume time-series data with long retention. Braket provides quantum computing capabilities."
   }
},
{
   id: 'sap_079',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "medium",
   timeRecommendation: 150,
   scenario: "A social media application uses DynamoDB (on-demand mode) for user posts, spending $80,000/month. Analysis shows: 80% of reads are for posts <24 hours old, write traffic is steady at 5,000 WCU, read traffic varies from 1,000 to 50,000 RCU, hot partition issues during viral posts, and global secondary indexes (GSI) consume 40% of costs but are rarely used. The application needs 50% cost reduction while improving performance.",
   question: "Which optimization strategy BEST reduces costs while improving performance?",
   options: [
       "Switch to provisioned capacity with auto-scaling for predictable writes, implement DynamoDB Accelerator (DAX) for recent posts caching, use contributor insights to identify and fix hot partitions, remove unused GSIs and create sparse indexes where needed, and implement S3 archival for posts >30 days with DynamoDB Streams",
       "Keep on-demand mode for flexibility, add ElastiCache for all caching, partition tables by date, create local secondary indexes instead of GSIs, and move old data to RDS",
       "Switch everything to provisioned with reserved capacity, implement application-level caching, use scan operations instead of GSIs, increase partition keys, and compress all items",
       "Use provisioned for writes and on-demand for reads, add CloudFront for API caching, create composite keys, duplicate data to avoid GSIs, and implement pagination"
   ],
   correct: 0,
   explanation: {
       correct: "Provisioned capacity with auto-scaling reduces costs 65% for predictable writes while maintaining flexibility for reads, DAX provides microsecond latency for hot data reducing read costs, contributor insights identifies hot partitions for targeted fixes, removing unused GSIs saves 40% immediately, sparse indexes reduce costs further, and S3 archival via Streams moves cold data to cheaper storage while maintaining query ability.",
       whyWrong: {
           1: "Keeping on-demand for steady writes wastes money, and moving to RDS breaks DynamoDB-optimized access patterns",
           2: "Scan operations are extremely expensive compared to GSI queries, and reserved capacity reduces flexibility needed for variable reads",
           3: "Mixed provisioned/on-demand isn't supported on same table, and data duplication increases storage costs and consistency complexity"
       },
       examStrategy: "For DynamoDB optimization: DAX for caching hot data, contributor insights for hot partition diagnosis, sparse indexes for cost reduction. Provisioned capacity saves significantly for predictable workloads."
   }
},
{
   id: 'sap_080',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A stock exchange is migrating its trading platform to AWS. The platform includes: ultra-low latency matching engine requiring <10 microsecond response, market data distribution to 50,000 subscribers, regulatory reporting system with 7-year audit trail, risk management requiring real-time position calculations, legacy FIX protocol gateways, and disaster recovery with <1 minute RTO. The migration cannot impact trading hours and must maintain FINRA compliance.",
   question: "Which migration approach ensures zero trading disruption while meeting extreme performance requirements?",
   options: [
       "AWS Local Zones for ultra-low latency with Nitro System enhanced networking, Amazon Managed Streaming for Kafka (MSK) for market data distribution, Amazon QLDB for immutable audit trail, Amazon MemoryDB for real-time risk calculations, AWS Mainframe Modernization for FIX protocol handling, and Aurora Global Database with write forwarding for <1 minute RTO",
       "AWS Outposts for on-premises latency, Kinesis Data Streams for market data, S3 with Object Lock for audit trail, ElastiCache for risk calculations, Direct Connect for FIX connectivity, and Multi-Region active-active for DR",
       "EC2 bare metal with SR-IOV, custom multicast for market data, DynamoDB for audit storage, Lambda for risk calculations, API Gateway for FIX translation, and pilot light DR strategy",
       "Wavelength for edge computing, EventBridge for market data, CloudTrail Lake for audit, SageMaker for risk models, App Mesh for FIX routing, and backup/restore for DR"
   ],
   correct: 0,
   explanation: {
       correct: "Local Zones provide metro-edge computing with single-digit microsecond latency via Nitro enhanced networking meeting <10μs requirement, MSK handles 50,000 subscribers with proven financial services scale, QLDB provides cryptographically verifiable immutable audit trail for FINRA compliance, MemoryDB offers microsecond latency with durability for risk calculations, Mainframe Modernization handles legacy FIX protocol without disruption, and Aurora Global write forwarding achieves <1 minute RTO with automatic failover.",
       whyWrong: {
           1: "Outposts still has AWS service latency overhead, and Kinesis has higher latency than MSK for financial markets",
           2: "Custom multicast is complex and risky for production trading, and Lambda cold starts conflict with microsecond requirements",
           3: "Wavelength is for mobile edge not financial trading, and backup/restore cannot achieve <1 minute RTO"
       },
       examStrategy: "For ultra-low latency financial systems: Local Zones provide metro-edge computing. MSK is preferred over Kinesis for financial services. QLDB provides immutable audit trails with cryptographic verification."
   }
},

// Continuing questions 81-90...
{
   id: 'sap_081',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global consulting firm with 100,000 employees has 500 AWS accounts across client projects, internal systems, and R&D. Requirements include: complete client data isolation with proof of segregation for audits, centralized billing with chargeback to 2,000 cost centers, prevent consultants from accessing multiple client environments, automated decommissioning when projects end, compliance with SOC2, ISO 27001, and various client-specific standards, and unified security monitoring without data commingling.",
   question: "Which architecture BEST ensures client isolation while enabling operational efficiency?",
   options: [
       "AWS Control Tower with customized Account Factory for client-specific accounts, AWS Organizations with SCPs enforcing data boundaries, AWS IAM Identity Center with permission sets per client and automatic deprovisioning, AWS Cost Categories for chargeback allocation, AWS Audit Manager for multi-framework compliance, and Amazon Security Lake with delegated admin per client segment",
       "AWS Organizations with separate OUs per client, AWS SSO with Active Directory integration, manual account creation process, Cost Explorer with tags, AWS Config for compliance, and centralized CloudTrail logging",
       "AWS Landing Zone for account vending, IAM roles with external IDs, consolidated billing with manual allocation, Security Hub for compliance, GuardDuty for threat detection, and separate log archives per client",
       "Multiple AWS Organizations for client separation, SAML federation for access, spreadsheet-based cost tracking, third-party SIEM for security, manual compliance audits, and S3 buckets for logs"
   ],
   correct: 0,
   explanation: {
       correct: "Control Tower with customized Account Factory automates compliant account provisioning with built-in isolation, SCPs provide preventive controls ensuring data boundaries that satisfy audit requirements, IAM Identity Center permission sets with automatic lifecycle management prevents cross-client access and handles deprovisioning, Cost Categories enable precise chargeback to 2,000 cost centers, Audit Manager provides evidence collection across multiple compliance frameworks, and Security Lake with delegated admin maintains security visibility while preserving data isolation.",
       whyWrong: {
           1: "Manual account creation doesn't scale for project velocity, and centralized CloudTrail violates client data isolation requirements",
           2: "Landing Zone is deprecated, and separate log archives increase operational complexity without improving isolation",
           3: "Multiple Organizations breaks centralized billing and increases management overhead, and spreadsheet tracking is error-prone for 2,000 cost centers"
       },
       examStrategy: "For complex multi-tenant scenarios: Control Tower provides governance at scale. Cost Categories enable sophisticated chargeback. Security Lake with delegated admin maintains isolation while enabling monitoring."
   }
},
{
   id: 'sap_082',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A defense contractor is building a battlefield simulation platform for military training. Requirements: simulate 100,000 concurrent entities (vehicles, soldiers, drones), real-time physics with terrain deformation, multi-spectral sensor simulation (visual, IR, radar), encrypted communications with classification levels, support for global exercises with 10,000 participants, after-action review with complete replay capability, and operation in disconnected/denied environments.",
   question: "Which architecture BEST supports military-grade simulation requirements?",
   options: [
       "AWS GovCloud with IL5 compliance, Amazon EC2 P4d instances for GPU simulation, AWS ParallelCluster for distributed physics, Amazon Timestream for entity state tracking, AWS KMS with CloudHSM for encryption, Amazon Kinesis Video Streams for replay capability, and AWS Snowball Edge for disconnected operations with AWS IoT Greengrass",
       "Standard AWS with compliance programs, Lambda for entity logic, DynamoDB for state management, SageMaker for physics, Secrets Manager for encryption, S3 for replay storage, and Direct Connect for reliability",
       "AWS Outposts for on-premise deployment, ECS for containerized simulation, RDS for entity tracking, Nitro Enclaves for encryption, ElastiCache for state synchronization, and EBS snapshots for replay",
       "EC2 with placement groups, Step Functions for simulation orchestration, Neptune for entity relationships, Parameter Store for secrets, CloudWatch for replay, and VPN for connectivity"
   ],
   correct: 0,
   explanation: {
       correct: "GovCloud provides IL5 compliance required for defense, P4d instances deliver massive GPU power for physics and sensor simulation, ParallelCluster enables distributed computing for 100,000 entities, Timestream efficiently tracks temporal entity states, CloudHSM provides FIPS 140-2 Level 3 encryption for classification levels, Kinesis Video Streams captures complete simulation for replay, and Snowball Edge with Greengrass enables disconnected training operations in denied environments.",
       whyWrong: {
           1: "Lambda lacks persistent state for complex simulations, and standard AWS may not meet defense compliance requirements",
           2: "Outposts requires connectivity defeating disconnected operation requirement, and EBS snapshots don't capture real-time simulation flow",
           3: "Step Functions isn't designed for real-time physics simulation, and VPN doesn't support disconnected operations"
       },
       examStrategy: "For defense/military workloads: GovCloud for compliance, CloudHSM for encryption requirements, Snowball Edge enables disconnected operations. ParallelCluster for HPC simulation workloads."
   }
},
{
   id: 'sap_083',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A ride-sharing platform processes 500,000 rides hourly across 200 cities. Current architecture: monolithic Node.js application on 300 t3.xlarge instances, PostgreSQL RDS with 15 read replicas, Redis for real-time driver locations, and REST APIs. Issues: 30-second driver assignment delays during rush hours, database CPU at 95%, $400,000 monthly costs, inability to implement surge pricing in real-time, and 10% timeout errors on payment processing.",
   question: "Which modernization approach BEST resolves all issues while enabling real-time features?",
   options: [
       "Decompose into microservices on EKS with KEDA autoscaling, implement Amazon MemoryDB for driver location with geo-queries, migrate to Aurora Serverless v2 for automatic scaling, use Amazon EventBridge for event-driven surge pricing, implement AWS Step Functions for payment orchestration with error handling, and GraphQL with AWS AppSync for efficient mobile APIs",
       "Scale to 600 instances, add more RDS replicas, implement application-level caching, add Kinesis for real-time processing, use Lambda for surge calculations, and implement circuit breakers",
       "Migrate to Lambda for serverless, DynamoDB for NoSQL performance, ElastiCache for all caching, API Gateway for APIs, SQS for payment queue, and CloudWatch for monitoring",
       "Containerize on Fargate, implement service mesh with App Mesh, use DocumentDB for flexibility, add Kafka for streaming, implement Cognito for authentication, and X-Ray for tracing"
   ],
   correct: 0,
   explanation: {
       correct: "EKS with KEDA provides Kubernetes Event-Driven Autoscaling ideal for traffic patterns, MemoryDB offers Redis compatibility with built-in geo-queries reducing driver assignment to <3 seconds, Aurora Serverless v2 eliminates database bottlenecks with automatic scaling, EventBridge enables real-time event-driven surge pricing, Step Functions handles payment complexity with built-in error handling reducing timeouts to near-zero, and AppSync provides efficient GraphQL reducing mobile bandwidth 60% and costs.",
       whyWrong: {
           1: "Simply adding more resources doesn't solve architectural issues and increases costs further without improving features",
           2: "Full serverless migration is risky for complex stateful ride-sharing logic, and DynamoDB requires complete data model redesign",
           3: "DocumentDB doesn't solve PostgreSQL performance issues, and adding service mesh increases complexity without addressing core problems"
       },
       examStrategy: "For modernization: KEDA enables advanced Kubernetes autoscaling. MemoryDB provides durable Redis with geo-queries. EventBridge excels at event-driven architectures. Step Functions handles complex orchestration."
   }
},
{
   id: 'sap_084',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A national tax authority must migrate its citizen services platform from mainframe to AWS before the next tax season (6 months). The system processes 50 million tax returns annually, integrates with 200 government agencies, stores 40 years of historical data (2PB), runs 10,000 COBOL programs, peaks at 1 million concurrent users on deadline day, and must maintain legal compliance for data residency and audit trails.",
   question: "Which migration strategy ensures tax season readiness while maintaining compliance?",
   options: [
       "AWS Mainframe Modernization with Micro Focus for rapid rehosting, implement Amazon API Gateway with caching for citizen portal, use AWS B2B Data Interchange for agency integrations, migrate historical data to S3 with Glacier Deep Archive for old records, implement Auto Scaling with predictive scaling for deadline day, and use AWS CloudTrail Lake for permanent audit trails",
       "Rewrite everything in microservices on Lambda, implement DynamoDB for citizen data, use EventBridge for agency communication, store all data in S3, handle peaks with reserved concurrency, and use CloudWatch Logs for auditing",
       "Hybrid approach with mainframe on Outposts, new citizen portal on ECS, keep existing integrations, replicate data to RDS, use Spot instances for peak capacity, and implement custom audit solution",
       "Lift-and-shift COBOL to EC2, build new React frontend, implement REST APIs for agencies, migrate to PostgreSQL, use CloudFront for scaling, and store audits in DynamoDB"
   ],
   correct: 0,
   explanation: {
       correct: "Mainframe Modernization with Micro Focus enables rapid rehosting meeting 6-month deadline while preserving COBOL logic, API Gateway with caching handles citizen portal traffic efficiently, B2B Data Interchange provides managed EDI/file transfer for government integrations, S3 with Glacier Deep Archive cost-effectively stores 40 years of data, predictive scaling anticipates deadline day surge preventing outages, and CloudTrail Lake provides immutable audit trails meeting legal requirements.",
       whyWrong: {
           1: "Rewriting 10,000 COBOL programs in 6 months is impossible, and Lambda has complexity limits for tax calculations",
           2: "Outposts for mainframe adds cost and complexity, and Spot instances are inappropriate for critical tax deadline processing",
           3: "Lift-and-shift doesn't leverage cloud benefits, and PostgreSQL migration requires extensive data transformation for mainframe data"
       },
       examStrategy: "For urgent mainframe migrations: rehosting is fastest, modernization comes later. B2B Data Interchange handles EDI/government integrations. CloudTrail Lake provides compliant audit storage."
   }
},
{
   id: 'sap_085',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A Fortune 500 company completed 20 acquisitions creating 300 AWS accounts with different naming conventions, tagging strategies, network architectures, and security tools. The CFO demands consolidated reporting, the CISO requires unified security posture, DevOps teams need self-service capabilities, and the board wants 30% cost reduction. Integration must complete in 90 days without disrupting operations.",
   question: "Which approach BEST achieves rapid integration while meeting all stakeholder requirements?",
   options: [
       "Implement AWS Control Tower Account Factory for Terraform (AFT) to standardize existing accounts, use AWS Organizations Resource Control Policies for tag enforcement, deploy AWS Transit Gateway for network consolidation, enable AWS Security Hub with automated remediation, implement AWS Service Catalog for self-service, and use Cost Intelligence Dashboard with AWS Compute Optimizer for cost reduction",
       "Manually reorganize accounts into OUs, create tagging policies, peer VPCs individually, deploy GuardDuty in each account, build custom portal for self-service, and use spreadsheets for cost tracking",
       "Create new Control Tower organization and migrate all accounts, enforce strict naming convention, rebuild all networks, centralize all security tools, restrict all self-service during transition, and implement FinOps team",
       "Keep existing structure, use Cost Explorer for reporting, add Security Hub without remediation, maintain separate networks, allow teams to continue current practices, and negotiate Enterprise Discount Program"
   ],
   correct: 0,
   explanation: {
       correct: "Control Tower AFT standardizes accounts without recreation meeting 90-day timeline, Resource Control Policies enforce tagging retroactively enabling CFO reporting, Transit Gateway consolidates networks without rebuilding, Security Hub with automation provides CISO's unified security posture, Service Catalog maintains DevOps self-service while adding governance, and Cost Intelligence Dashboard with Compute Optimizer identifies 30%+ savings opportunities through rightsizing and modernization.",
       whyWrong: {
           1: "Manual processes won't complete in 90 days for 300 accounts, and spreadsheet cost tracking lacks real-time visibility",
           2: "Migrating 300 accounts would be massively disruptive and exceed 90-day timeline",
           3: "Maintaining status quo doesn't achieve consolidation goals or cost reduction targets"
       },
       examStrategy: "For M&A integration: AFT enables Control Tower standardization without migration. Resource Control Policies retroactively enforce governance. Cost Intelligence Dashboard provides FinOps visibility."
   }
},
{
   id: 'sap_086',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "An agricultural technology company is building a precision farming platform managing 10 million acres globally. Requirements: process satellite imagery for crop health (100TB daily), IoT sensors for soil moisture/weather (50M data points/hour), drone video for pest detection, ML models for yield prediction, real-time irrigation control, integration with farm equipment (John Deere, etc.), and offline operation for rural areas with satellite internet.",
   question: "Which architecture BEST supports global precision agriculture at scale?",
   options: [
       "Amazon SageMaker Geospatial for satellite imagery ML analysis, AWS IoT Core with Basic Ingest for sensor data, Amazon Rekognition Custom Labels for pest detection in drone footage, AWS IoT FleetWise for equipment integration, Amazon Forecast for yield prediction, AWS IoT Greengrass for edge irrigation control, and AWS Ground Station with Snowball Edge for offline sync",
       "EC2 with GPU for image processing, Kinesis for IoT ingestion, Lambda for ML inference, custom APIs for equipment, Redshift for analytics, Direct Connect for reliability, and manual offline handling",
       "S3 for imagery storage, DynamoDB for sensor data, Batch for processing, generic ML on SageMaker, RDS for farm data, API Gateway for integrations, and cache data locally",
       "ECS for containerized processing, Timestream for sensors, Comprehend for analysis, QuickSight for visualization, AppSync for APIs, EventBridge for automation, and replicate to on-premise"
   ],
   correct: 0,
   explanation: {
       correct: "SageMaker Geospatial is purpose-built for satellite imagery analysis with agricultural ML models, IoT Core Basic Ingest reduces costs for 50M points/hour, Rekognition Custom Labels enables training specific pest detection models, IoT FleetWise handles proprietary agricultural equipment protocols, Forecast provides time-series yield prediction, Greengrass enables local irrigation control during connectivity loss, and Ground Station with Snowball Edge provides satellite downlink with offline operation for rural deployments.",
       whyWrong: {
           1: "Generic EC2 GPU lacks geospatial ML capabilities, and manual offline handling doesn't scale across millions of acres",
           2: "Generic SageMaker lacks geospatial features, and RDS isn't optimal for IoT time-series data at this scale",
           3: "Comprehend is for text not agricultural analysis, and on-premise replication defeats cloud advantages"
       },
       examStrategy: "SageMaker Geospatial is specialized for satellite imagery ML. IoT FleetWise handles proprietary protocols. Greengrass enables edge computing for offline operations. Ground Station provides satellite connectivity."
   }
},
{
   id: 'sap_087',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A cryptocurrency exchange handles $10B daily volume with infrastructure on AWS: 1000 c5.24xlarge instances for matching engine, DynamoDB for order book (costing $200K/month), PostgreSQL RDS for user accounts, Redis cluster for market data, and multi-region deployment. Issues: 15ms latency between regions affecting arbitrage, DynamoDB costs growing 20% monthly, 5-minute failover time, and inability to handle flash crashes with 100x normal volume.",
   question: "Which optimization strategy BEST addresses latency, costs, and reliability for financial markets?",
   options: [
       "Deploy AWS Local Zones in financial centers for <1ms latency, implement DynamoDB on-demand with reserved capacity for baseline load, use Aurora Global Database with write forwarding for <1 second failover, implement AWS Application Auto Scaling with custom CloudWatch metrics for flash crash handling, and Amazon MemoryDB replacing Redis for durability with performance",
       "Add more regions for proximity, use DynamoDB auto-scaling, implement read replicas, upgrade Redis, and add more EC2 instances for peak capacity",
       "Implement AWS Global Accelerator, partition DynamoDB by region, use Multi-AZ RDS, implement ElastiCache, and use Spot instances for cost savings",
       "Use CloudFront for API acceleration, compress DynamoDB items, implement connection pooling, cache everything in Redis, and implement rate limiting"
   ],
   correct: 0,
   explanation: {
       correct: "Local Zones in financial centers (NYC, London, Tokyo) provide <1ms latency critical for arbitrage, DynamoDB reserved capacity reduces costs 75% for predictable baseline while on-demand handles spikes, Aurora Global write forwarding achieves sub-second failover meeting exchange requirements, custom CloudWatch metrics with Application Auto Scaling handles 100x flash crash volumes, and MemoryDB provides Redis performance with Multi-AZ durability preventing data loss during failures.",
       whyWrong: {
           1: "Simply adding infrastructure doesn't solve architectural latency issues and increases costs without addressing root causes",
           2: "Global Accelerator doesn't reduce compute latency, and Spot instances are inappropriate for critical trading infrastructure",
           3: "CloudFront adds latency for real-time trading, and rate limiting during flash crashes could cause market manipulation issues"
       },
       examStrategy: "For financial markets: Local Zones provide ultra-low latency in financial centers. Aurora Global write forwarding enables true active-active. MemoryDB combines Redis performance with durability."
   }
},
{
   id: 'sap_088',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A 100-year-old insurance company is migrating from multiple data centers including: IBM z/OS mainframe with IMS database, AS/400 systems for claims, Sun SPARC servers running Solaris, Windows servers with .NET applications, 5PB of documents in EMC storage, and Citrix VDI for 20,000 users. Migration must maintain business continuity, complete in 18 months, reduce costs 40%, and enable modern digital services.",
   question: "Which comprehensive migration strategy addresses this heterogeneous environment?",
   options: [
       "AWS Mainframe Modernization Service for z/OS and AS/400 systems using dual-run approach, AWS Application Migration Service for SPARC/Solaris/Windows servers, AWS DataSync with File Gateway for EMC migration maintaining CIFS/NFS access, Amazon WorkSpaces replacing Citrix VDI, AWS Database Migration Service for IMS to Aurora PostgreSQL, and Savings Plans achieving 40% cost reduction",
       "Lift-and-shift everything to EC2, keep all databases as-is, use Storage Gateway for files, maintain Citrix on AWS, implement Direct Connect, and negotiate volume discounts",
       "Rewrite all legacy systems in cloud-native, replace mainframe with microservices, migrate to NoSQL databases, implement S3 for all storage, use AppStream for VDI, and go serverless",
       "Keep mainframe on-premises with Outposts, migrate only Windows to AWS, use hybrid storage, maintain Citrix on-premises, implement VPN connectivity, and partial cloud adoption"
   ],
   correct: 0,
   explanation: {
       correct: "Mainframe Modernization handles both z/OS and AS/400 with dual-run ensuring continuity, MGN seamlessly migrates SPARC/Solaris/Windows preserving configurations, DataSync with File Gateway maintains file protocol compatibility during 5PB migration, WorkSpaces provides superior VDI experience vs Citrix at lower cost, DMS with SCT converts IMS hierarchical to PostgreSQL relational, and Savings Plans deliver 40% savings on modernized infrastructure.",
       whyWrong: {
           1: "Pure lift-and-shift doesn't achieve 40% cost reduction and maintains legacy complexity without enabling digital services",
           2: "Complete rewrite of 100 years of systems in 18 months is impossible and extremely risky for insurance operations",
           3: "Partial migration doesn't achieve cost targets and maintains data center expenses while adding cloud costs"
       },
       examStrategy: "For heterogeneous migrations: Mainframe Modernization handles multiple legacy platforms. MGN works for various OS types. WorkSpaces is the AWS VDI solution. Combine multiple migration tools for complex environments."
   }
},
{
   id: 'sap_089',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global media conglomerate operates 50 streaming services, 100 news websites, and 500 mobile apps across 150 AWS accounts. The CEO mandates: unified customer identity across all properties, personalized content recommendations using viewing history from all services, GDPR/CCPA compliance with data deletion across all systems, centralized content moderation, and reduced authentication costs by 60%.",
   question: "Which architecture BEST provides unified identity and personalization at scale?",
   options: [
       "Amazon Cognito with user pools federation across all services for unified identity, Amazon Personalize with multi-tenant event tracking for cross-service recommendations, AWS Lake Formation with governed tables for GDPR/CCPA data lineage and deletion, Amazon Rekognition Content Moderation with centralized workflow, AWS Organizations consolidated billing revealing Cognito MAU optimization opportunities, and Amazon DataZone for privacy-compliant data sharing",
       "Build custom identity service on DynamoDB, implement Redis for session management, use SageMaker for recommendations, manual GDPR compliance, separate moderation per service, and traditional cost tracking",
       "Active Directory federation for identity, third-party personalization service, data warehouse for unified view, outsourced moderation, SAML for authentication, and manual cost analysis",
       "Okta for identity management, custom ML on EC2, S3 data lake with Glue, AI-based moderation, OAuth everywhere, and FinOps team for costs"
   ],
   correct: 0,
   explanation: {
       correct: "Cognito federation provides unified identity with built-in GDPR/CCPA features and scales to millions at low cost, Personalize with multi-tenant architecture enables cross-service recommendations while maintaining data isolation, Lake Formation governed tables track data lineage enabling compliant deletion across all systems, centralized Rekognition moderation ensures consistent standards, consolidated billing reveals per-service MAU enabling targeted optimization for 60% reduction, and DataZone enables secure data sharing for personalization while maintaining privacy compliance.",
       whyWrong: {
           1: "Custom identity service requires massive development and compliance effort, and manual GDPR across 150 accounts is error-prone",
           2: "AD doesn't scale well for consumer identity, and third-party services add cost without AWS integration benefits",
           3: "Okta is expensive at scale for consumer use cases, and custom ML lacks the managed features of Personalize"
       },
       examStrategy: "For consumer identity at scale: Cognito with federation. Personalize supports multi-tenant architectures. Lake Formation governed tables enable compliance. DataZone provides governed data sharing."
   }
},
{
   id: 'sap_090',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A renewable energy company is building a smart grid platform managing 100,000 solar installations, 50,000 battery systems, and 10,000 wind turbines. Requirements: real-time energy trading with <100ms execution, predict energy generation 48 hours ahead using weather data, optimize battery charging/discharging across the grid, detect equipment failures before they occur, integrate with 20 different utility companies' systems, and operate during internet outages.",
   question: "Which architecture BEST enables intelligent distributed energy management?",
   options: [
       "AWS IoT SiteWise Edge for distributed energy asset monitoring with local buffering, Amazon Forecast with Weather Index for 48-hour generation prediction, AWS IoT Analytics for grid optimization algorithms, Amazon Monitron for predictive maintenance on equipment, AWS B2B Data Interchange for utility company EDI integration, Amazon MemoryDB for real-time trading with <100ms latency, and AWS IoT Greengrass for autonomous edge operation during outages",
       "IoT Core for all devices, Kinesis for streaming, SageMaker for all ML, DynamoDB for trading, Lambda for optimization, API Gateway for integrations, and Direct Connect for reliability",
       "EC2 for device management, MSK for event streaming, custom weather prediction, RDS for data storage, Step Functions for workflows, REST APIs for utilities, and backup generators",
       "Timestream for telemetry, Batch for predictions, ElastiCache for trading, Glue for ETL, EventBridge for automation, AppSync for APIs, and redundant internet connections"
   ],
   correct: 0,
   explanation: {
       correct: "IoT SiteWise Edge provides industrial-grade monitoring with buffering for 100,000+ assets, Forecast with Weather Index leverages built-in weather data for accurate generation prediction, IoT Analytics enables complex grid optimization across battery systems, Monitron provides purpose-built predictive maintenance for energy equipment, B2B Data Interchange handles various utility EDI standards and protocols, MemoryDB delivers consistent <100ms trading with durability, and Greengrass enables autonomous operation during outages maintaining grid stability.",
       whyWrong: {
           1: "Generic IoT Core lacks industrial features for energy assets, and Lambda has cold start issues for real-time trading",
           2: "EC2-based management doesn't scale for 160,000 distributed assets, and custom weather prediction lacks Forecast's integrated data",
           3: "Timestream alone doesn't provide asset modeling, and redundant internet doesn't enable true offline operation"
       },
       examStrategy: "For smart grid/energy: IoT SiteWise Edge for industrial IoT, Forecast Weather Index for energy prediction, Monitron for predictive maintenance, B2B Data Interchange for utility integration."
   }
},

// Continuing questions 91-100...
{
   id: 'sap_091',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global e-commerce platform with 100M products runs on: 1000 m5.4xlarge EC2 instances, Elasticsearch cluster with 50 i3.8xlarge nodes for search, PostgreSQL RDS for inventory, Redis for cart sessions, and CloudFront. Problems: Elasticsearch costs $300K/month, search latency is 2 seconds, 30% of EC2 capacity unused overnight, cart abandonment due to session loss, and Black Friday scaling takes 30 minutes causing outages.",
   question: "Which optimization strategy provides the BEST cost reduction and performance improvement?",
   options: [
       "Migrate to Amazon OpenSearch Serverless for automatic scaling and 40% cost reduction, implement OpenSearch's Learning to Rank for ML-powered search relevance, use EC2 Auto Scaling with predictive scaling for Black Friday, replace Redis with Amazon MemoryDB for durable cart sessions, implement CloudFront Origin Shield to reduce backend load 85%, and Compute Savings Plans for baseline capacity",
       "Keep Elasticsearch but use Spot instances, add more cache layers, scale EC2 manually before events, use DynamoDB for carts, increase CloudFront TTLs, and negotiate Enterprise Agreement",
       "Move search to RDS full-text, implement application caching, use Reserved Instances, add Redis replicas, pre-scale for Black Friday, and use multiple CDNs",
       "Migrate to Solr on EC2, implement Varnish cache, use t3 instances with unlimited, store carts in PostgreSQL, add more CloudFront behaviors, and implement auto-scaling"
   ],
   correct: 0,
   explanation: {
       correct: "OpenSearch Serverless eliminates cluster management and reduces costs 40% through automatic scaling, Learning to Rank improves search relevance reducing query complexity and latency to <500ms, predictive scaling anticipates Black Friday patterns enabling proactive scaling, MemoryDB provides Redis-compatible performance with Multi-AZ durability preventing cart loss, Origin Shield reduces origin requests 85% cutting transfer costs and backend load, and Savings Plans optimize baseline costs by 27%.",
       whyWrong: {
           1: "Spot instances for search infrastructure risk availability, and manual scaling doesn't address 30-minute delay",
           2: "RDS full-text search can't handle 100M products efficiently, and pre-scaling wastes money for weeks",
           3: "Solr requires more management than OpenSearch, and t3 instances aren't suitable for consistent e-commerce workloads"
       },
       examStrategy: "OpenSearch Serverless provides managed Elasticsearch with automatic scaling. Learning to Rank is key for search relevance. MemoryDB provides durable Redis. Origin Shield dramatically reduces origin load."
   }
},
{
   id: 'sap_092',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A national postal service is migrating its logistics platform from on-premises to AWS. The system includes: Oracle RAC databases with 50TB of package tracking data, real-time integration with 100,000 handheld scanners, mainframe CICS applications for routing, SAP for finance, custom GIS system for route optimization, and 500 distribution centers with local servers. Migration must maintain 24/7 operations and integrate with international postal systems.",
   question: "Which migration approach ensures zero disruption to mail delivery operations?",
   options: [
       "AWS DMS with CDC for zero-downtime Oracle RAC migration to Amazon RDS Custom for Oracle, AWS IoT Core with Device Management for scanner fleet, AWS Mainframe Modernization with dual-run for CICS, SAP on AWS with HANA migration, Amazon Location Service replacing custom GIS, AWS Outposts for distribution centers maintaining local processing, and AWS B2B Data Interchange for international postal integration",
       "Lift-and-shift Oracle to EC2, build new scanner system, rewrite mainframe applications, keep SAP on-premises, maintain existing GIS, use Direct Connect to distribution centers, and custom APIs for integration",
       "Migrate to Aurora PostgreSQL, implement new IoT platform, containerize mainframe apps, move SAP to cloud, rebuild GIS on Lambda, centralize all processing, and use VPN for connectivity",
       "Use Oracle Cloud for databases, third-party MDM for scanners, keep mainframe as-is, hybrid SAP deployment, Google Maps for routing, edge servers at locations, and EDI for postal systems"
   ],
   correct: 0,
   explanation: {
       correct: "DMS with CDC ensures zero-downtime migration critical for 24/7 operations, RDS Custom maintains Oracle RAC features required for package tracking, IoT Core with Device Management handles 100,000 scanners at scale, dual-run Mainframe Modernization ensures routing continuity, SAP on AWS with automated HANA migration preserves finance operations, Location Service provides managed GIS with routing APIs, Outposts maintains local processing for distribution centers during network outages, and B2B Data Interchange handles various international postal EDI standards.",
       whyWrong: {
           1: "Lift-and-shift doesn't modernize for cloud benefits, and building new scanner system during operations risks package tracking",
           2: "Aurora PostgreSQL requires significant Oracle conversion affecting 24/7 operations, and centralizing removes local resilience",
           3: "Multi-cloud approach adds complexity, and keeping mainframe prevents modernization benefits"
       },
       examStrategy: "For zero-downtime migrations: DMS with CDC is essential. RDS Custom preserves Oracle RAC features. Location Service is AWS's GIS solution. B2B Data Interchange handles EDI standards."
   }
},
{
   id: 'sap_093',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global investment bank with 300 AWS accounts needs to implement: complete network isolation between trading desks for regulatory compliance, centralized egress filtering for data loss prevention, low-latency connectivity between accounts (<1ms), support for overlapping IP ranges from acquisitions, centralized DNS resolution with query logging, and network costs optimization (currently $2M/month).",
   question: "Which network architecture BEST meets these complex requirements?",
   options: [
       "AWS Transit Gateway with multiple route tables for isolation, AWS Network Firewall in centralized egress VPC for DLP, Transit Gateway peering for inter-region connectivity, AWS Cloud WAN for overlapping IP support with NAT, Route 53 Resolver with query logging in shared services VPC, and replace NAT Gateways with NAT instances in smaller accounts for cost optimization",
       "VPC peering mesh between all accounts, Security Groups for isolation, NAT Gateways in each VPC, VPN for overlapping IPs, Route 53 private hosted zones, and Direct Connect for all traffic",
       "AWS PrivateLink for everything, distributed firewalls, Direct Connect with VIFs, proxy servers for overlapping IPs, centralized DNS servers on EC2, and internet gateway in each VPC",
       "Hub-and-spoke with Transit VPC, virtual firewalls on EC2, software-defined WAN, NAT for overlapping ranges, BIND DNS servers, and centralized internet egress"
   ],
   correct: 0,
   explanation: {
       correct: "Transit Gateway with route tables provides scalable isolation without managing thousands of peering connections, Network Firewall in egress VPC enables centralized DLP and reduces costs vs distributed firewalls, Transit Gateway peering maintains <1ms latency, Cloud WAN elegantly handles overlapping IPs with built-in NAT, Route 53 Resolver with logging provides managed DNS with compliance logging, and NAT instances in small accounts can reduce costs 75% vs NAT Gateways for $500K+ annual savings.",
       whyWrong: {
           1: "VPC peering mesh with 300 accounts is unmanageable (44,850 connections), and NAT Gateways everywhere is expensive",
           2: "PrivateLink doesn't support all use cases, and distributed firewalls multiply costs significantly",
           3: "Transit VPC is outdated pattern, and managing BIND DNS servers adds operational overhead"
       },
       examStrategy: "For complex enterprise networking: Transit Gateway with route tables for isolation, Cloud WAN for overlapping IPs, Network Firewall for centralized security. NAT instances can still be cost-effective for small workloads."
   }
},
{
   id: 'sap_094',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A biotech company is building a drug discovery platform using AI/ML. Requirements: process 10 million molecular structures daily, run quantum chemistry simulations requiring 1000 GPUs for 48-hour jobs, store 500TB of genomic data with HIPAA compliance, enable collaboration between 50 research institutions, provide real-time visualization of 3D protein folding, support FDA submission with 21 CFR Part 11 compliance, and reduce time-to-discovery by 10x.",
   question: "Which architecture BEST accelerates drug discovery while maintaining compliance?",
   options: [
       "AWS ParallelCluster with P4d instances for GPU compute at scale, Amazon EC2 Hpc6a instances for quantum simulations, AWS HealthLake for HIPAA-compliant genomic storage with FHIR APIs, AWS Clean Rooms for multi-institution collaboration without data sharing, NICE DCV for high-performance 3D visualization, AWS Backup with compliance mode for 21 CFR Part 11, and SageMaker with distributed training for 10x acceleration",
       "EC2 Spot Fleet for GPU compute, S3 for all storage, DataSync for collaboration, Lambda for processing, QuickSight for visualization, CloudTrail for compliance, and EMR for distributed computing",
       "Batch with GPU jobs, DynamoDB for molecular data, Transfer Family for sharing, Fargate for simulations, AppStream for visualization, Config for compliance, and Glue for ETL",
       "ECS with GPU containers, RDS for structured data, Direct Connect to institutions, Step Functions for workflows, WorkSpaces for visualization, Macie for compliance, and Redshift for analytics"
   ],
   correct: 0,
   explanation: {
       correct: "ParallelCluster with P4d provides massive GPU scale with HPC scheduling for molecular processing, Hpc6a instances deliver 200 Gbps networking required for quantum simulations, HealthLake ensures HIPAA compliance with built-in de-identification and FHIR standards, Clean Rooms enables secure multi-party computation preserving research IP, NICE DCV provides GPU-accelerated remote visualization for complex 3D proteins, Backup compliance mode ensures immutable audit trails for FDA 21 CFR Part 11, and SageMaker distributed training achieves 10x speedup through optimized parallelization.",
       whyWrong: {
           1: "Spot Fleet interruptions disrupt 48-hour simulations, and S3 alone doesn't provide HIPAA-specific features",
           2: "Batch lacks HPC features for tightly-coupled GPU workloads, and DynamoDB isn't suitable for complex molecular structures",
           3: "ECS doesn't provide HPC scheduling, and WorkSpaces isn't optimized for scientific 3D visualization"
       },
       examStrategy: "For HPC drug discovery: ParallelCluster for compute orchestration, Hpc6a for network-intensive workloads, HealthLake for genomic compliance, Clean Rooms for secure collaboration."
   }
},
{
   id: 'sap_095',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "medium",
   timeRecommendation: 150,
   scenario: "A mobile gaming company with 50 million users runs entirely on AWS Lambda and DynamoDB. Monthly costs are $800,000 with: 60% from Lambda invocations, 30% from DynamoDB, and 10% from data transfer. Analysis shows: 70% of Lambda invocations are for session validation, average function duration is 50ms but billed for 100ms minimum, DynamoDB has 1000 provisioned WCU/RCU per table but uses only 300 average, and 5TB of Lambda ephemeral storage downloads daily.",
   question: "Which optimization strategy would provide the MOST significant cost reduction?",
   options: [
       "Implement Amazon Cognito for session management eliminating 70% of Lambda invocations, configure Lambda with ARM-based Graviton2 for 20% cost reduction, switch DynamoDB to on-demand mode for variable workloads, implement Lambda SnapStart to reduce cold starts and duration, and use Amazon EFS for shared storage eliminating repeated downloads",
       "Add API Gateway caching, use smaller Lambda memory settings, keep DynamoDB provisioned but lower values, implement CloudFront, and compress all payloads",
       "Move session validation to EC2, batch Lambda invocations, use DynamoDB auto-scaling, add ElastiCache, and optimize code",
       "Implement Step Functions, use Lambda layers, add DynamoDB global tables, implement X-Ray, and use Reserved Capacity"
   ],
   correct: 0,
   explanation: {
       correct: "Cognito eliminates 70% of Lambda invocations providing massive cost reduction on the largest cost driver, Graviton2 provides 20% better price-performance, on-demand DynamoDB better matches the 300 average vs 1000 provisioned capacity, SnapStart reduces Java/Python cold starts improving duration efficiency, and EFS eliminates 5TB daily downloads saving significant data transfer costs.",
       whyWrong: {
           1: "API Gateway caching helps but doesn't eliminate the root cause of excessive session validation invocations",
           2: "Moving to EC2 breaks the serverless architecture and adds management overhead",
           3: "Step Functions would increase costs for simple session validation, and Reserved Capacity doesn't exist for Lambda"
       },
       examStrategy: "For serverless optimization: Cognito eliminates auth Lambda invocations, Graviton2 reduces compute costs, SnapStart improves cold starts. Address the root cause not symptoms."
   }
},
{
   id: 'sap_096',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A century-old railroad company is modernizing its operations to AWS. Systems include: mainframe-based train scheduling and signaling, real-time GPS tracking for 10,000 locomotives, predictive maintenance using sensor data, passenger ticketing systems, freight logistics platform, and integration with government transportation systems. The platform must ensure safety-critical operations with zero downtime and enable new digital services.",
   question: "Which modernization approach BEST balances safety-critical requirements with innovation?",
   options: [
       "AWS Mainframe Modernization with dual-run for train control systems ensuring safety, AWS IoT FleetWise for locomotive tracking with railway-specific protocols, Amazon Monitron and Lookout for Equipment for predictive maintenance, modern ticketing on Amazon API Gateway with Cognito, AWS Supply Chain for freight logistics optimization, AWS B2B Data Interchange for government system integration, and AWS Outposts at critical signal locations for local resilience",
       "Keep mainframe for safety systems, build new IoT platform, use SageMaker for all ML, create custom ticketing system, build logistics from scratch, use APIs for integration, and hybrid cloud approach",
       "Full cloud migration of everything, serverless architecture, AI for all operations, mobile-first design, blockchain for logistics, microservices everywhere, and public cloud only",
       "Lift-and-shift mainframe to EC2, basic GPS tracking, manual maintenance, keep existing ticketing, separate logistics system, point-to-point integrations, and VPN connectivity"
   ],
   correct: 0,
   explanation: {
       correct: "Dual-run Mainframe Modernization maintains safety-critical signaling while enabling gradual modernization, IoT FleetWise handles railway-specific protocols (like Positive Train Control), Monitron and Lookout provide purpose-built predictive maintenance for industrial equipment, API Gateway with Cognito enables modern digital ticketing experiences, AWS Supply Chain provides end-to-end freight visibility and optimization, B2B Data Interchange handles government EDI requirements, and Outposts at signal locations ensures operations during network failures.",
       whyWrong: {
           1: "Keeping mainframe limits innovation potential, and building custom everything delays time-to-value",
           2: "Full serverless for safety-critical train control is irresponsible, and blockchain adds unnecessary complexity",
           3: "Lift-and-shift doesn't modernize capabilities, and basic GPS lacks railway-specific features"
       },
       examStrategy: "For safety-critical modernization: dual-run approach maintains safety while modernizing. IoT FleetWise supports industry protocols. Monitron/Lookout are purpose-built for industrial predictive maintenance."
   }
},
{
   id: 'sap_097',
   domain: "Domain 1: Design Solutions for Organizational Complexity",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A global law firm with 10,000 attorneys across 50 offices needs to implement: matter-centric security (each legal case has unique access requirements), compliant e-discovery across 100TB of documents, secure collaboration with external counsel and clients, AI-powered legal research while maintaining attorney-client privilege, billing integration with 200 client systems, and compliance with legal industry regulations (CCPA, GDPR, data residency).",
   question: "Which architecture BEST addresses legal industry security and compliance requirements?",
   options: [
       "Amazon WorkDocs with SDK for matter-based access controls, Amazon Kendra with access control lists for privileged legal research, AWS Clean Rooms for secure external collaboration without data exposure, Amazon Textract with Amazon Comprehend Legal for e-discovery processing, AWS B2B Data Interchange for client billing system integration, AWS Control Tower with data residency guardrails per jurisdiction, and Amazon Macie for privilege detection",
       "SharePoint on AWS for documents, Elasticsearch for research, S3 with bucket policies for sharing, Lambda for document processing, custom billing APIs, Organizations for compliance, and CloudTrail for audit",
       "Box.com integration for documents, third-party legal research, email for collaboration, manual e-discovery, point-to-point billing integration, manual compliance tracking, and basic logging",
       "EFS for file storage, OpenSearch for search, IAM roles for access, Textract for OCR, REST APIs for billing, Config for compliance, and CloudWatch for monitoring"
   ],
   correct: 0,
   explanation: {
       correct: "WorkDocs SDK enables programmatic matter-centric access beyond simple folder permissions, Kendra with ACLs maintains privilege during AI-powered research, Clean Rooms allows secure collaboration without exposing client data, Textract with Comprehend Legal extracts and understands legal documents for e-discovery, B2B Data Interchange handles various client billing formats (EDI, API, file), Control Tower guardrails enforce data residency automatically, and Macie identifies and protects privileged communications.",
       whyWrong: {
           1: "SharePoint lacks matter-centric security model, and bucket policies can't handle complex legal access patterns",
           2: "Third-party solutions lack AWS integration for compliance, and manual e-discovery doesn't scale to 100TB",
           3: "EFS lacks document management features, and IAM roles alone can't implement matter-based security"
       },
       examStrategy: "For legal industry: WorkDocs provides document management with SDK, Kendra maintains access controls for search, Clean Rooms enables secure collaboration, Comprehend has legal-specific features."
   }
},
{
   id: 'sap_098',
   domain: "Domain 2: Design for New Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "An autonomous shipping company is building a fleet management platform for 1,000 autonomous cargo ships. Requirements: real-time navigation processing with satellite and radar data, weather routing optimization across ocean routes, collision avoidance with ML prediction, remote vessel control with <2 second latency anywhere at sea, predictive maintenance for engines and systems, compliance with maritime regulations (IMO 2020), and integration with 500 ports worldwide.",
   question: "Which architecture BEST enables autonomous maritime operations globally?",
   options: [
       "AWS Ground Station for satellite communication with ships, AWS IoT FleetWise with maritime protocols (NMEA), Amazon Location Service with maritime charts for navigation, Amazon Forecast with Weather Index for route optimization, SageMaker reinforcement learning for collision avoidance, AWS Wavelength with maritime carriers for low-latency control, Amazon Lookout for Equipment for predictive maintenance, and AWS B2B Data Interchange for port system integration",
       "Direct satellite links, generic IoT platform, Google Maps for navigation, basic weather APIs, simple ML models, standard internet for control, threshold-based maintenance, and custom port APIs",
       "Inmarsat only, custom IoT solution, offline navigation, weather downloads, rule-based avoidance, VPN for control, scheduled maintenance, and EDI for ports",
       "Multiple satellite providers, Azure IoT, HERE maps, weather.com API, TensorFlow models, 5G for control, manual maintenance tracking, and point-to-point integration"
   ],
   correct: 0,
   explanation: {
       correct: "Ground Station provides global satellite coverage for continuous ship communication, IoT FleetWise supports maritime NMEA protocols and ship systems, Location Service with maritime charts ensures accurate ocean navigation, Forecast with Weather Index optimizes routing using integrated weather data, SageMaker RL adapts collision avoidance to changing conditions, Wavelength with maritime carriers achieves <2 second control latency at sea, Lookout for Equipment predicts failures in marine engines, and B2B Data Interchange handles diverse port EDI standards globally.",
       whyWrong: {
           1: "Google Maps lacks maritime navigation features, and standard internet doesn't work reliably at sea",
           2: "Offline navigation prevents real-time optimization, and scheduled maintenance misses failure predictions",
           3: "Multi-cloud adds complexity for ships at sea, and HERE maps lacks maritime-specific features"
       },
       examStrategy: "For maritime/shipping: Ground Station provides satellite connectivity, Location Service supports maritime navigation, Wavelength enables edge computing with carriers, IoT FleetWise handles industry protocols."
   }
},
{
   id: 'sap_099',
   domain: "Domain 3: Continuous Improvement for Existing Solutions",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A video streaming platform with 200M users spends $5M monthly on AWS. Architecture: CloudFront ($2M), EC2 for transcoding ($1.5M), S3 for videos ($1M), RDS for metadata ($500K). Issues: CloudFront cache hit ratio is 60%, transcoding takes 4x video duration, S3 has 100PB with 90% never accessed, RDS has complex queries causing timeouts, and new 8K content doubles infrastructure costs.",
   question: "Which optimization strategy achieves the MOST comprehensive cost reduction and performance improvement?",
   options: [
       "Implement CloudFront Origin Shield and Cache-Control headers to increase hit ratio to 95%, replace EC2 transcoding with AWS Elemental MediaConvert for parallel processing, enable S3 Intelligent-Tiering moving 90PB to Archive tiers, migrate RDS to Amazon Neptune for graph-based content relationships, implement AWS Elemental MediaPackage for just-in-time packaging reducing storage 40%, and use CloudFront compression for 8K content",
       "Add more CloudFront edges, use larger EC2 instances, manually move old videos to Glacier, add RDS read replicas, pre-transcode everything, and increase bandwidth",
       "Use multiple CDNs, build custom transcoding on GPUs, implement tape storage for old content, move to NoSQL, create more formats, and compress everything",
       "Reduce video quality, limit transcoding, delete old content, simplify database, charge users more, and block 8K content"
   ],
   correct: 0,
   explanation: {
       correct: "Origin Shield and proper cache headers achieve 95% hit ratio reducing origin costs 80%, MediaConvert provides managed parallel transcoding 10x faster than serial EC2, Intelligent-Tiering automatically moves 90PB to Archive saving $800K/month, Neptune's graph model perfectly fits content relationships eliminating query timeouts, MediaPackage just-in-time packaging eliminates storing multiple formats reducing storage 40%, and CloudFront compression reduces 8K bandwidth costs 30%.",
       whyWrong: {
           1: "Adding infrastructure without optimization increases costs further, and pre-transcoding wastes storage",
           2: "Multiple CDNs add complexity, and tape storage prevents streaming access to archived content",
           3: "Reducing quality and limiting features degrades user experience and competitive position"
       },
       examStrategy: "For video platform optimization: Origin Shield is crucial for CDN costs, MediaConvert beats EC2 for transcoding, MediaPackage enables just-in-time packaging, Neptune excels at content relationships."
   }
},
{
   id: 'sap_100',
   domain: "Domain 4: Accelerate Workload Migration and Modernization",
   difficulty: "hard",
   timeRecommendation: 180,
   scenario: "A 150-year-old national bank must migrate its entire technology stack to AWS within 24 months due to regulatory mandate. This includes: core banking on IBM mainframe (10M transactions/day), 500 branch applications, 50M customer accounts, credit card processing platform, mobile banking serving 30M users, data warehouse with 20 years history (5PB), and real-time fraud detection. Requirements: zero customer impact, maintain all regulatory reporting, achieve PCI-DSS and SOC2 compliance, and enable open banking APIs.",
   question: "Which migration strategy ensures regulatory compliance while meeting the aggressive timeline?",
   options: [
       "AWS Migration Hub Orchestra for program management, AWS Mainframe Modernization with blue-green deployment for core banking, AWS Application Migration Service wave-based migration for branch applications, AWS DataSync with DMS for account migration with validation, AWS Data Pipeline for warehouse migration to Redshift, Amazon Fraud Detector replacing existing system, AWS Control Tower with Financial Services Industry Lens for compliance, and API Gateway with Lambda authorizers for open banking",
       "Big bang migration over a weekend, lift-and-shift everything to EC2, keep all databases as-is, maintain existing fraud system, basic AWS setup, manual compliance checks, and REST APIs for open banking",
       "Gradual migration over 5 years, hybrid cloud permanently, keep mainframe running, selective application migration, partial data migration, maintain on-premises fraud detection, and delay open banking",
       "Outsource to system integrator, rewrite everything from scratch, move to microservices, implement blockchain, go cloud-native, build new fraud system, and create API marketplace"
   ],
   correct: 0,
   explanation: {
       correct: "Migration Hub Orchestra coordinates complex program meeting 24-month deadline, Mainframe Modernization with blue-green ensures zero customer impact during core banking migration, wave-based MGN systematically migrates 500 branch apps, DataSync with DMS handles massive data migration with validation ensuring account integrity, Data Pipeline efficiently migrates 5PB warehouse to Redshift, Fraud Detector provides superior ML-based detection, Control Tower with FSI Lens ensures PCI-DSS/SOC2 compliance from day one, and API Gateway with Lambda authorizers enables secure open banking APIs.",
       whyWrong: {
           1: "Big bang migration of a 150-year-old bank is impossibly risky and would cause massive customer impact",
           2: "5-year gradual migration exceeds regulatory mandate, and permanent hybrid increases costs and complexity",
           3: "Complete rewrite in 24 months is impossible for core banking, and blockchain is unnecessary complexity"
       },
       examStrategy: "For large-scale regulated migrations: Migration Hub Orchestra coordinates complexity, wave-based approach manages risk, blue-green deployment ensures zero downtime, FSI Lens provides compliance blueprints."
   }
},

// Additional SAP Questions 101-140 for the existing question bank
// Answer Distribution Target: 25% per option (A/B/C/D)

{
    id: 'sap_101',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global ride-sharing company is launching in 50 new cities simultaneously. The platform must handle 2 million ride requests per second during peak hours, provide real-time driver matching within 500ms, support dynamic surge pricing based on demand, integrate with local payment systems in each country, and maintain 99.99% availability. The solution must scale from zero to full capacity in 24 hours.",
    question: "Which architecture provides the BEST global scalability and real-time performance?",
    options: [
        "CloudFront with Lambda@Edge for request routing, ECS Fargate with auto-scaling for matching logic, DynamoDB Global Tables for driver locations, EventBridge for surge pricing events, and API Gateway for payment integrations",
        "Amazon ElastiCache for Redis with cluster mode for real-time driver locations, DynamoDB with Global Secondary Indexes for ride matching, Kinesis Data Analytics for surge pricing algorithms, Lambda with provisioned concurrency for sub-500ms response, and AWS Payment Cryptography for secure transactions",
        "RDS Aurora Global Database for ride data, EC2 Auto Scaling groups across regions, SQS for request queuing, Step Functions for matching workflow, and third-party payment gateways via VPC endpoints",
        "Amazon MSK for event streaming, EMR for real-time analytics, Neptune for driver-rider graphs, Batch for surge calculations, and Direct Connect to payment providers"
    ],
    correct: 1,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides microsecond latency for 2M TPS with geo-distributed drivers, DynamoDB GSI enables complex spatial queries for optimal matching, Kinesis Data Analytics processes demand patterns in real-time for dynamic pricing, Lambda provisioned concurrency eliminates cold starts ensuring <500ms response, and Payment Cryptography handles multi-country compliance securely.",
        whyWrong: {
            0: "Lambda@Edge has payload size limitations for complex matching logic, and EventBridge can't handle 2M events/second reliably",
            2: "Aurora Global has single-writer limitations that can't support 2M writes/second, and SQS adds latency to real-time matching",
            3: "MSK and EMR add unnecessary complexity for ride matching, and Neptune graph queries are too slow for real-time requirements"
        },
        examStrategy: "For ultra-high TPS real-time applications, ElastiCache cluster mode + DynamoDB is the proven pattern. Lambda provisioned concurrency is essential for consistent sub-second response times."
    }
},
{
    id: 'sap_102',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A enterprise SaaS platform serves 10,000 customers using a shared PostgreSQL Aurora cluster (db.r6g.16xlarge) with 50,000 schemas. The database is at 90% CPU utilization, backup windows exceed 8 hours affecting availability, cross-tenant queries leak data between customers, and costs have reached $200,000/month. Customer growth is accelerating 50% annually.",
    question: "Which optimization strategy provides the BEST long-term scalability while reducing costs and improving security?",
    options: [
        "Implement Aurora Serverless v2 for automatic scaling, use Aurora Backtrack for instant recovery, add read replicas for query distribution, implement Row Level Security (RLS) for tenant isolation, and enable Aurora MySQL mode for better concurrency",
        "Migrate to a database-per-tenant model using Aurora Serverless v2 clusters, implement AWS Database Migration Service for seamless tenant migration, use Aurora Global Database for backup redundancy, and AWS Service Catalog for automated provisioning",
        "Add more Aurora read replicas, implement ProxySQL for connection pooling, use Aurora Fast Cloning for backups, partition tables by tenant_id, and upgrade to db.r6g.24xlarge instances",
        "Transition to a hybrid model: Aurora dedicated clusters for large tenants (>1000 users), Aurora Serverless v2 shared clusters for small tenants, implement AWS DMS for tenant mobility between tiers, and use Aurora Multi-Master for high availability"
    ],
    correct: 3,
    explanation: {
        correct: "Hybrid model addresses the noisy neighbor problem with dedicated clusters for large customers while maintaining cost efficiency for small tenants through shared serverless clusters, DMS enables seamless tenant movement between tiers as they grow, Multi-Master provides better availability than single-writer, and the model scales economically with customer growth patterns.",
        whyWrong: {
            0: "Serverless v2 on a single cluster still faces noisy neighbor issues, and Aurora MySQL conversion breaks PostgreSQL compatibility",
            1: "Database-per-tenant for 10,000 customers creates management nightmare and connection pool exhaustion",
            2: "Simply scaling up doesn't solve fundamental multi-tenancy issues and increases costs without improving isolation"
        },
        examStrategy: "For multi-tenant scaling challenges, hybrid approaches often provide the best balance. Consider tenant size distribution when designing database architecture. Aurora Multi-Master can provide better concurrency than read replicas."
    }
},
{
    id: 'sap_103',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A defense contractor operates 100 AWS GovCloud accounts across classified programs. Each program requires complete isolation with no data sharing, custom compliance frameworks per classification level, centralized security monitoring without compromising isolation, unified identity management supporting multiple security clearance levels, and cost allocation to individual defense contracts.",
    question: "Which architecture maintains program isolation while enabling operational oversight?",
    options: [
        "AWS Control Tower in GovCloud with custom guardrails per classification, AWS Identity Center with permission sets based on clearance levels, Amazon Security Lake with delegated admin accounts per program, AWS Cost Categories with automated contract allocation, and AWS CloudTrail Lake for centralized audit without data commingling",
        "Separate AWS Organizations for each classification level, Active Directory federation for identity, GuardDuty standalone in each account, manual cost tracking per contract, and individual CloudTrail logs per account",
        "Single Organization with nested OUs per program, IAM roles with clearance-based policies, Security Hub cross-account aggregation, Cost Explorer with tagging, and consolidated CloudTrail logging",
        "AWS Landing Zone with security baseline, SAML federation with military identity providers, AWS Config aggregator for compliance, AWS Budgets for cost control, and S3 centralized logging"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower with classification-specific guardrails provides governed isolation, Identity Center permission sets enable clearance-based access control, Security Lake with delegated admin maintains security oversight while preserving program boundaries, Cost Categories automate contract-level allocation for billing compliance, and CloudTrail Lake aggregates audit data without exposing program details.",
        whyWrong: {
            1: "Separate Organizations break cost consolidation and create management complexity for 100 accounts",
            2: "Security Hub aggregation could leak classified information across programs, violating isolation requirements",
            3: "Landing Zone is deprecated, and centralized S3 logging violates data isolation requirements for classified programs"
        },
        examStrategy: "For classified/isolated environments, delegated administration is key - Security Lake and CloudTrail Lake provide oversight without data exposure. Control Tower works in GovCloud for compliance automation."
    }
},
{
    id: 'sap_104',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state government agency needs to migrate 200 applications from aging data centers to AWS GovCloud within 18 months. Applications include citizen services requiring 24/7 availability, legacy COBOL systems for benefits processing, Oracle databases with complex stored procedures, Windows-based case management systems, and integration with federal systems. The migration must maintain FISMA compliance and complete before budget cycle ends.",
    question: "Which migration approach ensures compliance while meeting the deadline?",
    options: [
        "AWS Application Discovery Service for inventory, CloudEndure Migration for lift-and-shift, AWS Database Migration Service for Oracle, AWS Systems Manager for patch management, and AWS Config for FISMA compliance",
        "Manual application assessment, AWS Application Migration Service (MGN) for servers, AWS Schema Conversion Tool for database migration, AWS Mainframe Modernization for COBOL, and AWS Control Tower for governance",
        "Third-party migration tools, lift-and-shift to EC2, keep Oracle on-premises with hybrid connectivity, modernize Windows apps to containers, and manual compliance tracking",
        "AWS Migration Hub for coordination, VM Import/Export for servers, RDS for Oracle databases, App2Container for Windows apps, and AWS Audit Manager for FISMA"
    ],
    correct: 1,
    explanation: {
        correct: "Manual assessment provides accurate inventory for government systems, MGN (successor to CloudEndure) handles bulk server migration efficiently, SCT automates complex Oracle conversion, Mainframe Modernization specifically supports COBOL migration for government, and Control Tower provides FISMA-compliant governance from day one.",
        whyWrong: {
            0: "CloudEndure is deprecated in favor of MGN, and Config alone doesn't provide comprehensive FISMA frameworks",
            2: "Hybrid Oracle approach doesn't reduce data center footprint and increases complexity",
            3: "VM Import/Export is slower than MGN for bulk migration, and Audit Manager is for ongoing compliance not initial setup"
        },
        examStrategy: "For government migrations, AWS MGN is now the primary server migration service. Mainframe Modernization supports government COBOL systems. Control Tower provides pre-built compliance frameworks."
    }
},
{
    id: 'sap_105',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A startup is building a real-time language translation platform for video conferences supporting 100 languages. Requirements include: <200ms translation latency, support for 10,000 concurrent meetings, real-time speech-to-text in multiple accents, contextual translation preserving business terminology, offline capability for secure meetings, and integration with major video platforms (Zoom, Teams, etc.).",
    question: "Which architecture delivers real-time multilingual translation at scale?",
    options: [
        "Amazon Transcribe for speech-to-text with custom vocabulary, Amazon Translate with custom terminology for business context, Lambda with provisioned concurrency for sub-200ms response, DynamoDB for terminology caching, and S3 for offline model storage",
        "Custom speech recognition on GPU instances, Google Translate API for accuracy, EC2 Auto Scaling for capacity, ElastiCache for translation caching, and EBS for model storage",
        "Amazon Transcribe streaming with custom language models, Amazon Translate with real-time translation, API Gateway with Lambda for orchestration, CloudFront for global distribution, and Snowball Edge for offline deployments",
        "SageMaker endpoints for custom ML models, Kinesis for audio streaming, Lambda@Edge for processing, DynamoDB global tables for terminology, and AWS Wavelength for latency optimization"
    ],
    correct: 2,
    explanation: {
        correct: "Transcribe streaming provides real-time speech-to-text with custom language model support for accents, Translate real-time mode achieves <200ms latency, API Gateway with Lambda orchestrates the pipeline efficiently, CloudFront edge locations minimize global latency, and Snowball Edge enables offline secure meeting support.",
        whyWrong: {
            0: "Standard Transcribe (non-streaming) adds too much latency for real-time requirements",
            1: "Custom GPU speech recognition requires extensive ML expertise and Google Translate API creates external dependency",
            3: "SageMaker endpoints add latency vs managed Transcribe/Translate services, and Wavelength isn't needed for this use case"
        },
        examStrategy: "For real-time AI applications, use streaming versions of AWS AI services. Transcribe streaming + Translate real-time provides the lowest latency. CloudFront helps with global distribution of real-time services."
    }
},
{
    id: 'sap_106',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A retail chain with 2,000 stores needs unified inventory management across all locations. Each store currently has its own AWS account and database. Corporate requires real-time inventory visibility, automated reordering when stock is low, compliance with state tax regulations, and the ability to transfer inventory between stores instantly.",
    question: "Which solution provides unified inventory management while maintaining store autonomy?",
    options: [
        "Implement AWS Lake Formation with cross-account permissions for centralized inventory data lake, use EventBridge for real-time inventory events, DynamoDB Global Tables for store inventory sync, and Lambda for automated reordering logic",
        "AWS DataSync for nightly inventory synchronization, centralized RDS database for all inventory, SQS for reorder notifications, and EC2 instances for processing",
        "Amazon DynamoDB with Global Secondary Indexes in each store account, AWS AppSync for real-time synchronization, Lambda functions for business logic, and S3 for inventory reports",
        "Amazon QLDB for immutable inventory ledger, Kinesis Data Streams for real-time updates, API Gateway for store integrations, and Step Functions for reorder workflows"
    ],
    correct: 0,
    explanation: {
        correct: "Lake Formation provides governed cross-account access to inventory data while maintaining store autonomy, EventBridge enables real-time event-driven architecture for inventory changes, DynamoDB Global Tables ensure consistent inventory state across stores, and Lambda provides serverless automated reordering based on business rules.",
        whyWrong: {
            1: "Nightly sync is too slow for real-time inventory requirements, and centralized RDS creates single point of failure",
            2: "AppSync alone doesn't provide the data governance needed for 2,000 stores, and GSI in each account creates data silos",
            3: "QLDB is overkill for inventory management, and doesn't provide the real-time sync capabilities needed"
        },
        examStrategy: "For multi-account data sharing with governance, Lake Formation is the key service. EventBridge provides loosely coupled event-driven architecture across accounts."
    }
},
{
    id: 'sap_107',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A music streaming service processes 500 million song plays daily. Current architecture uses EC2 instances with EBS volumes, costing $300,000/month. Analysis shows: 80% of plays are for songs released in the last 30 days, storage costs are 40% of total bill, CPU utilization averages 25%, and weekend traffic drops 60%. The service needs 50% cost reduction while maintaining performance.",
    question: "Which optimization approach provides the greatest cost savings?",
    options: [
        "Migrate to Lambda for serverless processing, implement S3 Intelligent-Tiering for audio files with CloudFront caching, use DynamoDB on-demand for metadata, and implement scheduled scaling for weekend traffic reduction",
        "Switch to Spot Instances with mixed instance types, add S3 lifecycle policies to move old music to Glacier, implement Auto Scaling with target tracking, and use EBS gp3 volumes",
        "Containerize on ECS with Fargate Spot, keep current storage unchanged, add ElastiCache for popular songs, and implement predictive scaling",
        "Keep EC2 but resize to smaller instances, compress all audio files, implement CDN caching, and use Reserved Instances for predictable workload"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda eliminates idle EC2 costs and scales to zero for weekend drops, S3 Intelligent-Tiering automatically moves 80% of music to cheaper tiers after 30 days addressing the 40% storage cost, CloudFront caching reduces backend processing for popular songs, and DynamoDB on-demand scales with actual usage patterns providing additional savings.",
        whyWrong: {
            1: "Spot Instances risk availability for a customer-facing streaming service, and Glacier has retrieval delays incompatible with streaming",
            2: "Fargate without changing storage doesn't address the 40% storage cost component",
            3: "Resizing instances doesn't eliminate idle weekend capacity, and compression affects audio quality"
        },
        examStrategy: "For variable workloads with idle time, serverless (Lambda) provides maximum cost efficiency. S3 Intelligent-Tiering automatically optimizes storage costs based on access patterns."
    }
},
{
    id: 'sap_108',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A manufacturing company operates 50 factories with legacy industrial control systems including Wonderware HMI, GE iFIX SCADA, Allen-Bradley PLCs, and proprietary MES systems. They need to migrate operational data to AWS for predictive analytics while maintaining real-time control operations and meeting industrial safety standards (IEC 61508).",
    question: "Which migration approach maintains operational safety while enabling cloud analytics?",
    options: [
        "Keep all control systems on-premises, use AWS IoT SiteWise Edge to collect and buffer data locally, implement AWS DataSync for secure cloud transfer, and deploy AWS IoT Analytics for predictive maintenance in the cloud",
        "Migrate SCADA systems to EC2 instances, implement VPN connectivity for PLCs, use Kinesis for data streaming, and SageMaker for predictive analytics",
        "Replace all systems with cloud-native IoT solutions, implement Lambda for real-time processing, use DynamoDB for operational data, and QuickSight for dashboards",
        "Implement AWS Outposts at each factory for hybrid control, use IoT Core for device management, maintain existing HMI systems, and implement Neptune for equipment relationships"
    ],
    correct: 0,
    explanation: {
        correct: "Keeping control systems on-premises maintains IEC 61508 safety certification, IoT SiteWise Edge provides industrial protocol support (OPC-UA, Modbus) with local buffering ensuring no data loss during connectivity issues, DataSync enables secure batch transfer of historical data, and IoT Analytics provides purpose-built industrial analytics without affecting real-time operations.",
        whyWrong: {
            1: "Migrating SCADA to cloud introduces network latency that could violate safety requirements",
            2: "Replacing safety-certified systems with cloud-native solutions would require extensive recertification",
            3: "Outposts at 50 factories is extremely expensive, and Neptune is overkill for industrial equipment relationships"
        },
        examStrategy: "For industrial control systems, safety certification trumps cloud benefits. IoT SiteWise Edge enables hybrid architecture maintaining safety while getting cloud analytics. Never migrate safety-critical control systems to cloud."
    }
},
{
    id: 'sap_109',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A biotech company is developing a personalized medicine platform requiring analysis of whole genome sequences (3 billion base pairs per patient), drug interaction modeling using molecular dynamics simulations, real-time patient monitoring data from 50,000 patients, HIPAA compliance, and AI-driven treatment recommendations. The platform must process 1,000 genomes daily.",
    question: "Which architecture best supports personalized medicine at scale?",
    options: [
        "AWS HealthLake for genomic data storage with FHIR compliance, Amazon Omics for genome analysis workflows, AWS Batch with Spot instances for molecular simulations, IoT Core for patient monitoring, and SageMaker with HIPAA for AI recommendations",
        "S3 for raw genomic data, EMR for bioinformatics processing, EC2 High Memory instances for simulations, Kinesis for patient data, and Lambda for treatment algorithms",
        "AWS ParallelCluster for genomic analysis, FSx for Lustre for high-performance storage, ECS for containerized simulations, TimeStream for patient monitoring, and Neptune for drug interaction graphs",
        "Amazon Genomics CLI for workflow orchestration, EC2 with NVME storage for performance, Fargate for scalable compute, DynamoDB for patient data, and Comprehend Medical for clinical insights"
    ],
    correct: 0,
    explanation: {
        correct: "HealthLake provides HIPAA-compliant storage with native FHIR support for medical data, Amazon Omics is purpose-built for genomic analysis with optimized bioinformatics workflows, Batch with Spot provides cost-effective molecular simulation compute, IoT Core handles real-time patient monitoring at scale, and SageMaker with HIPAA enables compliant AI model training and inference.",
        whyWrong: {
            1: "Generic EMR lacks genomics-specific optimizations, and Lambda has payload/timeout limitations for complex treatment algorithms",
            2: "ParallelCluster is complex to manage for genomics workflows compared to purpose-built Omics service",
            3: "Genomics CLI is deprecated in favor of Amazon Omics, and Comprehend Medical doesn't provide personalized treatment recommendations"
        },
        examStrategy: "For genomics/biotech: Amazon Omics is the specialized service for genomic analysis. HealthLake provides HIPAA-compliant healthcare data storage. Purpose-built services > generic compute for specialized workloads."
    }
},
{
    id: 'sap_110',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global consulting firm operates 1,000 AWS accounts across client projects. The firm needs to implement: complete client data isolation with audit proof, automated project closure and data deletion, dynamic team access based on project assignments, centralized billing with project-level chargeback, compliance with multiple industry standards (HIPAA, PCI, SOX), and prevention of consultants accessing multiple client environments simultaneously.",
    question: "Which governance model provides the strongest client isolation while maintaining operational efficiency?",
    options: [
        "AWS Organizations with dedicated OUs per client, AWS Control Tower Account Factory for automated provisioning, IAM Identity Center with time-bound sessions and exclusive access policies, AWS Cost Categories for project chargeback, AWS Audit Manager for multi-framework compliance, and automated Lambda functions for project lifecycle management",
        "Separate AWS Organizations per major client, manual account creation, Active Directory integration, consolidated billing across Organizations, Security Hub for compliance, and manual project cleanup",
        "Single Organization with shared accounts, IAM roles per project, resource tagging for isolation, Cost Explorer for tracking, Config for compliance, and manual access management",
        "Client-specific AWS Organizations, SAML federation per client, project-based OUs, manual cost allocation, third-party compliance tools, and ServiceNow for access requests"
    ],
    correct: 0,
    explanation: {
        correct: "Dedicated OUs provide organizational isolation, Control Tower Account Factory enables automated compliant provisioning, IAM Identity Center with time-bound exclusive sessions prevents cross-client access, Cost Categories enable precise project-level chargeback, Audit Manager automates evidence collection across compliance frameworks, and Lambda automation ensures consistent project lifecycle management including mandatory data deletion.",
        whyWrong: {
            1: "Separate Organizations break consolidated billing and multiply management overhead",
            2: "Shared accounts violate client isolation requirements and resource tagging isn't sufficient for audit compliance",
            3: "Client-specific Organizations are unnecessarily complex and manual allocation doesn't scale to 1,000 accounts"
        },
        examStrategy: "For consulting/multi-client scenarios: dedicated OUs provide isolation within single Organization, IAM Identity Center enables sophisticated access patterns, Cost Categories enable precise chargeback at scale."
    }
},
{
    id: 'sap_111',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "An online education platform supports 5 million students with current AWS costs of $200,000/month: EC2 ($80K), RDS ($60K), S3 ($40K), CloudFront ($20K). Issues include: 70% of video content is never watched after 6 months, database queries during exam periods cause 30-second timeouts, EC2 instances run at 20% utilization except during semester registration, and CloudFront cache hit ratio is only 65%.",
    question: "Which optimization strategy addresses all cost and performance issues?",
    options: [
        "Implement Aurora Serverless v2 for automatic database scaling, use S3 Intelligent-Tiering to archive unused content, enable CloudFront compression and optimize cache behaviors for 90% hit ratio, implement EC2 Auto Scaling with warm pools for registration periods, and use Spot Instances for development environments",
        "Add RDS read replicas, manually move old videos to Glacier, increase CloudFront TTLs, use larger EC2 instances, and implement ElastiCache",
        "Migrate to Lambda for serverless processing, keep S3 unchanged, add CloudFront behaviors, use EC2 Savings Plans, and implement DynamoDB",
        "Switch to Aurora Global Database, implement S3 lifecycle policies, add more CloudFront edge locations, use Reserved Instances, and implement API caching"
    ],
    correct: 0,
    explanation: {
        correct: "Aurora Serverless v2 auto-scales for exam period queries eliminating timeouts while saving money during normal periods, S3 Intelligent-Tiering automatically moves 70% of content to cheaper storage tiers, CloudFront optimization increases hit ratio to 90% reducing origin costs, warm pools enable instant scaling for registration without maintaining 20% utilization year-round, and Spot instances reduce development costs.",
        whyWrong: {
            1: "Adding read replicas and larger instances increases costs without addressing low utilization, manual Glacier moves are error-prone",
            2: "Lambda migration is complex for existing applications, and doesn't address storage or CDN issues",
            3: "Aurora Global Database is expensive overkill, and Reserved Instances don't help with variable utilization patterns"
        },
        examStrategy: "Aurora Serverless v2 is ideal for variable database workloads. S3 Intelligent-Tiering automates storage optimization. EC2 warm pools solve scaling delays without waste. Address multiple cost drivers simultaneously."
    }
},
{
    id: 'sap_112',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A regional airline must migrate its reservation system to AWS within 12 months before their mainframe lease expires. The system handles 50,000 bookings daily, integrates with Global Distribution Systems (GDS) like Amadeus and Sabre, manages frequent flyer programs, processes real-time seat availability across 200 aircraft, and must maintain IATA compliance for airline operations.",
    question: "Which migration strategy ensures airline operations continuity while meeting the deadline?",
    options: [
        "AWS Mainframe Modernization with automated refactoring for reservation logic, Amazon API Gateway for GDS integrations with caching, DynamoDB for real-time seat inventory, Lambda for frequent flyer calculations, and AWS B2B Data Interchange for IATA message standards",
        "Lift-and-shift mainframe to EC2, build REST APIs for GDS, use RDS for all data, implement microservices gradually, and custom IATA compliance solution",
        "Rewrite reservation system using modern cloud-native architecture, integrate with third-party GDS APIs, implement GraphQL for real-time updates, and use managed compliance services",
        "Keep mainframe on AWS Outposts, implement hybrid cloud connectivity, gradually migrate components, maintain existing GDS connections, and phase compliance migration"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization with automated refactoring can meet 12-month timeline while preserving critical reservation logic, API Gateway provides managed GDS integration with caching for performance, DynamoDB offers the single-digit millisecond latency needed for real-time seat inventory, Lambda scales perfectly for frequent flyer point calculations, and B2B Data Interchange handles IATA standard messages natively.",
        whyWrong: {
            1: "Manual lift-and-shift with gradual modernization likely won't complete in 12 months for complex airline systems",
            2: "Complete rewrite in 12 months is impossible and extremely risky for airline operations",
            3: "Outposts doesn't solve the mainframe lease expiration and adds unnecessary complexity"
        },
        examStrategy: "For tight migration deadlines, automated modernization tools are essential. B2B Data Interchange handles industry standard message formats. Real-time inventory requires single-digit millisecond databases like DynamoDB."
    }
},
{
    id: 'sap_113',
    domain: "Domain 2: Design for New Solutions", 
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart city initiative requires a traffic management system for 10,000 intersections across the metropolitan area. The system must: process real-time camera feeds for traffic counting, optimize traffic light timing using AI, handle emergency vehicle priority routing, integrate with public transit systems, provide citizen mobile app with traffic updates, predict traffic patterns 30 minutes ahead, and operate during internet outages.",
    question: "Which architecture provides comprehensive intelligent traffic management?",
    options: [
        "Amazon Kinesis Video Streams for traffic cameras, SageMaker for traffic optimization ML models, IoT Core for traffic light control, API Gateway for transit integration, EventBridge for emergency vehicle priority, and Local Zones for reduced latency",
        "AWS Panorama for edge computer vision at intersections, AWS IoT Greengrass for local traffic control, Amazon Forecast for traffic prediction, AWS Wavelength for mobile app performance, AWS B2B Data Interchange for transit integration, and MemoryDB for real-time state management with offline resilience",
        "EC2 with GPU for video processing, Lambda for traffic algorithms, DynamoDB for intersection state, Direct Connect for reliability, AppSync for mobile real-time updates, and ElastiCache for performance",
        "Rekognition for traffic analysis, Step Functions for light control workflows, Neptune for traffic graphs, CloudFront for mobile app, Timestream for historical data, and Aurora for configuration"
    ],
    correct: 1,
    explanation: {
        correct: "Panorama provides edge computer vision eliminating bandwidth costs for 10,000 camera feeds, Greengrass enables local traffic control during outages maintaining safety, Forecast with traffic patterns predicts congestion 30 minutes ahead, Wavelength provides <10ms latency for emergency vehicle apps, B2B Data Interchange handles public transit data formats, and MemoryDB provides durable real-time state with Multi-AZ persistence.",
        whyWrong: {
            0: "Streaming 10,000 camera feeds to cloud is bandwidth-prohibitive, and Local Zones don't provide offline operation",
            2: "EC2 GPU approach is expensive and doesn't solve offline operation requirement",
            3: "Cloud-based Rekognition adds latency and bandwidth costs, and Step Functions isn't suitable for real-time traffic control"
        },
        examStrategy: "For smart city applications, edge computing (Panorama, Greengrass) is essential for bandwidth and latency. Wavelength provides mobile edge computing. B2B Data Interchange handles municipal data standards."
    }
},
{
    id: 'sap_114',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A healthcare system with 20 hospitals operates separate AWS accounts for each facility due to patient privacy requirements. They need unified emergency response coordination, shared medical research data, consolidated IT security monitoring, and cost optimization across all facilities while maintaining HIPAA compliance and patient data isolation.",
    question: "Which approach enables collaboration while preserving privacy requirements?",
    options: [
        "AWS Organizations with healthcare guardrails, AWS HealthLake for HIPAA-compliant data sharing, Amazon Security Lake for unified threat detection, AWS Clean Rooms for privacy-preserving research analytics, and AWS Cost and Usage Reports with hospital-level allocation",
        "Merge all accounts into single healthcare account, implement IAM roles per hospital, use S3 bucket policies for isolation, GuardDuty for security, and consolidated billing",
        "Create hub-and-spoke network with Transit Gateway, replicate data to central research account, implement Security Hub federation, use Direct Connect between hospitals, and manual cost tracking",
        "Keep separate accounts but add VPC peering, use Lambda for emergency notifications, implement third-party research platform, deploy SIEM in each account, and negotiate volume discounts"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations provides governance while maintaining account isolation, HealthLake enables FHIR-compliant data sharing with built-in de-identification, Security Lake aggregates threats without exposing patient data, Clean Rooms allows medical research collaboration without raw data sharing, and CUR provides detailed cost allocation while preserving separation.",
        whyWrong: {
            1: "Merging accounts violates HIPAA requirements for patient data isolation between facilities",
            2: "Central data replication violates patient privacy and could break HIPAA compliance",
            3: "Third-party research platforms add compliance complexity and VPC peering creates potential data leakage paths"
        },
        examStrategy: "For healthcare multi-account scenarios, specialized services (HealthLake, Clean Rooms) enable collaboration while preserving privacy. Security Lake aggregates without exposing sensitive data."
    }
},
{
    id: 'sap_115',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A fintech company processes cryptocurrency transactions using: 100 c5.9xlarge EC2 instances for order matching, DynamoDB tables with 50,000 WCU/RCU provisioned capacity, ElastiCache Redis cluster for market data, and RDS PostgreSQL for user accounts. Monthly costs are $500,000. Performance issues include 15ms order matching latency, DynamoDB throttling during market volatility, Redis memory pressure during price spikes, and PostgreSQL connection exhaustion.",
    question: "Which optimization provides the BEST performance improvement and cost reduction for high-frequency trading?",
    options: [
        "Migrate to c6gn.16xlarge instances with enhanced networking, implement DynamoDB auto-scaling with burst capacity, upgrade to ElastiCache r6g instances with cluster mode, add RDS Proxy for connection management, and implement Spot Fleet for 60% cost savings",
        "Switch to DynamoDB on-demand mode for automatic scaling, implement MemoryDB for persistent market data with clustering, migrate order matching to Lambda with provisioned concurrency, use Aurora Serverless v2 for user accounts, and implement intelligent tiering for cost optimization",
        "Deploy EC2 instances in Placement Groups for low latency, add DynamoDB Accelerator (DAX) for microsecond reads, implement Redis clustering with read replicas, migrate to Aurora PostgreSQL with parallel query, and use Reserved Instances for cost optimization",
        "Implement custom C++ matching engine on bare metal instances, use Amazon QLDB for immutable transaction ledger, deploy Redis across multiple AZs, implement connection pooling, and negotiate Enterprise Discount Program"
    ],
    correct: 2,
    explanation: {
        correct: "Placement Groups provide the lowest network latency for order matching critical to reducing 15ms latency, DAX provides sub-millisecond DynamoDB reads preventing throttling during volatility, Redis clustering with read replicas scales memory and throughput for price spikes, Aurora PostgreSQL with parallel query handles complex user account operations efficiently, and Reserved Instances provide 75% savings on predictable capacity.",
        whyWrong: {
            0: "Spot Fleet is inappropriate for real-time trading due to interruption risk",
            1: "Lambda has cold start issues and concurrency limits inappropriate for high-frequency trading",
            3: "Bare metal instances require significant management overhead, and QLDB adds latency inappropriate for HFT"
        },
        examStrategy: "For low-latency financial trading: Placement Groups reduce network latency, DAX provides microsecond caching, clustering scales performance. Never use Spot instances for real-time trading systems."
    }
},
{
    id: 'sap_116',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A food processing company operates 30 manufacturing plants with Industrial IoT systems including temperature sensors, conveyor controls, quality inspection cameras, and MES (Manufacturing Execution Systems). They need to migrate operational data to AWS for supply chain optimization while maintaining food safety compliance (HACCP) and ensuring plant operations continue during internet outages.",
    question: "Which migration approach maintains food safety while enabling cloud analytics?",
    options: [
        "AWS IoT SiteWise Edge for local data collection and buffering, AWS DataSync for batch cloud transfer, Amazon Lookout for Equipment for predictive maintenance, AWS IoT Analytics for supply chain optimization, and FSx for Windows for MES compatibility",
        "Direct IoT Core connection from all sensors, real-time streaming to Kinesis, Lambda for processing, S3 for data lake, and EC2 for MES systems", 
        "Keep all systems on-premises, implement VPN to AWS, use Database Migration Service for data transfer, and EC2 instances for analytics",
        "AWS Outposts at each plant for hybrid processing, IoT Greengrass for device management, managed MES on AWS, and centralized monitoring"
    ],
    correct: 0,
    explanation: {
        correct: "IoT SiteWise Edge provides industrial protocol support with local buffering ensuring continuous operation during outages while maintaining HACCP compliance, DataSync enables secure batch transfer without affecting real-time operations, Lookout for Equipment provides food-industry specific predictive maintenance, IoT Analytics optimizes supply chain with historical data, and FSx for Windows maintains MES compatibility without migration risk.",
        whyWrong: {
            1: "Direct IoT Core connection creates single point of failure incompatible with food safety requirements",
            2: "Keeping systems on-premises defeats cloud benefits and VPN isn't reliable enough for continuous operations",
            3: "Outposts at 30 plants is extremely expensive and managed MES migration risks food safety compliance"
        },
        examStrategy: "For manufacturing/food safety: local buffering is critical for compliance, never risk production systems. IoT SiteWise Edge provides industrial protocol support with offline capability."
    }
},
{
    id: 'sap_117',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A renewable energy trading company is building a platform for peer-to-peer solar energy trading. Requirements include: real-time energy price matching across 100,000 residential solar installations, blockchain-based settlement for transparency, IoT monitoring of solar panels and battery storage, weather-based energy production forecasting, integration with utility grid systems, and compliance with energy market regulations.",
    question: "Which architecture enables decentralized energy trading with regulatory compliance?",
    options: [
        "AWS IoT Core for solar panel monitoring, Amazon Managed Blockchain for transparent settlements, Amazon Forecast with Weather Index for production prediction, DynamoDB with DAX for real-time price matching, AWS B2B Data Interchange for utility integration, and AWS Audit Manager for energy regulation compliance",
        "Custom IoT platform on EC2, Ethereum blockchain on containers, machine learning on SageMaker, ElastiCache for trading, API Gateway for utilities, and manual compliance tracking",
        "IoT SiteWise for energy assets, QLDB for transaction ledger, Timestream for forecasting, MemoryDB for trading engine, Direct Connect to utilities, and Config for compliance",
        "Kinesis for data ingestion, Lambda for blockchain logic, Redshift for analytics, Aurora for trading data, VPN to utilities, and third-party compliance tools"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core scales to 100,000 installations with device management, Managed Blockchain provides enterprise-grade blockchain with regulatory audit capabilities, Forecast with Weather Index delivers accurate solar production prediction, DynamoDB with DAX enables microsecond energy price matching, B2B Data Interchange handles utility EDI/API standards, and Audit Manager automates energy regulation compliance evidence collection.",
        whyWrong: {
            1: "Custom blockchain on containers lacks enterprise features needed for regulatory compliance",
            2: "QLDB doesn't provide the decentralized transparency required for peer-to-peer trading",
            3: "Lambda isn't suitable for complex blockchain consensus algorithms, and VPN doesn't scale to multiple utilities"
        },
        examStrategy: "For blockchain applications, Managed Blockchain provides enterprise features vs custom implementations. B2B Data Interchange handles utility industry standards. Weather Index in Forecast is ideal for renewable energy prediction."
    }
},
{
    id: 'sap_118',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational oil and gas company has 300 AWS accounts across exploration, refining, and retail divisions. They need: unified environmental compliance reporting across all operations, secure sharing of geological data between exploration teams, real-time monitoring of refinery operations for safety, cost allocation to individual oil fields and refineries, and integration with government regulatory systems in 20 countries.",
    question: "Which governance architecture addresses industry-specific requirements while enabling operational efficiency?",
    options: [
        "AWS Control Tower with custom energy industry guardrails, Amazon DataZone for secure geological data sharing, AWS IoT SiteWise for refinery monitoring, AWS Cost Categories with field-level allocation, AWS Audit Manager with environmental compliance frameworks, and AWS B2B Data Interchange for government regulatory reporting",
        "AWS Organizations with basic policies, S3 with bucket permissions for data sharing, CloudWatch for monitoring, tagging for cost allocation, Security Hub for compliance, and API Gateway for government integration",
        "Separate Organizations per division, Active Directory for access control, ECS for monitoring applications, manual cost tracking, third-party compliance software, and Direct Connect to government systems",
        "AWS Landing Zone for account structure, AWS Lake Formation for data governance, custom monitoring solutions, AWS Budgets for cost control, AWS Config for compliance rules, and VPN connectivity for regulatory systems"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower with energy-specific guardrails provides industry governance, DataZone enables secure geological data sharing with lineage tracking, IoT SiteWise monitors industrial refinery operations with safety protocols, Cost Categories enable granular field/refinery allocation, Audit Manager automates environmental compliance evidence, and B2B Data Interchange handles various government regulatory formats across 20 countries.",
        whyWrong: {
            1: "Basic S3 permissions lack the governance needed for sensitive geological data sharing",
            2: "Separate Organizations create management complexity and break cost consolidation",
            3: "Landing Zone is deprecated, and custom monitoring doesn't provide industrial safety features"
        },
        examStrategy: "For oil/gas industry: IoT SiteWise provides industrial monitoring, DataZone enables secure data sharing with governance, B2B Data Interchange handles regulatory reporting formats. Industry-specific guardrails are key."
    }
},
{
    id: 'sap_119',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium", 
    timeRecommendation: 150,
    scenario: "A global logistics company's shipment tracking system uses: 200 m5.large EC2 instances processing 1M package updates daily, MySQL RDS with 10 read replicas for tracking queries, S3 storing 500GB daily of shipping documents, and API Gateway handling customer queries. Costs are $150,000/month. Issues include: query response times increasing to 5 seconds during peak shipping seasons, database replica lag reaching 30 seconds, 60% of EC2 capacity unused overnight, and API Gateway throttling during Black Friday.",
    question: "Which optimization strategy addresses all performance and cost issues?",
    options: [
        "Implement Aurora Serverless v2 for automatic database scaling, migrate to Lambda with provisioned concurrency for package processing, use S3 Intelligent-Tiering for document storage, implement API Gateway caching with CloudFront distribution, and add DynamoDB for real-time tracking state",
        "Add more RDS read replicas, scale EC2 to larger instances, implement S3 lifecycle policies, increase API Gateway throttling limits, and add ElastiCache for query results",
        "Migrate to Aurora Global Database, containerize on ECS with auto-scaling, keep S3 unchanged, implement GraphQL with AppSync, and use DynamoDB Streams for real-time updates",
        "Keep current RDS but add connection pooling, implement EC2 Auto Scaling with predictive scaling, compress S3 objects, use multiple API Gateway endpoints, and implement Redis caching"
    ],
    correct: 0,
    explanation: {
        correct: "Aurora Serverless v2 scales automatically for peak seasons while reducing overnight costs, Lambda with provisioned concurrency eliminates EC2 idle time and scales to zero, S3 Intelligent-Tiering optimizes document storage costs, API Gateway caching with CloudFront handles Black Friday traffic spikes, and DynamoDB provides single-digit millisecond tracking queries eliminating replica lag.",
        whyWrong: {
            1: "Adding more resources increases costs without addressing architectural inefficiencies",
            2: "Aurora Global Database is expensive overkill, and ECS doesn't solve the fundamental utilization problem",
            3: "Connection pooling doesn't solve replica lag issues, and predictive scaling maintains some idle capacity"
        },
        examStrategy: "For variable logistics workloads, serverless architectures (Aurora Serverless v2, Lambda) provide optimal cost efficiency. DynamoDB excels at real-time tracking data with no replica lag."
    }
},
{
    id: 'sap_120',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A government tax agency must migrate its citizen tax filing system before the next tax season (8 months). Current system: mainframe COBOL processing 100M tax returns annually, Oracle database with 30 years of tax history, integration with 500 state and local tax systems, batch processing taking 72 hours for year-end calculations, and strict IRS compliance requirements including audit trails and data retention.",
    question: "Which migration approach ensures tax season readiness while maintaining compliance?",
    options: [
        "AWS Mainframe Modernization with automated COBOL conversion to Java, Amazon RDS Custom for Oracle maintaining stored procedures, AWS B2B Data Interchange for state/local integrations, AWS Batch with Spot Fleet for batch processing, and AWS CloudTrail Lake for IRS audit requirements",
        "Manual COBOL rewrite to microservices, Aurora PostgreSQL for modernized database, custom APIs for integrations, Lambda for serverless processing, and S3 for audit logs",
        "Lift-and-shift mainframe to EC2, Oracle migration to RDS, point-to-point VPN connections, EMR for batch processing, and CloudWatch for monitoring",
        "Keep mainframe on AWS Outposts, hybrid Oracle deployment, maintain existing integrations, optimize batch jobs in-place, and implement backup solutions"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization with automated conversion can meet 8-month deadline while preserving tax calculation logic, RDS Custom maintains Oracle compatibility critical for 30 years of tax procedures, B2B Data Interchange handles diverse state/local system formats, Batch with Spot provides cost-effective processing for 72-hour jobs, and CloudTrail Lake provides IRS-compliant immutable audit storage.",
        whyWrong: {
            1: "Manual COBOL rewrite in 8 months is impossible for 100M return processing system",
            2: "Lift-and-shift doesn't modernize or improve processing efficiency, and EMR is overkill for tax calculations",
            3: "Outposts doesn't solve modernization needs and maintains data center dependencies"
        },
        examStrategy: "For government systems with tight deadlines, automated modernization tools are essential. RDS Custom preserves database compatibility. B2B Data Interchange handles government EDI standards. CloudTrail Lake provides compliant audit storage."
    }
},

// Questions 121-140 with properly randomized answers (5 per option A/B/C/D)
{
    id: 'sap_121',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A pharmaceutical company is building a clinical trial management platform supporting 500 simultaneous trials across 50 countries. Requirements include: patient recruitment matching using ML, real-time adverse event reporting with <5 minute regulatory notification, integration with 200 hospital systems, secure multi-party data analysis between sponsors and CROs, GDPR/HIPAA compliance with data residency, and FDA 21 CFR Part 11 validation.",
    question: "Which architecture BEST supports global clinical trial operations while maintaining regulatory compliance?",
    options: [
        "Amazon HealthLake for clinical data with FHIR compliance, SageMaker for patient matching algorithms, Lambda for adverse event processing, API Gateway for hospital integrations, and S3 with Object Lock for audit trails",
        "Custom RDBMS for trial data, rule-based patient matching, real-time alerting with SNS, REST APIs for hospital connections, and manual compliance processes",
        "DynamoDB for patient data, Comprehend Medical for matching, Step Functions for event workflows, Direct Connect to hospitals, and Config for compliance monitoring",
        "AWS Clean Rooms for multi-party analytics without data sharing, Amazon Monitron for adverse event detection, AWS HealthLake for regulatory-compliant storage, AWS B2B Data Interchange for hospital system integration, and AWS Backup with compliance mode for 21 CFR Part 11 validation"
    ],
    correct: 3,
    explanation: {
        correct: "Clean Rooms enables secure collaboration between sponsors and CROs without exposing raw patient data, Monitron provides real-time equipment/patient monitoring for adverse events, HealthLake ensures HIPAA/GDPR compliance with built-in de-identification, B2B Data Interchange handles diverse hospital EDI/HL7 formats, and Backup compliance mode provides immutable audit trails meeting FDA 21 CFR Part 11 requirements.",
        whyWrong: {
            0: "HealthLake alone doesn't solve multi-party analytics, and Lambda may not meet 5-minute regulatory notification SLAs consistently",
            1: "Custom RDBMS lacks built-in healthcare compliance features, and manual processes don't scale to 500 trials",
            2: "DynamoDB doesn't provide healthcare-specific compliance features, and Comprehend Medical isn't designed for patient matching"
        },
        examStrategy: "For clinical trials: Clean Rooms enables secure multi-party collaboration, HealthLake provides healthcare compliance, B2B Data Interchange handles medical system integrations. Compliance mode services are key for FDA validation."
    }
},
{
    id: 'sap_122',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global online marketplace processes 10 million transactions daily using: 500 EC2 instances for order processing, Elasticsearch cluster on 20 i3.4xlarge nodes for product search, RDS MySQL with 10 read replicas for inventory, and Redis for session management. Monthly costs are $800,000. Issues include: search latency >3 seconds, inventory inconsistencies, session loss during scale events, and 40% EC2 idle time.",
    question: "Which optimization strategy provides the MOST comprehensive improvement in performance and cost?",
    options: [
        "Migrate to ECS Fargate for elastic compute, implement OpenSearch Serverless for automatic search scaling, use Aurora Global Database for inventory consistency, replace Redis with MemoryDB for session durability, and implement Compute Savings Plans",
        "Migrate to OpenSearch Serverless with machine learning for search relevance, implement Aurora Serverless v2 for automatic inventory scaling, use MemoryDB for durable sessions, containerize on ECS Fargate Spot for cost efficiency, and implement CloudFront for edge caching",
        "Add more Elasticsearch nodes, scale EC2 to larger instances, implement Aurora read replicas, use Redis clustering, and purchase Reserved Instances",
        "Keep current architecture but optimize configurations, implement application-level caching, add database connection pooling, use Spot instances for batch processing, and negotiate enterprise pricing"
    ],
    correct: 1,
    explanation: {
        correct: "OpenSearch Serverless eliminates cluster management while ML-powered search reduces latency to <1 second, Aurora Serverless v2 auto-scales for inventory load preventing inconsistencies, MemoryDB provides Redis performance with Multi-AZ durability eliminating session loss, Fargate Spot reduces costs 70% while eliminating EC2 idle time, and CloudFront caching reduces backend load for popular products.",
        whyWrong: {
            0: "Regular Fargate is more expensive than Fargate Spot, and Global Database is overkill for single-region inventory",
            2: "Adding more nodes increases costs without solving architectural inefficiencies",
            3: "Optimizing current architecture doesn't solve fundamental scaling and durability issues"
        },
        examStrategy: "For e-commerce optimization: OpenSearch Serverless with ML improves search, Aurora Serverless v2 handles variable loads, MemoryDB prevents session loss. Fargate Spot dramatically reduces costs vs EC2."
    }
},
{
    id: 'sap_123',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment bank with 500 AWS accounts needs to implement: complete network segmentation between trading desks for regulatory compliance, centralized egress monitoring for data loss prevention, low-latency connectivity between regions (<5ms), support for high-frequency trading requiring 10Gbps+ throughput, integration with market data providers, and quarterly network security audits.",
    question: "Which network architecture provides optimal performance while meeting regulatory requirements?",
    options: [
        "VPC peering mesh between all accounts, NAT Gateways in each VPC, Direct Connect with hosted VIFs, security groups for segmentation, and VPC Flow Logs for auditing",
        "AWS Transit Gateway with dedicated route tables per trading desk, AWS Network Firewall in centralized inspection VPC, Transit Gateway Network Manager for monitoring, Direct Connect with dedicated 100Gbps connections, and VPC Flow Logs to S3 with Athena analysis",
        "Transit Gateway with multiple route tables for desk isolation, Network Firewall for centralized DLP inspection, Local Zones in financial centers for ultra-low latency, Direct Connect with LAG for high throughput, AWS Network Manager for monitoring, and automated VPC Flow Log analysis with Security Lake",
        "AWS PrivateLink for all inter-desk communication, distributed firewalls in each account, Global Accelerator for performance, multiple Direct Connect locations, and manual audit processes"
    ],
    correct: 2,
    explanation: {
        correct: "Transit Gateway with route table isolation provides scalable desk segmentation, Network Firewall enables centralized DLP without performance impact, Local Zones in financial centers achieve <5ms latency requirements, Direct Connect LAG provides >10Gbps throughput with redundancy, Network Manager offers real-time monitoring, and Security Lake automates audit evidence collection.",
        whyWrong: {
            0: "VPC peering mesh becomes unmanageable at scale (124,750 connections for 500 accounts)",
            1: "Standard regions may not meet <5ms latency requirements for HFT, and manual Athena analysis doesn't scale for quarterly audits",
            3: "PrivateLink doesn't provide the throughput needed for HFT, and manual audits are error-prone"
        },
        examStrategy: "For financial services networking: Local Zones provide ultra-low latency in financial centers, Transit Gateway scales better than VPC peering, LAG provides high-throughput redundancy. Security Lake automates audit evidence."
    }
},
{
    id: 'sap_124',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A credit union needs to migrate its core banking system to AWS within 15 months. Current system includes IBM AS/400 with RPG programs for account management, Oracle database with 25 years of transaction history, Windows-based teller applications, regulatory reporting systems, and integration with Federal Reserve networks. The migration must maintain FFIEC compliance.",
    question: "Which migration strategy ensures banking compliance while meeting the timeline?",
    options: [
        "AWS Mainframe Modernization Service for AS/400 migration with RPG compatibility, Amazon RDS Custom for Oracle maintaining stored procedures, AWS Application Migration Service for Windows teller systems, AWS B2B Data Interchange for Federal Reserve integration, and AWS Audit Manager for FFIEC compliance frameworks",
        "Lift-and-shift AS/400 to EC2, Oracle migration to Aurora, rewrite teller applications, custom Federal Reserve APIs, and manual compliance tracking",
        "Keep AS/400 on-premises with hybrid connectivity, migrate Oracle to RDS, modernize Windows apps to web-based, implement VPN to Federal Reserve, and use third-party compliance tools",
        "Replace core banking with modern SaaS solution, migrate data to cloud databases, build new teller interfaces, integrate with Federal Reserve APIs, and implement cloud compliance frameworks"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization handles AS/400 RPG programs efficiently meeting 15-month deadline, RDS Custom preserves Oracle procedures critical for banking calculations, MGN ensures seamless Windows migration, B2B Data Interchange provides native Federal Reserve network protocols, and Audit Manager offers pre-built FFIEC frameworks for banking compliance.",
        whyWrong: {
            1: "Lift-and-shift AS/400 to EC2 lacks proper RPG runtime support and Aurora migration breaks stored procedures",
            2: "Hybrid approach doesn't reduce data center footprint and VPN lacks reliability for Federal Reserve integration",
            3: "SaaS replacement in 15 months is impossible for core banking with 25 years of customization"
        },
        examStrategy: "For core banking migration: Mainframe Modernization supports AS/400 systems, RDS Custom preserves database compatibility, B2B Data Interchange handles financial industry protocols. Audit Manager has banking-specific frameworks."
    }
},
{
    id: 'sap_125',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A space exploration company is building a mission control platform for Mars rover operations. Requirements include: 20-minute communication delay with Earth, autonomous rover decision-making during communication blackouts, processing 1TB daily of scientific data, 3D terrain modeling for navigation, integration with NASA Deep Space Network, real-time collaboration between global mission teams, and 25-year data archival for scientific analysis.",
    question: "Which architecture handles deep space mission requirements with communication constraints?",
    options: [
        "EC2 instances for mission control, S3 for data storage, Lambda for processing, Direct Connect to NASA, and RDS for mission data",
        "Ground Station for Earth communication, IoT Greengrass for autonomous rover logic, ParallelCluster for scientific computing, FSx for Lustre for high-performance data processing, and S3 Intelligent-Tiering for 25-year archival",
        "Custom ground stations, Kubernetes for rover autonomy, EMR for data processing, EFS for storage, and Glacier for archival",
        "AWS Ground Station for NASA Deep Space Network integration, AWS IoT Greengrass for autonomous rover operations during blackouts, Amazon ParallelCluster for 3D terrain modeling compute, Amazon FSx for Lustre for scientific data processing, S3 Intelligent-Tiering with Glacier Deep Archive for 25-year retention, and AWS AppSync for global team collaboration with offline sync"
    ],
    correct: 3,
    explanation: {
        correct: "Ground Station handles NASA Deep Space Network protocols and 20-minute delays, Greengrass enables autonomous rover decision-making during communication blackouts, ParallelCluster provides HPC for 3D terrain modeling, FSx for Lustre handles 1TB daily scientific data efficiently, S3 Intelligent-Tiering with Deep Archive optimizes 25-year storage costs, and AppSync provides real-time collaboration with offline sync for global teams.",
        whyWrong: {
            0: "Generic EC2 and Direct Connect lack space-specific communication protocols and delay handling",
            1: "Doesn't address global team collaboration or autonomous operations during blackouts",
            2: "Custom ground stations lack AWS integration and Kubernetes isn't optimized for autonomous space operations"
        },
        examStrategy: "For space applications: Ground Station provides specialized communication, Greengrass enables edge autonomy, ParallelCluster for HPC workloads. Consider communication delays and autonomous operation requirements."
    }
},
{
    id: 'sap_126',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A news website experiences massive traffic spikes during breaking news. Current setup: 50 EC2 instances behind ALB, RDS PostgreSQL, ElastiCache Redis, CloudFront CDN. Monthly costs: $100,000. Issues: scaling takes 10 minutes causing outages, database CPU hits 100% during spikes, Redis memory pressure, CDN cache hit ratio only 70%, and $30,000 unexpected data transfer costs during major events.",
    question: "Which optimization strategy BEST handles viral traffic while controlling costs?",
    options: [
        "Implement EC2 Auto Scaling with target tracking, add RDS read replicas, increase Redis cluster size, optimize CloudFront cache policies, and use Origin Shield to reduce data transfer costs",
        "Migrate to Lambda@Edge for dynamic content, implement Aurora Serverless v2 for automatic database scaling, use ElastiCache for Redis with auto-scaling, implement CloudFront Origin Shield for cost reduction, and optimize cache behaviors for 95% hit ratio",
        "Pre-scale EC2 instances before events, upgrade RDS instance, add Redis sharding, increase CloudFront TTLs, and purchase data transfer packages",
        "Containerize on EKS with cluster autoscaler, migrate to Aurora PostgreSQL, implement Redis clustering, add more CloudFront distributions, and negotiate CDN pricing"
    ],
    correct: 1,
    explanation: {
        correct: "Lambda@Edge eliminates cold start delays for dynamic content generation, Aurora Serverless v2 auto-scales database capacity preventing CPU spikes, ElastiCache auto-scaling handles Redis memory pressure, Origin Shield reduces data transfer costs by 85% during viral events, and optimized cache behaviors increase hit ratio to 95% reducing origin load.",
        whyWrong: {
            0: "Auto Scaling still takes minutes to scale, and adding read replicas doesn't solve write capacity issues",
            2: "Pre-scaling wastes money most of the time, and higher TTLs conflict with breaking news freshness",
            3: "EKS adds complexity without solving scaling speed, and Aurora PostgreSQL conversion is unnecessary"
        },
        examStrategy: "For viral traffic: Lambda@Edge provides instant scaling, Aurora Serverless v2 handles variable database load, Origin Shield dramatically reduces data transfer costs. Focus on automatic scaling vs manual intervention."
    }
},
{
    id: 'sap_127',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A defense contractor manages 200 classified projects across different security levels (Unclassified, Confidential, Secret, Top Secret). Requirements include: complete isolation between security levels, centralized identity management with security clearance integration, automated security scanning without data exposure, cost allocation per project and security level, and compliance with NIST 800-53 controls.",
    question: "Which architecture provides the strongest security isolation while enabling operational oversight?",
    options: [
        "AWS Organizations with separate OUs per security level, AWS Identity Center with clearance-based permission sets, AWS Security Hub with level-specific aggregation, AWS Cost Categories for project allocation, and AWS Config with NIST compliance packs",
        "Separate AWS Organizations per security level with no cross-level connectivity, Active Directory per level, isolated Security Hub instances, manual cost tracking, and separate compliance audits",
        "AWS GovCloud with Control Tower, clearance-based guardrails, Security Lake with delegated administration per level, automated cost allocation with detailed project tracking, and NIST 800-53 compliance automation with AWS Audit Manager",
        "Single Organization with strict IAM policies, SAML federation, centralized security monitoring, tagging for costs, and third-party compliance tools"
    ],
    correct: 2,
    explanation: {
        correct: "GovCloud provides the high-security environment required for classified work, Control Tower with clearance-based guardrails ensures proper isolation, Security Lake with delegated administration enables security oversight without cross-level data exposure, automated cost allocation handles complex project tracking requirements, and Audit Manager provides pre-built NIST 800-53 compliance frameworks.",
        whyWrong: {
            0: "Standard AWS regions may not meet classified data requirements for higher security levels",
            1: "Separate Organizations eliminate cost consolidation benefits and increase operational complexity",
            3: "Single Organization with IAM alone doesn't provide the physical isolation required for classified levels"
        },
        examStrategy: "For classified/defense workloads: GovCloud for compliance, delegated administration maintains isolation while enabling oversight, Audit Manager provides NIST frameworks. Physical isolation is critical for classified data."
    }
},
{
    id: 'sap_128',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A healthcare network with 100 clinics must migrate patient management systems to AWS. Current systems: Epic EHR, PACS imaging (50TB), HL7 interfaces to 500 medical devices, legacy Visual Basic applications, and SQL Server databases. Requirements: maintain patient care during migration, HIPAA compliance, real-time device integration, and complete within 18 months.",
    question: "Which migration approach ensures healthcare continuity while meeting compliance requirements?",
    options: [
        "AWS HealthLake for EHR data aggregation with FHIR compliance, AWS HealthImaging for PACS migration, AWS IoT Core for medical device integration, AWS Application Migration Service for VB applications, SQL Server on Amazon RDS with Multi-AZ, and AWS DataSync for phased data migration with validation",
        "Lift-and-shift Epic to EC2, manual PACS migration, custom device interfaces, rewrite VB applications, Aurora for databases, and weekend migration windows",
        "Keep Epic on-premises, cloud PACS solution, API Gateway for devices, modernize VB to web apps, PostgreSQL for data, and gradual migration",
        "Third-party healthcare cloud, managed PACS service, device cloud integration, SaaS alternatives to VB apps, managed databases, and professional services migration"
    ],
    correct: 0,
    explanation: {
        correct: "HealthLake aggregates EHR data with native FHIR support ensuring HIPAA compliance, HealthImaging provides DICOM-native PACS storage optimized for medical imaging, IoT Core handles HL7 device protocols at scale, MGN ensures seamless VB application migration, RDS SQL Server Multi-AZ maintains database availability, and DataSync enables validated phased migration ensuring data integrity throughout the process.",
        whyWrong: {
            1: "Manual PACS migration risks data loss, and weekend windows aren't sufficient for 50TB of medical imaging",
            2: "Keeping Epic on-premises doesn't achieve cloud benefits, and PostgreSQL conversion breaks medical workflows",
            3: "Third-party solutions add compliance complexity and may not integrate with existing Epic workflows"
        },
        examStrategy: "For healthcare migrations: HealthLake and HealthImaging are purpose-built for medical data. IoT Core handles HL7 protocols. MGN provides seamless application migration. Phased migration maintains continuity."
    }
},
{
    id: 'sap_129',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A quantum computing research consortium is building a hybrid classical-quantum platform for drug discovery. Requirements include: orchestrating quantum circuits across multiple hardware providers (IBM, Google, Rigetti), classical preprocessing of molecular data requiring 10,000 GPUs, real-time collaboration between 100 research institutions, quantum algorithm version control, results verification through classical simulation, and 10-year research data retention.",
    question: "Which architecture enables cutting-edge quantum-classical drug discovery research?",
    options: [
        "EC2 with GPU instances for classical compute, custom quantum APIs, S3 for collaboration, Git for version control, Batch for verification, and Glacier for archival",
        "ParallelCluster for GPU compute, third-party quantum services, EFS for shared storage, CodeCommit for algorithms, EMR for simulation, and S3 lifecycle policies",
        "Custom GPU clusters, direct quantum hardware access, NFS for collaboration, SVN for version control, on-premises simulation, and tape storage",
        "Amazon Braket for multi-provider quantum access, AWS ParallelCluster with P4d instances for molecular preprocessing, AWS CodeCommit with fine-grained access for algorithm IP protection, Amazon FSx for Lustre for research collaboration, AWS Batch for classical verification simulations, and S3 Intelligent-Tiering for 10-year retention"
    ],
    correct: 3,
    explanation: {
        correct: "Braket provides unified access to multiple quantum hardware providers (IBM, Google, Rigetti) with native AWS integration, ParallelCluster with P4d instances delivers massive GPU scale for molecular preprocessing, CodeCommit with IAM provides secure algorithm version control with IP protection, FSx for Lustre enables high-performance collaboration across institutions, Batch orchestrates classical verification at scale, and S3 Intelligent-Tiering optimizes long-term research data costs.",
        whyWrong: {
            0: "Custom quantum APIs lack the unified interface and don't provide queue management across different quantum hardware",
            1: "Third-party quantum services don't integrate well with AWS compute and storage for hybrid workflows",
            2: "Direct quantum hardware access is complex to manage and doesn't provide unified programming interface"
        },
        examStrategy: "Amazon Braket is AWS's quantum computing service supporting multiple hardware providers. For research computing: ParallelCluster for HPC, FSx for Lustre for collaboration, CodeCommit for secure version control."
    }
},
{
    id: 'sap_130',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A SaaS company serves 50,000 customers using shared Aurora PostgreSQL cluster costing $150,000/month. Issues: largest 100 customers cause 80% of database load, query timeouts during monthly billing runs, backup windows of 6 hours affecting availability, noisy neighbor effects between tenants, and difficulty scaling for 10x customer growth projections.",
    question: "Which database optimization strategy provides the BEST scalability and cost efficiency?",
    options: [
        "Add more Aurora read replicas, implement connection pooling, schedule billing during off-hours, use larger instances, and implement query optimization",
        "Implement a hybrid approach: Aurora dedicated clusters for top 100 customers, Aurora Serverless v2 shared clusters for remaining customers, DynamoDB for billing operations, automated tenant movement based on usage patterns, and Aurora Backtrack for instant recovery",
        "Migrate everything to DynamoDB, implement DAX for caching, use Lambda for billing, maintain single-table design, and implement global tables",
        "Keep shared cluster but add partitioning, implement RDS Proxy, use Aurora Global Database, add more storage, and optimize queries"
    ],
    correct: 1,
    explanation: {
        correct: "Hybrid approach isolates heavy customers preventing noisy neighbor issues, Aurora Serverless v2 provides cost-efficient scaling for smaller tenants, DynamoDB handles billing workloads without affecting transactional queries, automated movement enables customer lifecycle management, and Backtrack provides instant recovery without lengthy backup windows.",
        whyWrong: {
            0: "Adding resources doesn't solve fundamental noisy neighbor and architectural scaling issues",
            2: "Complete DynamoDB migration requires extensive application rewrite and loses PostgreSQL compatibility",
            3: "Shared cluster still suffers from noisy neighbor effects and doesn't solve billing impact issues"
        },
        examStrategy: "For multi-tenant database scaling: hybrid models balance cost and performance. Separate billing workloads from transactional systems. Aurora Serverless v2 provides automatic scaling for variable workloads."
    }
},
{
    id: 'sap_131',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational retailer with 5,000 stores operates 300 AWS accounts across regions. Requirements: real-time inventory synchronization globally, personalized pricing based on local regulations, cross-border financial reporting, supply chain visibility across vendors, protection of competitive pricing data, and compliance with 50+ country regulations including GDPR and data localization laws.",
    question: "Which solution enables global retail operations while meeting diverse regulatory requirements?",
    options: [
        "Single global AWS account with regional VPCs, DynamoDB Global Tables for inventory, Lambda for pricing logic, Redshift for reporting, and S3 for data lake",
        "AWS Organizations with regional OUs, EventBridge global replication for inventory events, Route 53 Application Recovery Controller for pricing failover, Cost and Usage Reports with DataZone for financial reporting, and Clean Rooms for vendor collaboration with data residency controls",
        "Regional AWS Organizations, manual inventory synchronization, local pricing systems, separate reporting per region, direct vendor integration, and country-specific compliance tools",
        "Hybrid cloud with on-premises inventory, cloud analytics, separate pricing engines, manual reporting consolidation, and third-party vendor platforms"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with regional OUs provides governance while respecting data residency, EventBridge global replication ensures real-time inventory sync across regions, Route 53 ARC provides automated regional pricing failover, DataZone enables governed financial reporting across borders while maintaining compliance, and Clean Rooms allows vendor collaboration without exposing competitive pricing data while enforcing data residency requirements.",
        whyWrong: {
            0: "Single global account violates data localization requirements and doesn't provide regional governance",
            2: "Regional Organizations break cost consolidation and manual sync doesn't meet real-time requirements",
            3: "Hybrid approach doesn't provide global visibility and manual processes don't scale to 5,000 stores"
        },
        examStrategy: "For global retail with data residency: regional OUs provide governance within single Organization, EventBridge enables global event replication, Clean Rooms protects competitive data while enabling collaboration."
    }
},
{
    id: 'sap_132',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A municipal water utility must migrate SCADA control systems and customer billing to AWS within 24 months. Current systems: Wonderware HMI for plant operations, Oracle database with 20 years of billing history, integration with 200 field sensors, Windows-based customer service applications, and regulatory reporting for EPA compliance.",
    question: "Which migration approach maintains critical utility operations while achieving modernization goals?",
    options: [
        "AWS IoT SiteWise Edge for SCADA data collection maintaining local control, AWS Database Migration Service for Oracle to Aurora PostgreSQL conversion, AWS Application Migration Service for Windows applications, AWS IoT Analytics for regulatory reporting, and hybrid connectivity ensuring operational continuity",
        "Migrate SCADA to IoT Core, lift-and-shift Oracle to RDS, containerize Windows apps, implement Kinesis for sensor data, and Direct Connect for reliability",
        "Keep SCADA on-premises, cloud-based billing system, API integration for sensors, web-based customer service, and VPN connectivity",
        "Replace SCADA with cloud-native IoT, rewrite billing system, modernize to microservices, implement real-time analytics, and edge computing for sensors"
    ],
    correct: 0,
    explanation: {
        correct: "IoT SiteWise Edge maintains critical SCADA operations locally ensuring safety while enabling cloud analytics, DMS with Aurora PostgreSQL provides managed database with better performance than Oracle, MGN ensures seamless Windows application migration, IoT Analytics provides purpose-built regulatory reporting capabilities, and hybrid connectivity ensures no service disruption during migration.",
        whyWrong: {
            1: "Direct IoT Core migration of SCADA creates single point of failure for critical utility operations",
            2: "Keeping SCADA on-premises doesn't achieve full modernization benefits",
            3: "Complete system replacement in 24 months is too risky for critical utility infrastructure"
        },
        examStrategy: "For utility SCADA migration: maintain local control for safety-critical systems, IoT SiteWise Edge enables hybrid operation. Never migrate safety systems directly to cloud. Hybrid approach ensures continuity."
    }
},
{
    id: 'sap_133',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous shipping company is designing a fleet management platform for 500 cargo ships operating globally. Requirements include: real-time navigation with satellite and weather data, autonomous collision avoidance using AI, predictive maintenance for engines and systems, integration with 1,000 ports worldwide, optimal route planning considering fuel costs and weather, and compliance with international maritime regulations.",
    question: "Which architecture enables intelligent autonomous maritime operations?",
    options: [
        "EC2 for ship communications, Lambda for navigation logic, RDS for ship data, API Gateway for port integration, and S3 for route storage",
        "IoT Core for ship connectivity, SageMaker for AI models, DynamoDB for real-time data, Direct Connect to ports, and Kinesis for streaming",
        "Custom satellite communication, on-premises AI processing, PostgreSQL for ship databases, VPN to ports, and file-based route exchange",
        "AWS Ground Station for satellite communication, AWS IoT FleetWise for ship systems integration, Amazon Location Service with maritime navigation data, SageMaker reinforcement learning for collision avoidance, Amazon Forecast with Weather Index for route optimization, AWS B2B Data Interchange for port system integration worldwide"
    ],
    correct: 3,
    explanation: {
        correct: "Ground Station provides global satellite coverage for ship communication, IoT FleetWise handles ship systems and maritime protocols, Location Service with maritime charts enables precise navigation, SageMaker RL adapts collision avoidance to dynamic conditions, Forecast with Weather Index optimizes routes considering weather patterns, and B2B Data Interchange handles diverse port EDI standards globally.",
        whyWrong: {
            0: "EC2 doesn't provide satellite communication capabilities and Lambda has limitations for complex navigation processing",
            1: "Direct Connect isn't available to ships at sea, and IoT Core alone lacks maritime-specific features",
            2: "Custom solutions require extensive development and don't provide managed maritime capabilities"
        },
        examStrategy: "For maritime applications: Ground Station provides satellite connectivity, IoT FleetWise handles vehicle/ship systems, Location Service supports maritime navigation. Weather Index in Forecast is ideal for shipping route optimization."
    }
},
{
    id: 'sap_134',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A video conferencing platform serves 10 million daily users with infrastructure costing $2M monthly: EC2 for media processing ($800K), S3 for recordings ($600K), CloudFront for delivery ($400K), RDS for metadata ($200K). Issues: media processing delays during peak hours, 90% of recordings never accessed after 30 days, CloudFront costs spike during events, database queries timeout during large meetings, and scaling takes 15 minutes.",
    question: "Which optimization strategy provides the MOST significant cost reduction and performance improvement?",
    options: [
        "Use larger EC2 instances, implement S3 lifecycle policies, add CloudFront behaviors, increase RDS capacity, and implement Auto Scaling",
        "Migrate to AWS Elemental MediaConvert for serverless transcoding, implement S3 Intelligent-Tiering moving 90% to Archive tiers, add CloudFront Origin Shield reducing costs 80%, migrate to Aurora Serverless v2 for automatic scaling, and implement Lambda@Edge for dynamic content optimization",
        "Containerize media processing on EKS, manually move old recordings to Glacier, optimize CloudFront configurations, add read replicas, and use Spot instances",
        "Keep current architecture but optimize settings, implement CDN compression, add database indexing, use Reserved Instances, and negotiate volume discounts"
    ],
    correct: 1,
    explanation: {
        correct: "MediaConvert provides serverless media processing eliminating 15-minute scaling delays and reducing costs through pay-per-use model, S3 Intelligent-Tiering automatically moves 90% of recordings to cheaper Archive tiers saving $540K annually, Origin Shield reduces CloudFront costs 80% during event spikes, Aurora Serverless v2 eliminates query timeouts through automatic scaling, and Lambda@Edge optimizes delivery reducing processing load.",
        whyWrong: {
            0: "Larger instances increase costs without solving architectural scaling issues",
            2: "Manual Glacier movement is error-prone and EKS adds complexity without solving core issues",
            3: "Optimizing current architecture doesn't address fundamental serverless opportunities and cost reduction potential"
        },
        examStrategy: "For video platforms: MediaConvert provides serverless transcoding, S3 Intelligent-Tiering automatically optimizes storage costs, Origin Shield dramatically reduces CDN costs. Serverless services eliminate scaling delays."
    }
},
{
    id: 'sap_135',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A consulting firm operates 100 client projects, each requiring isolated AWS environments with project-specific compliance (HIPAA, PCI, SOX). Requirements include: complete client data isolation, automated project provisioning and decommissioning, time-based access for consultants, centralized billing with client chargeback, and consultant restriction to single client environment at a time.",
    question: "Which governance model provides the strongest client isolation while maintaining operational efficiency?",
    options: [
        "AWS Organizations with nested OUs per client, manual account creation, IAM roles with external IDs, Cost Explorer for billing, and security policies for access control",
        "AWS Control Tower Account Factory for automated client provisioning, IAM Identity Center with time-bound exclusive access sessions, AWS Cost Categories for precise chargeback, compliance-specific guardrails per client, and Lambda automation for project lifecycle management",
        "Separate AWS Organizations per major client, Active Directory integration, consolidated billing, manual project tracking, and access request workflows",
        "Single Organization with shared accounts, resource tagging for isolation, automated billing allocation, Config rules for compliance, and time-based IAM policies"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower Account Factory enables automated compliant account provisioning for rapid project startup, IAM Identity Center with time-bound exclusive access prevents consultants from accessing multiple clients simultaneously, Cost Categories provide precise project-level chargeback to clients, compliance-specific guardrails ensure each project meets required standards, and Lambda automation ensures consistent project decommissioning.",
        whyWrong: {
            0: "Manual account creation is too slow for consulting project velocity and lacks compliance automation",
            2: "Separate Organizations eliminate billing consolidation benefits and increase management overhead",
            3: "Shared accounts violate client isolation requirements and resource tagging isn't sufficient for strong isolation"
        },
        examStrategy: "For consulting scenarios: Account Factory enables rapid provisioning, IAM Identity Center provides sophisticated access patterns, Cost Categories enable precise chargeback. Automation is key for project lifecycle management."
    }
},
{
    id: 'sap_136',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A insurance company must migrate their claims processing system to AWS within 12 months before mainframe contract expires. System includes: COBOL programs for claims logic, DB2 database with 30 years of claims history, integration with 100 state insurance departments, real-time fraud detection, and regulatory reporting. Peak processing: 100,000 claims daily.",
    question: "Which migration approach ensures claims processing continuity while meeting regulatory requirements?",
    options: [
        "AWS Mainframe Modernization with automated COBOL conversion, Amazon RDS Custom for DB2 compatibility, AWS B2B Data Interchange for state department integration, Amazon Fraud Detector for real-time detection, AWS DataSync for claims history migration, and AWS Audit Manager for insurance regulation compliance",
        "Manual COBOL rewrite to Java, Aurora PostgreSQL for database, REST APIs for state integration, custom fraud algorithms, Snowball for data transfer, and manual compliance processes",
        "Lift-and-shift COBOL to EC2, Oracle migration for DB2, VPN to state departments, machine learning on SageMaker, Database Migration Service for data, and Config for compliance",
        "Keep mainframe hybrid, gradual database migration, maintain existing integrations, enhance fraud detection, partial data migration, and phased compliance approach"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization with automated conversion preserves 30 years of claims logic while meeting 12-month deadline, RDS Custom maintains DB2 compatibility for complex procedures, B2B Data Interchange handles state department EDI formats natively, Fraud Detector provides superior ML-based detection, DataSync ensures complete claims history migration with validation, and Audit Manager provides insurance-specific compliance frameworks.",
        whyWrong: {
            1: "Manual COBOL rewrite in 12 months is impossible for complex claims processing with 30 years of business rules",
            2: "Oracle doesn't provide DB2 compatibility and would break existing stored procedures",
            3: "Hybrid approach doesn't solve mainframe contract expiration and maintains data center dependencies"
        },
        examStrategy: "For mainframe migrations with tight deadlines: automated modernization tools are essential. RDS Custom preserves database compatibility. B2B Data Interchange handles insurance industry EDI standards."
    }
},
{
    id: 'sap_137',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A renewable energy company is building an AI-powered grid management system for 10,000 solar farms and wind turbines. Requirements include: real-time energy production forecasting, automated load balancing across the grid, integration with 50 utility companies, predictive maintenance for equipment, energy trading optimization, and compliance with NERC reliability standards.",
    question: "Which architecture provides intelligent distributed energy management at scale?",
    options: [
        "IoT Core for device connectivity, Lambda for load balancing, RDS for energy data, API Gateway for utility integration, and CloudWatch for monitoring",
        "EC2 for control systems, Kinesis for data streaming, DynamoDB for real-time data, Direct Connect to utilities, and custom ML models",
        "Batch processing for forecasting, SQS for grid commands, S3 for historical data, VPN to utilities, and third-party trading platforms",
        "AWS IoT SiteWise for industrial energy asset management, Amazon Forecast with Weather Index for production prediction, AWS IoT Analytics for grid optimization, AWS B2B Data Interchange for utility integration, Amazon Lookout for Equipment for predictive maintenance, and AWS Supply Chain for energy trading optimization"
    ],
    correct: 3,
    explanation: {
        correct: "IoT SiteWise provides purpose-built industrial asset management for energy equipment at scale, Forecast with Weather Index delivers accurate renewable energy production prediction, IoT Analytics enables complex grid optimization algorithms, B2B Data Interchange handles diverse utility communication standards, Lookout for Equipment provides predictive maintenance for renewable energy assets, and Supply Chain optimizes energy trading operations.",
        whyWrong: {
            0: "Generic IoT Core lacks industrial features needed for grid management, and Lambda has limitations for complex load balancing",
            1: "EC2-based control systems don't scale to 10,000 assets efficiently and lack managed industrial features",
            2: "Batch processing can't provide real-time grid management, and third-party trading platforms lack AWS integration"
        },
        examStrategy: "For renewable energy grid management: IoT SiteWise for industrial assets, Forecast Weather Index for renewable prediction, Lookout for Equipment for industrial predictive maintenance. Supply Chain handles trading optimization."
    }
},
{
    id: 'sap_138',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A food delivery platform processes 5 million orders daily using: 300 EC2 instances for order processing, DynamoDB for order tracking (50,000 WCU/RCU), ElastiCache Redis for real-time driver locations, and API Gateway for mobile apps. Monthly costs: $400,000. Issues: 70% traffic is lunch/dinner peaks, EC2 idle 60% overnight, DynamoDB throttling during promotions, Redis memory pressure during rush hours.",
    question: "Which optimization strategy provides the BEST cost-performance improvement for variable food delivery traffic?",
    options: [
        "Implement EC2 Auto Scaling with scheduled actions, DynamoDB auto-scaling with burst capacity, ElastiCache cluster mode with auto-scaling, API Gateway caching for mobile responses, and Reserved Instances for baseline capacity",
        "Migrate to ECS Fargate Spot for order processing eliminating idle costs, implement DynamoDB on-demand for automatic promotion scaling, upgrade to MemoryDB with clustering for driver tracking, add CloudFront for mobile API acceleration, and use Savings Plans for predictable workloads",
        "Use larger EC2 instances, increase DynamoDB provisioned capacity, add Redis read replicas, implement API throttling, and negotiate volume discounts",
        "Containerize on EKS with KEDA autoscaling, keep DynamoDB provisioned, implement Redis sharding, add ALB for APIs, and use Spot instances"
    ],
    correct: 1,
    explanation: {
        correct: "Fargate Spot eliminates 60% idle EC2 costs while providing perfect scaling for rush hours, DynamoDB on-demand handles promotion spikes without pre-provisioning, MemoryDB clustering scales memory and provides Redis durability for critical driver location data, CloudFront caching reduces API Gateway costs and improves mobile performance, and Savings Plans optimize remaining predictable workloads.",
        whyWrong: {
            0: "Scheduled scaling helps but doesn't eliminate idle costs, and auto-scaling still requires baseline provisioning",
            2: "Larger instances increase costs without solving utilization problems, and over-provisioning wastes money",
            3: "EKS adds operational complexity and KEDA requires Kubernetes expertise vs simpler Fargate"
        },
        examStrategy: "For variable workloads with peak patterns: Fargate Spot + DynamoDB on-demand provides optimal cost-performance. MemoryDB offers Redis performance with managed durability. CloudFront improves mobile API performance."
    }
},
{
    id: 'sap_139',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global pharmaceutical company has 400 AWS accounts across research, clinical trials, manufacturing, and commercial divisions. Requirements include: segregation of research data by therapeutic area, cross-division collaboration for drug development, automated compliance with FDA, EMA, and 30+ country regulations, intellectual property protection, and unified reporting for regulatory submissions.",
    question: "Which governance architecture enables secure collaboration while protecting IP and ensuring compliance?",
    options: [
        "Single AWS Organization with therapeutic area OUs, IAM cross-account roles, S3 bucket policies for IP protection, manual compliance tracking, and custom reporting solutions",
        "AWS Organizations with nested OUs per division and therapeutic area, AWS Clean Rooms for IP-protected collaboration, AWS DataZone for governed data sharing, AWS Audit Manager for multi-regulatory compliance, Amazon Macie for automated IP classification, and AWS B2B Data Interchange for regulatory submission formats",
        "Separate AWS Organizations per therapeutic area, Active Directory federation, encrypted data sharing, third-party compliance platforms, and manual regulatory reporting",
        "AWS Control Tower with basic guardrails, standard IAM policies, S3 for data sharing, Security Hub for compliance, and QuickSight for reporting"
    ],
    correct: 1,
    explanation: {
        correct: "Nested OUs provide granular governance by division and therapeutic area, Clean Rooms enables secure multi-party analytics without exposing IP, DataZone provides governed data sharing with lineage tracking for regulatory requirements, Audit Manager automates evidence collection across 30+ regulatory frameworks, Macie automatically classifies and protects IP, and B2B Data Interchange handles various regulatory submission formats globally.",
        whyWrong: {
            0: "Basic S3 policies don't provide sophisticated IP protection needed for pharmaceutical research",
            2: "Separate Organizations break collaboration capabilities and increase management complexity",
            3: "Basic guardrails don't address pharmaceutical-specific compliance requirements and IP protection"
        },
        examStrategy: "For pharmaceutical governance: Clean Rooms protects IP during collaboration, DataZone provides data lineage for regulatory compliance, Audit Manager handles multiple regulatory frameworks. Nested OUs enable granular governance."
    }
},
{
    id: 'sap_140',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A regional airline must migrate reservation and operations systems to AWS within 14 months due to data center lease expiration. Systems include: passenger service system with 30 years of data, crew scheduling applications, maintenance tracking, baggage handling integration, and interfaces to global distribution systems (Amadeus, Sabre). Peak traffic: 50,000 bookings daily.",
    question: "Which migration strategy ensures airline operations continuity while meeting the deadline?",
    options: [
        "AWS Application Migration Service for passenger systems, Amazon RDS Custom for legacy database compatibility, AWS B2B Data Interchange for GDS integration, AWS IoT Core for baggage tracking, modernized crew scheduling on containerized platforms, and phased cutover approach ensuring zero downtime",
        "Lift-and-shift all systems to EC2, migrate databases to Aurora, build new GDS APIs, implement IoT for baggage, modernize crew systems, and parallel testing",
        "Replace passenger system with SaaS solution, keep crew scheduling on-premises, cloud baggage tracking, maintain GDS connections, selective modernization, and gradual migration",
        "Manual application migration, database replication to RDS, custom GDS integration, RFID baggage system, web-based crew scheduling, and weekend cutover windows"
    ],
    correct: 0,
    explanation: {
        correct: "MGN provides seamless migration of passenger systems preserving 30 years of operational data, RDS Custom maintains legacy database compatibility critical for airline operations, B2B Data Interchange handles GDS protocols natively, IoT Core scales baggage tracking to modern standards, containerized crew scheduling enables modernization, and phased cutover ensures continuous airline operations.",
        whyWrong: {
            1: "Aurora migration may break legacy airline database procedures and GDS integration complexity",
            2: "SaaS replacement doesn't work for airlines with extensive customization and regulatory requirements",
            3: "Manual migration is too risky for airline operations and weekend windows insufficient for complex systems"
        },
        examStrategy: "For airline migrations: preserve existing systems first, then modernize. B2B Data Interchange handles airline industry standards (GDS protocols). RDS Custom maintains database compatibility. Phased approach ensures continuity."
    }
},

// Questions 141-160 with pre-planned randomization: A(5), B(5), C(5), D(5)
{
    id: 'sap_141',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global telecommunications company operates 200 AWS accounts across 80 countries providing services to 500 million customers. Requirements include: data sovereignty compliance with each country's telecommunications laws, real-time network monitoring across all regions, unified billing for international roaming, secure sharing of network performance data with regulators, prevention of customer data crossing borders, and 99.999% availability for emergency services.",
    question: "Which architecture provides global telecom operations while ensuring regulatory compliance and data sovereignty?",
    options: [
        "AWS Organizations with country-specific OUs and data residency guardrails, Amazon CloudWatch cross-region dashboards for unified monitoring, AWS Clean Rooms for regulatory data sharing without exposing customer information, AWS DataZone for governed international roaming billing, and AWS Local Zones in major cities for emergency services ultra-low latency",
        "Single Organization with regional VPCs, basic CloudWatch monitoring, S3 bucket policies for data protection, consolidated billing, and CloudFront for performance",
        "Separate AWS Organizations per country, manual monitoring aggregation, individual billing systems, direct data sharing with regulators, and EC2 for emergency systems",
        "Hub-and-spoke architecture with Transit Gateway, centralized monitoring, manual billing reconciliation, API-based regulatory reporting, and Multi-AZ deployments"
    ],
    correct: 0,
    explanation: {
        correct: "Country-specific OUs with data residency guardrails ensure telecommunications law compliance automatically, CloudWatch cross-region provides unified monitoring without data movement, Clean Rooms enables regulatory sharing while protecting customer privacy, DataZone provides governed roaming billing with data lineage, and Local Zones deliver <10ms latency for emergency services mandated by telecommunications regulations.",
        whyWrong: {
            1: "Basic S3 policies don't provide sophisticated data sovereignty controls required for telecom regulations",
            2: "Separate Organizations break roaming billing capabilities and create management complexity for 200 accounts",
            3: "Transit Gateway doesn't address data sovereignty requirements and manual processes don't scale globally"
        },
        examStrategy: "For telecom/utilities: data sovereignty is critical, Clean Rooms enables regulatory compliance, Local Zones provide emergency services latency. DataZone ensures data governance across borders."
    }
},
{
    id: 'sap_142',
    domain: "Domain 2: Design for New Solutions", 
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A biotechnology startup is developing a personalized cancer treatment platform requiring analysis of patient tumor genomes (3 billion base pairs), real-time liquid biopsy monitoring, AI-driven drug combination predictions, clinical trial matching for 50,000 patients, HIPAA compliance with 99.99% data durability, and integration with 500 oncology practices nationwide.",
    question: "Which architecture enables precision oncology while maintaining compliance and scalability?",
    options: [
        "S3 for genomic storage, EMR for analysis, Lambda for processing, RDS for patient data, and API Gateway for practice integration",
        "Amazon Omics for genomic analysis workflows, AWS HealthLake for HIPAA-compliant patient data with FHIR support, Amazon SageMaker for AI drug predictions with healthcare compliance, AWS IoT Core for liquid biopsy device integration, Amazon Comprehend Medical for clinical trial matching, and AWS Clean Rooms for secure practice collaboration",
        "Custom bioinformatics on EC2, DynamoDB for patient records, Batch for processing, Kinesis for device data, and Direct Connect to practices",
        "Redshift for genomic data, Aurora for patient information, Step Functions for workflows, MSK for real-time data, and VPN connectivity"
    ],
    correct: 1,
    explanation: {
        correct: "Amazon Omics provides purpose-built genomic analysis optimized for cancer research workflows, HealthLake ensures HIPAA compliance with native FHIR support for oncology standards, SageMaker with healthcare features enables AI drug predictions while maintaining compliance, IoT Core handles liquid biopsy devices at scale, Comprehend Medical extracts clinical insights for trial matching, and Clean Rooms enables secure collaboration with practices without exposing patient data.",
        whyWrong: {
            0: "Generic EMR lacks genomics-specific optimizations and doesn't provide HIPAA-compliant healthcare data management",
            2: "Custom bioinformatics requires extensive development and DynamoDB lacks healthcare compliance features",
            3: "Redshift isn't optimized for genomic data analysis and MSK adds unnecessary complexity for healthcare workflows"
        },
        examStrategy: "For genomics/healthcare: Amazon Omics for genomic analysis, HealthLake for HIPAA compliance, Clean Rooms for secure healthcare collaboration. Comprehend Medical extracts clinical insights from unstructured data."
    }
},
{
    id: 'sap_143',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard", 
    timeRecommendation: 180,
    scenario: "A cryptocurrency exchange processes 5 million trades daily with infrastructure costs of $1.2M monthly: DynamoDB ($400K), EC2 for matching engine ($500K), ElastiCache ($200K), API Gateway ($100K). Issues include: DynamoDB hot partitions during market volatility, 20ms latency between matching engine components, Redis memory exhaustion during flash crashes, API throttling during high-volume trading, and inability to scale beyond current limits.",
    question: "Which optimization strategy provides the BEST performance improvement for high-frequency cryptocurrency trading?",
    options: [
        "Add more DynamoDB capacity, upgrade to larger EC2 instances, increase Redis cluster size, raise API Gateway limits, and implement caching",
        "Migrate to Aurora for better consistency, containerize matching engine, add Redis read replicas, implement API caching, and use Reserved Instances",
        "Implement DynamoDB Accelerator (DAX) for microsecond latency eliminating hot partitions, deploy matching engine on EC2 instances in Placement Groups for ultra-low network latency, migrate to Amazon MemoryDB for durable sub-millisecond trading state, implement API Gateway regional endpoints with caching, and add Amazon MemoryDB clustering for horizontal scaling during flash crashes",
        "Switch to single-table DynamoDB design, use Graviton-based instances, implement ElastiCache for Redis clustering, add CloudFront for APIs, and optimize application code"
    ],
    correct: 2,
    explanation: {
        correct: "DAX provides consistent microsecond access eliminating DynamoDB hot partition issues during volatility, Placement Groups minimize network latency to sub-millisecond levels critical for HFT, MemoryDB offers Redis performance with Multi-AZ durability preventing data loss during crashes, API Gateway regional endpoints with caching handle high-volume spikes, and MemoryDB clustering scales horizontally for unlimited capacity during flash crashes.",
        whyWrong: {
            0: "Simply adding capacity doesn't solve hot partition architecture issues or network latency between components",
            1: "Aurora adds unnecessary complexity for trading systems and containerization can introduce latency",
            3: "CloudFront adds latency for real-time trading APIs and single-table design doesn't solve hot partition issues"
        },
        examStrategy: "For cryptocurrency/HFT: DAX eliminates hot partitions, Placement Groups provide lowest latency, MemoryDB combines performance with durability. Never sacrifice latency for other concerns in trading systems."
    }
},
{
    id: 'sap_144',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A federal government agency must migrate classified defense systems to AWS GovCloud within 18 months. Systems include: mainframe processing satellite imagery, classified databases with 40 years of intelligence data, real-time communications with field operations, integration with allied nation systems, and compliance with FISMA High and IL5 requirements. Zero data loss and continuous operations are mandatory.",
    question: "Which migration approach maintains national security while achieving modernization goals?",
    options: [
        "Lift-and-shift mainframe to EC2, replicate databases to RDS, maintain existing communications, build new integrations, and implement security controls",
        "Replace all systems with cloud-native architecture, implement modern databases, upgrade communication systems, develop new integration APIs, and apply security frameworks",
        "Keep critical systems on-premises, migrate non-classified components, implement hybrid connectivity, maintain existing integrations, and gradual modernization approach",
        "AWS Mainframe Modernization with dual-run approach for zero-downtime satellite processing, AWS DataSync with encryption for classified database migration, AWS B2B Data Interchange for allied nation integration, AWS Ground Station for field communications, AWS GovCloud with IL5 compliance controls, and AWS CloudTrail Lake for FISMA audit requirements"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization with dual-run ensures continuous satellite imagery processing critical for national security, DataSync with encryption maintains data classification during migration, B2B Data Interchange handles complex allied nation protocols securely, Ground Station provides secure field communications infrastructure, GovCloud IL5 compliance meets defense requirements, and CloudTrail Lake provides FISMA High audit trails.",
        whyWrong: {
            0: "Simple lift-and-shift doesn't modernize capabilities and may not meet IL5 security requirements",
            1: "Complete replacement in 18 months is impossible for 40 years of classified systems and risks national security",
            2: "Hybrid approach doesn't achieve full modernization goals and maintains data center dependencies"
        },
        examStrategy: "For classified government migration: dual-run maintains security, GovCloud provides required compliance, B2B Data Interchange handles international protocols. Ground Station provides secure communications infrastructure."
    }
},
{
    id: 'sap_145',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational mining company operates 150 remote mines across 40 countries with limited internet connectivity. Requirements include: real-time safety monitoring with immediate emergency response, predictive maintenance for mining equipment, environmental compliance reporting to multiple governments, secure data sharing with joint venture partners, and operational cost optimization across all sites.",
    question: "Which architecture enables global mining operations with intermittent connectivity and multi-partner collaboration?",
    options: [
        "AWS IoT SiteWise Edge for local safety monitoring with autonomous emergency protocols, AWS Outposts for remote site compute resilience, Amazon Lookout for Equipment for predictive maintenance, AWS Clean Rooms for secure joint venture data sharing, AWS B2B Data Interchange for government environmental reporting, and AWS Snow Family for periodic data synchronization during connectivity outages",
        "IoT Core for device connectivity, CloudWatch for monitoring, Lambda for emergency response, S3 for data storage, and VPN for partner access",
        "Edge computing with local servers, satellite connectivity, manual emergency procedures, file-based partner sharing, and quarterly government reporting",
        "AWS Wavelength for edge processing, real-time streaming to cloud, automated emergency systems, API-based partner integration, and automated compliance reporting"
    ],
    correct: 0,
    explanation: {
        correct: "IoT SiteWise Edge provides industrial safety monitoring with local autonomy during outages, Outposts ensures compute availability in remote locations, Lookout for Equipment predicts failures on critical mining equipment, Clean Rooms enables secure joint venture collaboration without exposing proprietary data, B2B Data Interchange handles diverse government reporting formats globally, and Snow Family provides reliable data sync when connectivity is poor.",
        whyWrong: {
            1: "IoT Core requires consistent connectivity not available in remote mines, and Lambda can't handle emergency response during outages",
            2: "Manual processes don't scale to 150 sites and file-based sharing lacks security for joint ventures",
            3: "Wavelength isn't available in remote mining locations and real-time streaming requires reliable connectivity"
        },
        examStrategy: "For remote industrial operations: IoT SiteWise Edge enables offline operation, Outposts provides compute resilience, Snow Family handles intermittent connectivity. Clean Rooms protects proprietary data in partnerships."
    }
},
{
    id: 'sap_146',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart agriculture company is building an AI-powered farming platform for 100,000 farms globally. Requirements include: satellite imagery analysis for crop health monitoring, soil sensor data processing from 10 million IoT devices, weather-based irrigation optimization, pest detection using drone cameras, supply chain coordination with 1,000 food processors, and compliance with organic certification standards in 50 countries.",
    question: "Which architecture provides comprehensive precision agriculture capabilities at global scale?",
    options: [
        "EC2 for image processing, Kinesis for sensor data, Lambda for irrigation, Rekognition for pest detection, API Gateway for supply chain, and manual compliance tracking",
        "Amazon SageMaker Geospatial for satellite crop analysis, AWS IoT Core with Basic Ingest for cost-effective sensor processing, Amazon Forecast with Weather Index for irrigation optimization, Amazon Rekognition Custom Labels for pest detection, AWS Supply Chain for food processor coordination, and AWS Audit Manager for organic certification compliance across multiple countries",
        "Custom ML models on GPU instances, DynamoDB for sensor storage, Step Functions for automation, computer vision on EC2, Direct Connect to processors, and third-party compliance platforms",
        "Batch processing for satellite data, Timestream for sensors, SageMaker for predictions, custom drone analysis, MSK for supply chain events, and Config for compliance"
    ],
    correct: 1,
    explanation: {
        correct: "SageMaker Geospatial provides purpose-built satellite imagery analysis for agriculture, IoT Core Basic Ingest reduces costs for 10M sensors while maintaining reliability, Forecast with Weather Index optimizes irrigation using integrated weather data, Rekognition Custom Labels enables training specific pest detection models, Supply Chain provides end-to-end coordination with food processors, and Audit Manager automates organic certification compliance across 50 countries.",
        whyWrong: {
            0: "Generic EC2 processing lacks agricultural-specific ML capabilities and manual compliance doesn't scale globally",
            2: "Custom models require extensive ML expertise and DynamoDB is expensive for 10M device time-series data",
            3: "Batch processing can't provide real-time irrigation optimization and custom drone analysis lacks managed features"
        },
        examStrategy: "For precision agriculture: SageMaker Geospatial for satellite analysis, IoT Core Basic Ingest for cost-effective IoT, Supply Chain for end-to-end coordination. Weather Index provides integrated weather data for agriculture."
    }
},
{
    id: 'sap_147',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global logistics company tracks 10 million packages daily using: RDS PostgreSQL cluster ($150K/month), 200 EC2 instances for tracking updates ($100K/month), S3 for shipping documents ($50K/month), CloudFront for customer portal ($30K/month). Issues include: database queries taking 10+ seconds during peak shipping seasons, 50% EC2 idle capacity overnight, document retrieval delays, and portal timeouts during holiday rushes.",
    question: "Which optimization strategy addresses all performance issues while achieving significant cost reduction?",
    options: [
        "Add RDS read replicas, implement Auto Scaling, enable S3 Transfer Acceleration, increase CloudFront cache TTLs, and purchase Reserved Instances",
        "Migrate to Aurora Global Database, containerize on EKS, implement S3 lifecycle policies, optimize CloudFront behaviors, and use Savings Plans",
        "Migrate to Aurora Serverless v2 for automatic scaling during peak seasons, implement Lambda with DynamoDB for package tracking state eliminating idle EC2 costs, use S3 Intelligent-Tiering for automatic document optimization, implement CloudFront Origin Shield and compression to reduce portal timeouts, and add ElastiCache for frequently accessed tracking data",
        "Keep current setup but optimize configurations, implement connection pooling, add CDN compression, use Spot instances for batch processing, and negotiate enterprise pricing"
    ],
    correct: 2,
    explanation: {
        correct: "Aurora Serverless v2 automatically scales for peak seasons eliminating 10+ second queries while reducing overnight costs, Lambda with DynamoDB provides real-time tracking without EC2 idle costs, S3 Intelligent-Tiering automatically optimizes document storage reducing retrieval delays, Origin Shield with compression prevents portal timeouts during holiday traffic, and ElastiCache provides sub-second access to popular tracking data.",
        whyWrong: {
            0: "Adding read replicas increases costs and doesn't solve peak season write performance issues",
            1: "Aurora Global Database is expensive overkill and EKS doesn't eliminate fundamental idle capacity issues",
            3: "Optimizing current architecture doesn't solve fundamental scaling and idle capacity problems"
        },
        examStrategy: "For logistics with variable load: Aurora Serverless v2 handles seasonal peaks, Lambda eliminates idle costs, S3 Intelligent-Tiering automates optimization. Origin Shield prevents CDN origin overload during traffic spikes."
    }
},
{
    id: 'sap_148',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state transportation department must migrate traffic management systems to AWS within 20 months. Current systems include: SCADA controlling 5,000 traffic signals, incident management with emergency responders, toll collection processing 50 million transactions monthly, integration with 200 local jurisdictions, and compliance with federal transportation regulations. System downtime during rush hours is unacceptable.",
    question: "Which migration approach maintains traffic safety while achieving modernization goals?",
    options: [
        "Migrate SCADA to IoT Core, containerize incident management, move toll processing to Lambda, implement API Gateway for jurisdictions, and parallel testing approach",
        "Keep SCADA on-premises with hybrid connectivity, modernize incident management on AWS, migrate toll processing to managed services, implement B2B Data Interchange for jurisdictions, and phased migration ensuring zero downtime",
        "Replace SCADA with cloud-native IoT, build new incident system, implement serverless toll processing, create unified jurisdiction portal, and weekend migration windows",
        "AWS IoT SiteWise Edge for traffic signal control maintaining local operation during outages, Amazon Connect for emergency responder coordination, AWS Batch for toll transaction processing, AWS B2B Data Interchange for jurisdiction integration, and dual-run approach ensuring continuous traffic management"
    ],
    correct: 3,
    explanation: {
        correct: "IoT SiteWise Edge maintains local traffic signal control during connectivity issues ensuring public safety, Amazon Connect provides scalable emergency responder communication, Batch efficiently processes 50M monthly toll transactions, B2B Data Interchange handles diverse jurisdiction data formats, and dual-run approach ensures zero downtime during critical infrastructure migration.",
        whyWrong: {
            0: "Direct IoT Core migration creates single point of failure for traffic signals endangering public safety",
            1: "Hybrid approach doesn't fully modernize and maintains data center dependencies for critical systems",
            2: "Complete replacement risks public safety and weekend windows are insufficient for traffic infrastructure"
        },
        examStrategy: "For traffic/public safety systems: maintain local control for safety-critical functions, IoT SiteWise Edge enables hybrid operation. Amazon Connect handles emergency communications. Never migrate safety systems without local backup."
    }
},
{
    id: 'sap_149',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment firm with 500 AWS accounts manages $2 trillion in assets across hedge funds, private equity, and institutional clients. Requirements include: complete fund isolation for regulatory compliance, real-time risk monitoring across all portfolios, automated compliance with SEC, FCA, and 20+ international regulations, secure data sharing with audit firms, and cost allocation to individual funds and strategies.",
    question: "Which governance architecture provides regulatory compliance while enabling efficient operations?",
    options: [
        "AWS Organizations with fund-specific OUs and automated compliance guardrails, Amazon FinSpace for financial data management with built-in risk analytics, AWS Clean Rooms for secure auditor collaboration without exposing trading strategies, AWS Cost Categories with automated fund-level allocation, AWS Audit Manager for multi-regulatory compliance frameworks, and AWS Security Lake for unified threat detection across all funds",
        "Separate Organizations per fund type, manual risk monitoring, individual compliance processes, direct auditor access, spreadsheet cost tracking, and isolated security monitoring",
        "Single Organization with IAM policies, CloudWatch for monitoring, manual compliance tracking, S3 for data sharing, tagging for costs, and GuardDuty for security",
        "Hub-and-spoke architecture, centralized databases, shared compliance systems, API-based auditor access, automated billing, and Security Hub federation"
    ],
    correct: 0,
    explanation: {
        correct: "Fund-specific OUs with compliance guardrails ensure regulatory isolation, FinSpace provides purpose-built financial analytics with risk monitoring for investment management, Clean Rooms enables auditor collaboration while protecting proprietary trading strategies, Cost Categories automate complex fund allocation requirements, Audit Manager handles multiple international regulatory frameworks, and Security Lake provides unified threat detection without compromising fund isolation.",
        whyWrong: {
            1: "Separate Organizations break operational efficiency and cost consolidation for $2T asset management",
            2: "Basic IAM policies don't provide sophisticated financial compliance and isolation required for investment management",
            3: "Hub-and-spoke doesn't provide fund isolation required by financial regulations"
        },
        examStrategy: "For investment management: FinSpace provides financial industry-specific features, Clean Rooms protects proprietary strategies, Audit Manager handles multiple regulatory frameworks. Fund isolation is critical for compliance."
    }
},
{
    id: 'sap_150',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A renewable energy consortium is building a peer-to-peer energy trading platform connecting 1 million residential solar installations, 10,000 commercial wind farms, and 500 utility companies. Requirements include: real-time energy price matching with <100ms latency, blockchain-based settlement for transparency, weather-based production forecasting, grid stability monitoring, regulatory compliance with energy market rules in 50 states, and carbon credit tracking.",
    question: "Which architecture enables decentralized renewable energy trading at massive scale?",
    options: [
        "DynamoDB for price matching, custom blockchain on EC2, Lambda for forecasting, CloudWatch for monitoring, API Gateway for regulation, and manual carbon tracking",
        "Amazon MemoryDB for ultra-fast energy price matching, Amazon Managed Blockchain for transparent settlement, Amazon Forecast with Weather Index for production prediction, AWS IoT SiteWise for grid monitoring, AWS B2B Data Interchange for multi-state regulatory compliance, and Amazon QLDB for immutable carbon credit ledger with regulatory audit trails",
        "ElastiCache for pricing, Ethereum on containers, SageMaker for predictions, IoT Core for grid data, Direct Connect to regulators, and DynamoDB for carbon credits",
        "RDS for energy data, third-party blockchain, EMR for analytics, Kinesis for grid streams, VPN to utilities, and S3 for carbon documentation"
    ],
    correct: 1,
    explanation: {
        correct: "MemoryDB provides consistent <100ms latency for 1M+ energy price matching operations, Managed Blockchain offers enterprise-grade blockchain with regulatory compliance for energy settlements, Forecast with Weather Index delivers accurate renewable production prediction, IoT SiteWise monitors grid stability with industrial protocols, B2B Data Interchange handles diverse state regulatory formats, and QLDB provides immutable carbon credit tracking with cryptographic verification for compliance.",
        whyWrong: {
            0: "Custom blockchain lacks enterprise features needed for regulatory compliance and manual tracking doesn't scale",
            2: "Ethereum containers require significant management and ElastiCache alone can't handle 1M+ concurrent price matching",
            3: "Third-party blockchain adds complexity and VPN doesn't scale to 500 utility integrations"
        },
        examStrategy: "For energy trading: MemoryDB provides ultra-low latency, Managed Blockchain offers enterprise features vs custom solutions. Weather Index is ideal for renewable forecasting. QLDB provides immutable ledgers for carbon credits."
    }
},
{
    id: 'sap_151',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A social media platform with 100 million daily active users operates on: 1,000 EC2 instances for content processing ($600K/month), OpenSearch cluster on 30 i3.8xlarge nodes for search ($300K/month), RDS PostgreSQL for user data ($200K/month), Redis cluster for real-time features ($150K/month). Issues include: search response times >5 seconds, content processing backlogs during viral events, database connection exhaustion, Redis memory pressure during peak hours, and 40% infrastructure idle overnight.",
    question: "Which optimization provides the MOST comprehensive performance and cost improvement?",
    options: [
        "Add more OpenSearch nodes, scale EC2 instances, increase RDS connections, expand Redis cluster, and implement Reserved Instances for cost savings",
        "Containerize on EKS with autoscaling, migrate to Amazon OpenSearch Serverless, add Aurora read replicas, implement Redis clustering, and use Spot instances for development",
        "Migrate to Amazon OpenSearch Serverless for automatic scaling and machine learning-powered search relevance, implement Lambda with SQS for elastic content processing during viral events, migrate to Aurora Serverless v2 for automatic database scaling, replace Redis with Amazon MemoryDB for durability and clustering, and implement CloudFront for edge caching reducing backend load",
        "Optimize OpenSearch configurations, implement EC2 Auto Scaling, add connection pooling, use ElastiCache auto-scaling, and purchase Savings Plans"
    ],
    correct: 2,
    explanation: {
        correct: "OpenSearch Serverless eliminates cluster management while ML-powered search reduces response time to <1 second, Lambda with SQS provides infinite scaling for viral content processing without maintaining idle EC2 capacity, Aurora Serverless v2 auto-scales database preventing connection exhaustion, MemoryDB provides Redis performance with Multi-AZ durability and clustering for peak loads, and CloudFront edge caching reduces backend processing by 70%.",
        whyWrong: {
            0: "Adding more resources increases costs without solving architectural inefficiencies and idle capacity issues",
            1: "EKS adds operational complexity and doesn't address fundamental serverless opportunities for cost optimization",
            3: "Optimizing current architecture doesn't eliminate idle capacity or provide auto-scaling benefits of serverless"
        },
        examStrategy: "For social media optimization: OpenSearch Serverless with ML improves search while reducing management, Lambda eliminates idle costs, Aurora Serverless v2 handles variable database load. MemoryDB provides Redis with durability."
    }
},
{
    id: 'sap_152',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national postal service must modernize package tracking and logistics systems within 15 months due to competitive pressure. Current systems include: mainframe for package routing with 40 years of postal code data, Oracle databases for customer information, integration with international postal systems, handheld scanner fleet management, and regulatory compliance with postal service regulations in 25 countries.",
    question: "Which modernization approach maintains postal operations while enabling competitive capabilities?",
    options: [
        "Manual mainframe migration, Aurora for customer data, REST APIs for international systems, IoT Core for scanners, and basic compliance monitoring",
        "Keep mainframe with cloud connectivity, RDS for Oracle, custom international integration, device management on EC2, and manual compliance processes",
        "Replace mainframe with microservices, PostgreSQL for all data, GraphQL for integration, modern scanner apps, and automated compliance",
        "AWS Mainframe Modernization with automated postal routing logic conversion, Amazon Location Service for optimized delivery route calculation, AWS IoT Device Management for handheld scanner fleet, AWS B2B Data Interchange for international postal integration, RDS Custom for Oracle maintaining postal procedures, and AWS Audit Manager for multi-country postal regulation compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 40 years of postal routing logic while enabling cloud benefits, Location Service provides optimized delivery routing with traffic and address data, IoT Device Management handles scanner fleet at scale, B2B Data Interchange manages international postal EDI standards efficiently, RDS Custom maintains Oracle postal procedures, and Audit Manager automates compliance across 25 countries' postal regulations.",
        whyWrong: {
            0: "Manual migration in 15 months is unrealistic for complex postal systems with 40 years of routing logic",
            1: "Hybrid approach doesn't achieve competitive modernization goals and maintains legacy limitations",
            2: "Complete rewrite risks postal operations and loses decades of optimized routing knowledge"
        },
        examStrategy: "For postal service modernization: preserve routing logic with automated modernization, Location Service optimizes delivery routes. IoT Device Management handles fleet scale. B2B Data Interchange manages postal industry standards."
    }
},
{
    id: 'sap_153',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A healthcare network with 50 hospitals operates separate AWS accounts per facility for patient privacy. Requirements include: unified emergency response coordination across hospitals, shared medical research while protecting patient privacy, centralized IT security monitoring, real-time bed availability tracking, integration with ambulance services, and compliance with HIPAA across all facilities.",
    question: "Which solution enables healthcare collaboration while maintaining patient privacy and HIPAA compliance?",
    options: [
        "AWS Organizations with healthcare-specific guardrails, AWS HealthLake for FHIR-compliant emergency coordination, AWS Clean Rooms for privacy-preserving medical research, Amazon Security Lake for unified threat detection, AWS IoT Core for real-time bed tracking, and AWS B2B Data Interchange for ambulance service integration with HL7 standards",
        "Merge all hospitals into single account, implement resource-based policies, use S3 for medical data, GuardDuty for security, custom bed tracking, and manual ambulance coordination",
        "VPC peering between hospitals, shared databases for research, centralized logging, custom applications for beds, API Gateway for ambulances, and manual HIPAA compliance",
        "Keep separate accounts with no integration, individual research projects, isolated security monitoring, separate bed systems, phone-based ambulance coordination, and facility-level compliance"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations provides governance while preserving account isolation for HIPAA, HealthLake enables FHIR-compliant emergency coordination across hospitals, Clean Rooms allows medical research collaboration without exposing patient data, Security Lake provides unified monitoring without data commingling, IoT Core scales bed tracking across 50 hospitals, and B2B Data Interchange handles HL7 ambulance service protocols.",
        whyWrong: {
            1: "Merging hospital accounts violates HIPAA patient data isolation requirements",
            2: "VPC peering with shared databases creates potential HIPAA violations and data leakage risks",
            3: "Complete isolation prevents emergency coordination and research collaboration beneficial for patient care"
        },
        examStrategy: "For healthcare multi-account: HealthLake enables FHIR compliance, Clean Rooms preserves privacy in research, Security Lake aggregates without exposing patient data. B2B Data Interchange handles healthcare protocols."
    }
},
{
    id: 'sap_154',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A space tourism company is building mission control for commercial space flights carrying 100 passengers per flight. Requirements include: real-time spacecraft telemetry monitoring, passenger health tracking during weightlessness, automated emergency response systems, communication with ground stations worldwide, flight trajectory optimization, regulatory compliance with FAA commercial space regulations, and 25-year flight data retention for safety analysis.",
    question: "Which architecture provides comprehensive space tourism mission control capabilities?",
    options: [
        "EC2 for mission control, IoT Core for spacecraft data, Lambda for health monitoring, Direct Connect to ground stations, and S3 for flight data",
        "AWS Ground Station for spacecraft communication with global coverage, AWS IoT FleetWise for spacecraft systems telemetry, Amazon HealthLake for passenger medical monitoring with HIPAA compliance, AWS Lambda for automated emergency protocols, Amazon Location Service for trajectory optimization, AWS Audit Manager for FAA commercial space compliance, and S3 Intelligent-Tiering with Glacier Deep Archive for 25-year safety data retention",
        "Custom ground stations, Kinesis for telemetry, DynamoDB for health data, Step Functions for emergencies, custom trajectory calculations, manual compliance tracking, and tape storage",
        "Satellite modems on EC2, MSK for spacecraft data, RDS for passenger information, Batch for emergency processing, third-party flight planning, Config for compliance, and EFS for archives"
    ],
    correct: 1,
    explanation: {
        correct: "Ground Station provides global spacecraft communication infrastructure, IoT FleetWise handles complex spacecraft systems telemetry, HealthLake ensures HIPAA-compliant passenger medical monitoring, Lambda provides real-time automated emergency response, Location Service optimizes flight trajectories with Earth's gravitational models, Audit Manager automates FAA commercial space compliance, and S3 Intelligent-Tiering with Deep Archive optimizes 25-year retention costs.",
        whyWrong: {
            0: "Generic IoT Core lacks spacecraft-specific communication protocols and Direct Connect doesn't provide global coverage",
            2: "Custom ground stations lack global redundancy and manual compliance doesn't meet commercial space requirements",
            3: "MSK adds unnecessary complexity for spacecraft telemetry and third-party flight planning lacks space-specific optimization"
        },
        examStrategy: "For space applications: Ground Station provides spacecraft communications, IoT FleetWise handles complex vehicle systems, HealthLake for medical compliance. Location Service supports trajectory calculations with gravitational modeling."
    }
},
{
    id: 'sap_155',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A financial trading platform processes 2 million transactions daily with costs of $800K/month: DynamoDB ($300K), Lambda ($200K), API Gateway ($150K), CloudFront ($100K), S3 ($50K). Issues include: DynamoDB throttling during market open, Lambda timeout errors during high-volume periods, API Gateway 5xx errors during volatility, CloudFront cache misses for dynamic pricing, and exponential cost growth with trading volume.",
    question: "Which optimization strategy provides the BEST cost control and performance improvement for high-volume trading?",
    options: [
        "Increase DynamoDB provisioned capacity, use Lambda reserved concurrency, raise API Gateway throttling limits, optimize CloudFront TTLs, and monitor S3 usage patterns",
        "Migrate to Aurora for consistency, containerize Lambda functions, implement API caching, add CloudFront behaviors, and use S3 lifecycle policies",
        "Implement DynamoDB on-demand for automatic scaling during market events, migrate to Lambda with provisioned concurrency for consistent performance, add API Gateway regional endpoints with caching, implement CloudFront with Lambda@Edge for dynamic pricing optimization, and use S3 Intelligent-Tiering for automatic cost optimization",
        "Add DynamoDB global tables, implement Step Functions, use Application Load Balancer, deploy multiple CloudFront distributions, and compress S3 objects"
    ],
    correct: 2,
    explanation: {
        correct: "DynamoDB on-demand automatically handles market volatility without pre-provisioning while providing cost predictability, Lambda provisioned concurrency eliminates timeout errors during high-volume trading, API Gateway regional endpoints with caching handle volatility spikes while reducing costs, Lambda@Edge processes dynamic pricing at edge reducing origin load, and S3 Intelligent-Tiering automatically optimizes storage costs as trading data ages.",
        whyWrong: {
            0: "Pre-provisioning DynamoDB capacity leads to over-provisioning costs and doesn't handle unpredictable market spikes",
            1: "Aurora adds unnecessary complexity for trading systems optimized for DynamoDB's performance characteristics",
            3: "Global tables are expensive overkill and Step Functions add latency inappropriate for real-time trading"
        },
        examStrategy: "For trading platforms: DynamoDB on-demand handles market volatility cost-effectively, Lambda provisioned concurrency prevents timeouts. Lambda@Edge processes dynamic pricing at edge for performance."
    }
},
{
    id: 'sap_156',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A regional bank must migrate core banking systems to AWS within 18 months to reduce operational costs by 40%. Current systems include: IBM z/OS mainframe with customer accounts, Oracle RAC for transaction processing, Windows-based teller applications, ATM network management, and integration with Federal Reserve payment systems. Downtime during business hours is unacceptable.",
    question: "Which migration strategy achieves cost reduction while maintaining banking operations continuity?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora, teller apps to containers, custom ATM management, and API-based Federal Reserve integration",
        "Keep mainframe hybrid, Oracle migration to RDS, modernize teller systems, cloud ATM monitoring, and maintain existing Federal Reserve connections",
        "Replace core banking with SaaS, PostgreSQL for transactions, web-based teller applications, IoT for ATMs, and modern payment APIs",
        "AWS Mainframe Modernization for z/OS with automated account system conversion, Amazon RDS Custom for Oracle RAC maintaining banking procedures, AWS Application Migration Service for seamless teller application migration, AWS IoT Device Management for ATM fleet monitoring, and AWS B2B Data Interchange for Federal Reserve payment system integration"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization achieves 40% cost reduction while preserving banking logic, RDS Custom maintains Oracle RAC procedures critical for transaction integrity, MGN ensures zero-downtime teller migration, IoT Device Management provides modern ATM monitoring at scale, and B2B Data Interchange handles Federal Reserve payment protocols natively without custom integration development.",
        whyWrong: {
            0: "Lift-and-shift doesn't achieve 40% cost reduction and Aurora migration risks breaking banking procedures",
            1: "Hybrid approach doesn't achieve full cost benefits and maintains data center expenses",
            2: "SaaS replacement isn't realistic for regional bank with regulatory requirements and custom procedures"
        },
        examStrategy: "For banking cost reduction: Mainframe Modernization provides significant savings while preserving logic. RDS Custom maintains database compatibility. B2B Data Interchange handles banking industry standards (Federal Reserve)."
    }
},
{
    id: 'sap_157',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global pharmaceutical company with 300 AWS accounts conducts clinical trials across 60 countries. Requirements include: complete trial data isolation between competing drugs, secure collaboration with 500 research institutions, automated compliance with FDA, EMA, and local health authorities, real-time safety monitoring across all trials, protection of intellectual property, and unified regulatory reporting.",
    question: "Which governance model provides trial isolation while enabling global pharmaceutical research collaboration?",
    options: [
        "AWS Organizations with drug-specific OUs and automated regulatory guardrails, AWS Clean Rooms for IP-protected research collaboration with institutions, AWS HealthLake for clinical trial data management with FHIR compliance, Amazon Monitron for real-time safety signal detection, AWS DataZone for governed cross-trial analytics, and AWS Audit Manager for multi-country health authority compliance frameworks",
        "Separate Organizations per therapeutic area, manual research collaboration, custom clinical databases, traditional safety monitoring, individual analytics, and separate compliance processes",
        "Single Organization with IAM policies, S3 bucket sharing, RDS for trial data, CloudWatch for monitoring, manual data governance, and basic compliance tracking",
        "Hub-and-spoke architecture, API-based collaboration, centralized databases, automated monitoring, shared analytics, and unified compliance platform"
    ],
    correct: 0,
    explanation: {
        correct: "Drug-specific OUs with regulatory guardrails ensure complete trial isolation and automated compliance, Clean Rooms enables research collaboration while protecting competing drug IP, HealthLake provides FHIR-compliant clinical data management, Monitron detects safety signals in real-time across trials, DataZone enables governed analytics while maintaining trial boundaries, and Audit Manager automates evidence collection for 60+ health authorities.",
        whyWrong: {
            1: "Separate Organizations break collaboration capabilities and multiply compliance management overhead",
            2: "Basic IAM policies don't provide clinical trial isolation required for competing drug development",
            3: "Hub-and-spoke doesn't provide IP protection and centralized databases violate trial isolation requirements"
        },
        examStrategy: "For pharmaceutical trials: Clean Rooms protects IP during collaboration, HealthLake provides clinical compliance, DataZone enables governed analytics. Monitron provides real-time safety monitoring across trials."
    }
},
{
    id: 'sap_158',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard", 
    timeRecommendation: 180,
    scenario: "A autonomous vehicle company is developing a fleet management platform for 100,000 self-driving delivery vehicles. Requirements include: real-time route optimization considering traffic and weather, vehicle-to-vehicle communication for coordinated delivery, predictive maintenance using sensor data, remote vehicle control for emergency situations, integration with logistics companies, and compliance with transportation safety regulations.",
    question: "Which architecture enables intelligent autonomous fleet management at scale?",
    options: [
        "EC2 for route processing, custom vehicle communication, Lambda for maintenance, manual emergency control, API Gateway for logistics, and basic compliance monitoring",
        "AWS IoT FleetWise for vehicle systems integration and OTA updates, Amazon Location Service for real-time route optimization with traffic data, AWS IoT Core for vehicle-to-vehicle mesh communication, Amazon Lookout for Equipment for predictive maintenance, AWS IoT Device Management for remote emergency control, AWS Supply Chain for logistics coordination, and AWS Audit Manager for transportation safety compliance",
        "Kinesis for vehicle data, third-party mapping, MSK for communication, SageMaker for predictions, Lambda for emergency response, Direct Connect to logistics, and Config for compliance",
        "Custom fleet management on containers, Google Maps integration, MQTT for communication, custom ML models, Step Functions for control, REST APIs for integration, and manual safety compliance"
    ],
    correct: 1,
    explanation: {
        correct: "IoT FleetWise provides vehicle lifecycle management with OTA updates for 100,000 vehicles, Location Service offers real-time route optimization with traffic and weather integration, IoT Core enables scalable vehicle-to-vehicle mesh communication, Lookout for Equipment predicts failures in autonomous vehicle systems, IoT Device Management handles remote emergency control securely, Supply Chain coordinates with logistics companies, and Audit Manager automates transportation safety compliance.",
        whyWrong: {
            0: "Custom vehicle communication doesn't scale to 100,000 vehicles and manual emergency control isn't reliable",
            2: "Third-party mapping lacks real-time optimization features and MSK adds unnecessary complexity for vehicle communication",
            3: "Custom solutions require extensive development and Google Maps integration lacks AWS service integration"
        },
        examStrategy: "For autonomous vehicles: IoT FleetWise manages vehicle fleets, Location Service provides route optimization, Lookout for Equipment predicts vehicle failures. Supply Chain handles logistics coordination."
    }
},
{
    id: 'sap_159',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A biotechnology company is developing a drug discovery platform using quantum-classical hybrid computing. Requirements include: molecular simulation on quantum processors from multiple vendors, classical preprocessing of 100TB protein databases, real-time collaboration between 200 research institutions, quantum algorithm version control with IP protection, results validation through classical verification, and 15-year research data retention for patent applications.",
    question: "Which architecture enables cutting-edge quantum drug discovery research while protecting intellectual property?",
    options: [
        "Custom quantum APIs, GPU clusters for classical compute, file sharing for collaboration, Git repositories for algorithms, EC2 for verification, and S3 for storage",
        "Third-party quantum platforms, EMR for data processing, SharePoint for collaboration, private repositories, Batch for validation, and Glacier for archives",
        "Amazon Braket for multi-vendor quantum access with native AWS integration, AWS ParallelCluster with P4d instances for 100TB protein database processing, AWS Clean Rooms for IP-protected research collaboration, AWS CodeCommit with fine-grained IAM for quantum algorithm version control, AWS Batch with Spot instances for cost-effective classical verification, and S3 Intelligent-Tiering with Glacier Deep Archive for 15-year patent data retention",
        "Direct quantum hardware access, HPC on EC2, VPN for collaboration, SVN for version control, Lambda for verification, and EBS for storage"
    ],
    correct: 2,
    explanation: {
        correct: "Braket provides unified access to multiple quantum vendors (IBM, Rigetti, IonQ) with native AWS integration, ParallelCluster with P4d delivers massive GPU scale for protein preprocessing, Clean Rooms enables research collaboration while protecting quantum algorithm IP, CodeCommit with IAM provides secure version control for proprietary algorithms, Batch with Spot provides cost-effective classical verification at scale, and S3 Intelligent-Tiering with Deep Archive optimizes 15-year storage costs for patent applications.",
        whyWrong: {
            0: "Custom quantum APIs lack unified programming interface and vendor management capabilities",
            1: "Third-party quantum platforms don't integrate well with AWS compute and storage services",
            3: "Direct quantum access is complex to manage and doesn't provide unified interface across vendors"
        },
        examStrategy: "Amazon Braket provides quantum computing access with AWS integration. Clean Rooms protects IP in research collaboration. ParallelCluster scales HPC workloads. Use Intelligent-Tiering for long-term research data."
    }
},
{
    id: 'sap_160',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global streaming platform serves 200 million subscribers with monthly AWS costs of $8M: CloudFront ($3M), EC2 transcoding ($2M), S3 video storage ($2M), DynamoDB user data ($800K), RDS content metadata ($200K). Issues include: CloudFront costs spiking 300% during major releases, transcoding queues backing up 48+ hours, 95% of video content never watched after 90 days, DynamoDB throttling during concurrent viewing peaks, and complex queries timing out in RDS.",
    question: "Which comprehensive optimization strategy provides the GREATEST cost reduction and performance improvement?",
    options: [
        "Add more CloudFront distributions, increase transcoding capacity, implement S3 lifecycle policies, scale DynamoDB provisioning, and add RDS read replicas for improved performance",
        "Implement CloudFront Origin Shield and compression reducing costs 85%, migrate to AWS Elemental MediaConvert for parallel serverless transcoding, enable S3 Intelligent-Tiering moving 95% content to Archive tiers, implement DynamoDB on-demand with DAX caching, and migrate to Aurora Serverless v2 for automatic metadata scaling",
        "Optimize CloudFront configurations, containerize transcoding on EKS, manually move old videos to Glacier, add DynamoDB auto-scaling, and implement Aurora Global Database",
        "Use multiple CDN providers, build custom transcoding pipeline, compress video files, implement ElastiCache, and migrate to PostgreSQL on RDS"
    ],
    correct: 1,
    explanation: {
        correct: "Origin Shield reduces CloudFront costs 85% during major releases by eliminating duplicate origin requests, MediaConvert provides serverless transcoding eliminating 48-hour queues through parallel processing, S3 Intelligent-Tiering automatically moves 95% of content to cheaper Archive tiers saving $1.9M annually, DynamoDB on-demand with DAX handles viewing peaks without throttling, and Aurora Serverless v2 eliminates RDS query timeouts through automatic scaling.",
        whyWrong: {
            0: "Adding more resources increases costs without solving architectural inefficiencies that cause the problems",
            2: "Manual video movement is error-prone and EKS adds complexity without solving fundamental transcoding queue issues",
            3: "Multiple CDNs add complexity and custom transcoding requires extensive development vs managed services"
        },
        examStrategy: "For streaming platforms: Origin Shield dramatically reduces CDN costs, MediaConvert provides serverless transcoding, S3 Intelligent-Tiering automates massive storage savings. Combine multiple optimization strategies for maximum impact."
    }
},

// Questions 161-180 with pre-planned randomization: A(5), B(5), C(5), D(5)
{
    id: 'sap_161',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global shipping company must migrate port management systems to AWS within 12 months before mainframe contracts expire. Systems include: container tracking across 500 ports, customs integration with 100+ countries, ship scheduling with real-time berth allocation, cargo manifests with hazardous material handling, and integration with international shipping lines. Peak processing: 10,000 container movements daily per major port.",
    question: "Which migration approach ensures global shipping operations continuity while meeting regulatory requirements?",
    options: [
        "Lift-and-shift mainframe to EC2, custom APIs for customs, manual ship scheduling, S3 for manifests, and point-to-point shipping line integration",
        "Replace with SaaS port management, cloud customs solutions, automated scheduling algorithms, DynamoDB for cargo data, and modern shipping APIs",
        "AWS Mainframe Modernization for container tracking logic preservation, AWS B2B Data Interchange for automated customs integration across 100+ countries, Amazon Location Service for optimized berth allocation, AWS IoT Core for real-time container tracking with hazmat compliance, and AWS Supply Chain for shipping line coordination",
        "Keep mainframe hybrid, gradual customs migration, basic scheduling system, file-based manifests, and email-based shipping coordination"
    ],
    correct: 2,
    explanation: {
        correct: "Mainframe Modernization preserves decades of container tracking logic while enabling cloud scalability, B2B Data Interchange handles diverse customs EDI formats across 100+ countries automatically, Location Service optimizes berth allocation considering ship size and port constraints, IoT Core tracks 10,000+ daily container movements with hazmat compliance monitoring, and Supply Chain provides end-to-end shipping line coordination at global scale.",
        whyWrong: {
            0: "Custom customs APIs are extremely complex and error-prone for 100+ countries with different regulations",
            1: "SaaS replacement doesn't work for ports with extensive customization and international compliance requirements",
            3: "Hybrid approach doesn't solve mainframe contract expiration and limits modernization benefits"
        },
        examStrategy: "For port/shipping migration: preserve complex logistics algorithms, B2B Data Interchange handles international customs standards. Location Service optimizes physical space allocation. Supply Chain coordinates complex logistics networks."
    }
},
{
    id: 'sap_162',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational oil company operates 200 AWS accounts across exploration, drilling, refining, and retail divisions spanning 50 countries. Requirements include: environmental compliance reporting to multiple governments, secure geological data sharing between exploration teams, real-time safety monitoring across all operations, cost allocation to individual oil fields and refineries, and protection of proprietary exploration data.",
    question: "Which governance architecture addresses oil industry requirements while enabling operational efficiency?",
    options: [
        "AWS Organizations with division-specific OUs and automated environmental guardrails, AWS Clean Rooms for IP-protected geological data collaboration, AWS IoT SiteWise for industrial safety monitoring across all operations, AWS Cost Categories for field-level cost allocation, AWS DataZone for governed exploration data sharing, and AWS B2B Data Interchange for automated government environmental reporting",
        "Separate Organizations per country, manual environmental reporting, basic data sharing policies, spreadsheet cost tracking, isolated safety systems, and individual compliance processes",
        "Single Organization with basic IAM, S3 bucket sharing, CloudWatch monitoring, tagging for costs, manual data governance, and quarterly compliance reports",
        "Hub-and-spoke architecture, centralized databases, shared monitoring, automated billing, unified analytics, and single compliance platform"
    ],
    correct: 0,
    explanation: {
        correct: "Division-specific OUs with environmental guardrails ensure automated compliance across 50 countries, Clean Rooms enables geological data collaboration while protecting exploration IP, IoT SiteWise provides industrial-grade safety monitoring for oil operations, Cost Categories enable granular field/refinery allocation, DataZone provides governed data sharing with lineage tracking, and B2B Data Interchange automates environmental reporting to multiple governments.",
        whyWrong: {
            1: "Separate Organizations break operational coordination and multiply compliance management overhead",
            2: "Basic controls don't provide sophisticated environmental compliance and IP protection needed for oil industry",
            3: "Centralized databases violate IP protection requirements between competing exploration projects"
        },
        examStrategy: "For oil/gas governance: IoT SiteWise monitors industrial operations, Clean Rooms protects exploration IP, DataZone provides data lineage for environmental compliance. B2B Data Interchange handles government reporting standards."
    }
},
{
    id: 'sap_163',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national emergency management agency is building a disaster response platform coordinating 10,000 first responders across natural disasters. Requirements include: real-time location tracking of emergency personnel, satellite communication during infrastructure outages, AI-powered resource allocation optimization, integration with 500 local emergency services, predictive modeling for disaster impact, and compliance with federal emergency management regulations.",
    question: "Which architecture provides comprehensive emergency response coordination capabilities?",
    options: [
        "EC2 for coordination systems, GPS tracking via mobile apps, VPN for communication, manual resource allocation, API Gateway for local services, and basic compliance monitoring",
        "IoT Core for responder tracking, Direct Connect for reliability, Lambda for optimization, custom integration APIs, machine learning on SageMaker, and Config for compliance",
        "Custom emergency software, satellite modems, spreadsheet coordination, point-to-point integration, historical analysis, and manual compliance processes",
        "AWS IoT Core for first responder device tracking with emergency protocols, AWS Ground Station for satellite communication backup during infrastructure failures, Amazon Forecast for disaster impact prediction, AWS Supply Chain for emergency resource optimization, AWS B2B Data Interchange for automated local emergency service coordination, and AWS Audit Manager for federal emergency management compliance"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core handles 10,000 first responder devices with emergency communication protocols, Ground Station provides satellite backup when terrestrial infrastructure fails, Forecast predicts disaster impact using weather and historical data, Supply Chain optimizes emergency resource allocation across affected areas, B2B Data Interchange automates coordination with 500 local services using emergency management standards, and Audit Manager ensures federal emergency compliance frameworks.",
        whyWrong: {
            0: "GPS mobile apps fail during infrastructure outages and manual allocation can't optimize 10,000 responders efficiently",
            1: "Direct Connect fails during disasters and doesn't provide satellite backup capabilities",
            2: "Custom solutions take too long to develop and satellite modems don't scale to 10,000 responders"
        },
        examStrategy: "For emergency management: Ground Station provides satellite backup, Supply Chain optimizes resource allocation, B2B Data Interchange handles emergency service standards. IoT Core scales to thousands of devices with emergency protocols."
    }
},
{
    id: 'sap_164',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A ride-sharing platform supports 50 million rides monthly using: 800 EC2 instances for matching ($400K/month), DynamoDB for driver locations ($200K/month), ElastiCache for real-time data ($150K/month), API Gateway for mobile apps ($100K/month). Issues include: 15-second driver matching during peak hours, location data inconsistencies between regions, cache memory pressure during events, API throttling during surge periods, and 60% infrastructure idle during overnight hours.",
    question: "Which optimization strategy provides the BEST performance improvement and cost reduction for variable ride-sharing traffic?",
    options: [
        "Add more EC2 instances, increase DynamoDB capacity, expand ElastiCache clusters, raise API Gateway limits, and implement Reserved Instances",
        "Migrate to ECS Fargate Spot for elastic matching engine scaling, implement DynamoDB Global Tables for consistent driver locations across regions, upgrade to MemoryDB for durable real-time cache, add API Gateway regional endpoints with caching, and implement Lambda with provisioned concurrency for sub-second matching algorithms",
        "Containerize on EKS with autoscaling, add DynamoDB auto-scaling, implement Redis clustering, optimize API configurations, and use Spot instances for cost savings",
        "Keep current architecture but optimize settings, add connection pooling, implement application caching, use CDN for mobile APIs, and negotiate volume discounts"
    ],
    correct: 1,
    explanation: {
        correct: "Fargate Spot eliminates 60% idle costs while providing instant scaling for peak demand, DynamoDB Global Tables ensure driver location consistency across regions, MemoryDB provides Redis performance with Multi-AZ durability for critical real-time data, API Gateway regional endpoints with caching handle surge traffic efficiently, and Lambda with provisioned concurrency reduces matching time to <3 seconds.",
        whyWrong: {
            0: "Adding more resources increases costs without solving architectural inefficiencies and idle capacity issues",
            2: "EKS adds operational complexity and doesn't address fundamental cost optimization opportunities of serverless",
            3: "Optimizing current architecture doesn't eliminate idle capacity or solve regional consistency issues"
        },
        examStrategy: "For variable ride-sharing loads: Fargate Spot eliminates idle costs, DynamoDB Global Tables solve regional consistency, MemoryDB provides durable real-time performance. Lambda with provisioned concurrency optimizes matching speed."
    }
},
{
    id: 'sap_165',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A state lottery system must migrate to AWS within 24 months due to security compliance requirements. Current systems include: mainframe for ticket validation with 30 years of winning number algorithms, Oracle databases for retailer networks, real-time integration with 50,000 lottery terminals, financial reporting for state revenue, and compliance with gaming regulations in multiple states.",
    question: "Which migration approach maintains lottery security while enabling modern capabilities?",
    options: [
        "AWS Mainframe Modernization for ticket validation algorithms, Amazon RDS Custom for Oracle maintaining lottery procedures, AWS IoT Core for lottery terminal connectivity, Amazon QuickSight for financial reporting, AWS B2B Data Interchange for multi-state gaming regulation compliance, and AWS CloudHSM for cryptographic lottery security requirements",
        "Lift-and-shift mainframe to EC2, Oracle migration to Aurora, custom terminal integration, manual financial reports, and basic compliance monitoring",
        "Replace lottery system with modern architecture, PostgreSQL for all data, API Gateway for terminals, automated reporting, and cloud compliance tools",
        "Keep mainframe with cloud connectivity, hybrid Oracle deployment, maintain terminal connections, upgrade reporting, and gradual compliance migration"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization preserves 30 years of proven lottery algorithms while adding cloud benefits, RDS Custom maintains Oracle procedures critical for lottery integrity, IoT Core handles 50,000 terminal connections at scale, QuickSight provides real-time state revenue reporting, B2B Data Interchange automates multi-state gaming compliance, and CloudHSM provides FIPS 140-2 Level 3 security for lottery cryptographic requirements.",
        whyWrong: {
            1: "Aurora migration could break lottery validation procedures and simple EC2 doesn't meet gaming security requirements",
            2: "Complete replacement risks lottery integrity and PostgreSQL doesn't provide gaming industry compliance features",
            3: "Hybrid approach doesn't achieve full security compliance benefits and maintains legacy vulnerabilities"
        },
        examStrategy: "For gaming/lottery migration: preserve proven algorithms, CloudHSM provides gaming-grade security, B2B Data Interchange handles gaming regulations. RDS Custom maintains database integrity for financial systems."
    }
},
{
    id: 'sap_166',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A retail chain with 2,000 stores operates separate AWS accounts per region for local compliance. Requirements include: unified inventory management across all stores, real-time price synchronization during promotions, centralized fraud detection, customer loyalty program integration, seasonal demand forecasting, and compliance with state tax regulations.",
    question: "Which solution enables unified retail operations while maintaining regional compliance?",
    options: [
        "Merge all accounts into single retail account, implement resource policies, use RDS for inventory, Lambda for pricing, and manual tax compliance",
        "VPC peering between regions, shared databases, centralized applications, basic fraud detection, and quarterly compliance reviews",
        "AWS Organizations with regional OUs for tax compliance, Amazon EventBridge for real-time inventory and pricing events, AWS Fraud Detector for centralized fraud detection, Amazon Personalize for loyalty program optimization, Amazon Forecast for demand prediction, and AWS B2B Data Interchange for automated state tax reporting",
        "Keep separate accounts with no integration, individual systems per region, manual fraud monitoring, basic loyalty programs, and local forecasting"
    ],
    correct: 2,
    explanation: {
        correct: "Regional OUs ensure tax compliance while enabling operational coordination, EventBridge provides real-time event-driven inventory and pricing synchronization, Fraud Detector offers centralized ML-powered fraud detection across all stores, Personalize optimizes loyalty programs using cross-store data, Forecast predicts seasonal demand patterns, and B2B Data Interchange automates state tax reporting compliance.",
        whyWrong: {
            0: "Merging accounts violates regional compliance requirements for state tax regulations",
            1: "VPC peering with shared databases creates potential compliance violations and data residency issues",
            3: "Complete isolation prevents unified operations and customer experience optimization"
        },
        examStrategy: "For retail multi-region: EventBridge enables real-time coordination across accounts, Fraud Detector provides centralized security, Personalize optimizes customer experience. Organizations OUs maintain compliance boundaries."
    }
},
{
    id: 'sap_167',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart manufacturing company is building an Industry 4.0 platform for 100 factories producing automotive parts. Requirements include: predictive quality control using computer vision, supply chain optimization with 1,000 suppliers, energy consumption optimization, worker safety monitoring, integration with automotive OEM systems, and compliance with automotive industry standards (IATF 16949).",
    question: "Which architecture enables comprehensive smart manufacturing capabilities?",
    options: [
        "EC2 for quality control, custom supply chain software, CloudWatch for energy monitoring, manual safety procedures, and basic OEM integration",
        "Lambda for image processing, third-party supply chain platforms, IoT Core for energy data, wearable device integration, and REST APIs for OEMs",
        "Custom computer vision models, spreadsheet supply chain management, manual energy tracking, paper-based safety logs, and email coordination with OEMs",
        "AWS Panorama for edge computer vision quality control, AWS Supply Chain for supplier optimization and visibility, AWS IoT SiteWise for industrial energy monitoring, Amazon Monitron for worker safety equipment monitoring, AWS B2B Data Interchange for automotive OEM integration, and AWS Audit Manager for IATF 16949 compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Panorama provides edge computer vision for real-time quality control without bandwidth costs, Supply Chain optimizes 1,000 supplier relationships with end-to-end visibility, IoT SiteWise monitors industrial energy consumption with optimization recommendations, Monitron provides predictive monitoring for worker safety equipment, B2B Data Interchange handles automotive OEM EDI standards, and Audit Manager automates IATF 16949 compliance evidence collection.",
        whyWrong: {
            0: "Generic EC2 computer vision lacks automotive industry-specific quality models and real-time edge processing",
            1: "Third-party platforms lack AWS integration and Lambda has limitations for complex computer vision processing",
            2: "Manual processes don't scale to 100 factories and lack real-time capabilities needed for automotive production"
        },
        examStrategy: "For smart manufacturing: Panorama for edge computer vision, Supply Chain for supplier optimization, IoT SiteWise for industrial monitoring. Monitron provides predictive maintenance for safety equipment."
    }
},
{
    id: 'sap_168',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A fintech company processes loan applications using: 300 EC2 instances for underwriting ($500K/month), RDS PostgreSQL for applications ($200K/month), SageMaker for credit scoring ($150K/month), S3 for documents ($100K/month). Issues include: 24-hour application processing time, database timeouts during month-end, ML model retraining taking 8 hours, document retrieval delays affecting customer experience, and 70% infrastructure idle overnight.",
    question: "Which optimization strategy provides the MOST significant improvement in processing speed and cost efficiency?",
    options: [
        "Add more EC2 instances, increase RDS capacity, optimize SageMaker training, implement S3 Transfer Acceleration, and purchase Reserved Instances for cost savings",
        "Migrate to Step Functions with Lambda for parallel loan processing reducing time to 2 hours, implement Aurora Serverless v2 for automatic scaling during month-end loads, use SageMaker Serverless Inference for cost-effective real-time scoring, implement S3 Intelligent-Tiering with CloudFront for faster document access, and eliminate idle costs through serverless architecture",
        "Containerize on EKS with autoscaling, add Aurora read replicas, implement SageMaker multi-model endpoints, enable S3 lifecycle policies, and use Spot instances where possible",
        "Optimize current EC2 configurations, implement connection pooling, schedule model training during off-hours, compress documents, and negotiate enterprise pricing"
    ],
    correct: 1,
    explanation: {
        correct: "Step Functions with Lambda enables parallel processing reducing 24-hour application time to 2 hours, Aurora Serverless v2 eliminates month-end database timeouts through automatic scaling, SageMaker Serverless Inference provides cost-effective real-time scoring without idle costs, S3 Intelligent-Tiering with CloudFront dramatically improves document access speed, and serverless architecture eliminates 70% idle costs overnight.",
        whyWrong: {
            0: "Adding more resources increases costs without solving fundamental processing inefficiencies and idle capacity",
            2: "EKS adds complexity and doesn't provide the processing speed improvements of parallel serverless workflows",
            3: "Optimizing current architecture doesn't solve fundamental 24-hour processing time or eliminate idle capacity"
        },
        examStrategy: "For loan processing optimization: Step Functions enables parallel workflows reducing processing time, Aurora Serverless v2 handles variable loads, SageMaker Serverless eliminates ML idle costs. Serverless architecture eliminates idle infrastructure costs."
    }
},
{
    id: 'sap_169',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A regional electric utility must migrate grid management systems to AWS within 18 months for regulatory compliance. Current systems include: SCADA for power grid control, customer billing with 20 years of usage history, outage management with emergency responder integration, demand forecasting, and integration with renewable energy sources. System reliability is critical for public safety.",
    question: "Which migration approach maintains power grid reliability while achieving compliance goals?",
    options: [
        "AWS IoT SiteWise Edge for local SCADA control ensuring grid stability during connectivity issues, AWS Application Migration Service for billing system preservation, Amazon Connect for emergency responder coordination, Amazon Forecast for demand prediction, AWS IoT Analytics for renewable energy integration, and hybrid connectivity maintaining operational continuity",
        "Migrate SCADA to IoT Core, lift-and-shift billing to EC2, build new outage management, use SageMaker for forecasting, and Direct Connect for renewable integration",
        "Keep SCADA on-premises, cloud billing system, modernize outage management, implement ML forecasting, and API integration for renewables",
        "Replace grid systems with cloud-native solutions, modern billing platform, automated outage detection, AI-powered forecasting, and real-time renewable coordination"
    ],
    correct: 0,
    explanation: {
        correct: "IoT SiteWise Edge maintains local SCADA control for grid safety during outages, MGN preserves 20 years of billing history and procedures, Amazon Connect provides scalable emergency coordination, Forecast optimizes demand prediction with weather integration, IoT Analytics handles renewable energy data efficiently, and hybrid connectivity ensures no service interruption during migration.",
        whyWrong: {
            1: "Direct IoT Core SCADA migration creates single point of failure risking public safety",
            2: "Keeping SCADA on-premises doesn't achieve full compliance and modernization goals",
            3: "Complete replacement of grid systems is too risky for critical infrastructure in 18 months"
        },
        examStrategy: "For utility grid migration: maintain local control for safety-critical systems, IoT SiteWise Edge enables hybrid operation. Amazon Connect handles emergency communications. Never migrate safety systems without local backup."
    }
},
{
    id: 'sap_170',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global aerospace company with 400 AWS accounts manufactures aircraft components across 30 countries. Requirements include: export control compliance (ITAR/EAR), secure collaboration with international partners, supply chain traceability for safety regulations, intellectual property protection for aircraft designs, real-time quality monitoring across factories, and cost allocation to individual aircraft programs.",
    question: "Which governance architecture addresses aerospace industry security and compliance requirements?",
    options: [
        "Basic AWS Organizations structure, standard IAM policies, S3 for design sharing, manual export control tracking, CloudWatch monitoring, and spreadsheet cost allocation",
        "Separate Organizations per country, individual compliance processes, isolated partner access, manual supply chain tracking, basic monitoring systems, and project-based billing",
        "AWS Organizations with ITAR-compliant guardrails and export control automation, AWS Clean Rooms for IP-protected international collaboration, AWS Supply Chain for component traceability, AWS DataZone for governed design data sharing, AWS IoT SiteWise for factory quality monitoring, and AWS Cost Categories for aircraft program allocation",
        "Hub-and-spoke architecture, centralized databases, shared partner access, automated tracking systems, unified monitoring, and consolidated billing"
    ],
    correct: 2,
    explanation: {
        correct: "ITAR-compliant guardrails ensure export control automation, Clean Rooms enables international collaboration while protecting aircraft IP, Supply Chain provides end-to-end component traceability for safety compliance, DataZone governs sensitive design data sharing with lineage tracking, IoT SiteWise monitors industrial quality across 30 countries, and Cost Categories enable precise aircraft program cost allocation.",
        whyWrong: {
            0: "Standard controls don't meet ITAR export compliance requirements and basic monitoring lacks aerospace industry features",
            1: "Separate Organizations break international collaboration and supply chain visibility",
            3: "Centralized databases violate ITAR requirements and don't provide IP protection for competing aircraft programs"
        },
        examStrategy: "For aerospace/defense: ITAR compliance is critical, Clean Rooms protects IP in international collaboration, Supply Chain provides traceability for safety regulations. DataZone ensures data governance for sensitive designs."
    }
},
{
    id: 'sap_171',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A precision medicine company is developing a cancer treatment platform requiring analysis of patient genomics, proteomics, and metabolomics data. Requirements include: processing 10,000 patient multi-omics datasets, AI-driven drug combination predictions, clinical trial patient matching, real-time biomarker monitoring, integration with 200 cancer centers, and FDA compliance for clinical decision support.",
    question: "Which architecture enables comprehensive precision oncology while maintaining regulatory compliance?",
    options: [
        "Custom bioinformatics on EC2, RDS for patient data, Lambda for drug predictions, API Gateway for centers, and manual FDA compliance",
        "S3 data lake for omics data, EMR for processing, SageMaker for AI models, Direct Connect to centers, and Config for compliance monitoring",
        "Redshift for multi-omics analysis, Aurora for clinical data, Batch for drug screening, VPN to cancer centers, and third-party compliance tools",
        "Amazon Omics for multi-omics data analysis workflows, AWS HealthLake for HIPAA-compliant clinical data with FHIR support, Amazon SageMaker with healthcare compliance for AI drug predictions, AWS IoT Core for real-time biomarker devices, AWS Clean Rooms for cancer center collaboration, and AWS Audit Manager for FDA clinical decision support compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Amazon Omics provides purpose-built multi-omics analysis for genomics, proteomics, and metabolomics, HealthLake ensures HIPAA compliance with FHIR support for oncology workflows, SageMaker with healthcare features enables compliant AI drug predictions, IoT Core handles real-time biomarker monitoring devices, Clean Rooms enables cancer center collaboration without exposing patient data, and Audit Manager automates FDA compliance for clinical decision support.",
        whyWrong: {
            0: "Custom bioinformatics lacks optimized multi-omics capabilities and manual FDA compliance doesn't scale",
            1: "Generic EMR doesn't provide specialized omics analysis and lacks healthcare compliance features",
            2: "Redshift isn't optimized for genomics data and VPN doesn't scale to 200 cancer centers securely"
        },
        examStrategy: "For precision medicine: Amazon Omics for multi-omics analysis, HealthLake for clinical compliance, Clean Rooms for healthcare collaboration. SageMaker with healthcare features for compliant AI."
    }
},
{
    id: 'sap_172',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global food delivery platform handles 20 million orders daily using: 1,200 EC2 instances for order processing ($800K/month), DynamoDB for order tracking ($300K/month), ElastiCache for driver locations ($200K/month), API Gateway for mobile apps ($150K/month). Issues include: order processing delays during lunch/dinner peaks, driver location lag causing poor matching, cache evictions during high demand, API timeouts during promotional campaigns, and massive over-provisioning for 2-hour peak periods.",
    question: "Which optimization strategy provides the BEST cost-performance improvement for highly variable food delivery traffic?",
    options: [
        "Pre-scale infrastructure before peak hours, increase DynamoDB provisioned capacity, expand cache clusters, raise API Gateway quotas, and use Reserved Instances for cost optimization",
        "Migrate to ECS Fargate Spot for elastic order processing eliminating over-provisioning costs, implement DynamoDB on-demand for automatic promotional scaling, upgrade to MemoryDB with clustering for real-time driver tracking, add API Gateway regional endpoints with intelligent caching, and implement CloudFront for mobile app acceleration",
        "Implement predictive Auto Scaling, add DynamoDB auto-scaling, use ElastiCache for Redis clustering, optimize API configurations, and purchase Compute Savings Plans",
        "Containerize on EKS with KEDA autoscaling, keep current DynamoDB setup, add Redis read replicas, implement API versioning, and use Spot instances for development"
    ],
    correct: 1,
    explanation: {
        correct: "Fargate Spot eliminates over-provisioning costs while providing instant scaling for 2-hour peaks, DynamoDB on-demand handles promotional traffic spikes without pre-provisioning, MemoryDB clustering scales driver location tracking with durability, API Gateway regional endpoints with caching handle promotional campaign traffic, and CloudFront acceleration improves mobile app performance globally.",
        whyWrong: {
            0: "Pre-scaling maintains expensive over-provisioning and doesn't address fundamental cost optimization opportunities",
            2: "Predictive scaling still requires baseline over-provisioning and doesn't eliminate fundamental waste",
            3: "EKS adds operational complexity and doesn't address the core over-provisioning cost issue"
        },
        examStrategy: "For highly variable food delivery: Fargate Spot eliminates over-provisioning, DynamoDB on-demand handles promotion spikes, MemoryDB provides scalable real-time performance. Regional API endpoints improve resilience."
    }
},
{
    id: 'sap_173',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A state department of motor vehicles must migrate driver licensing systems to AWS within 16 months for security compliance. Current systems include: mainframe for license validation with 40 years of driving records, Oracle databases for DMV transactions, integration with law enforcement systems, photo capture and facial recognition, and compliance with federal REAL ID requirements.",
    question: "Which migration approach maintains public service continuity while achieving security compliance?",
    options: [
        "AWS Mainframe Modernization for license validation logic preservation, Amazon RDS Custom for Oracle maintaining DMV procedures, AWS IoT Core for photo capture device integration, Amazon Rekognition for facial recognition with law enforcement integration, AWS B2B Data Interchange for law enforcement system connectivity, and AWS Audit Manager for REAL ID federal compliance automation",
        "Lift-and-shift mainframe to EC2, Aurora for Oracle migration, custom photo systems, third-party facial recognition, REST APIs for law enforcement, and manual REAL ID compliance",
        "Replace DMV system with modern platform, PostgreSQL for all data, cloud photo processing, custom biometric solutions, API Gateway for integration, and cloud compliance tools",
        "Keep mainframe with hybrid connectivity, gradual Oracle migration, modernize photo capture, upgrade facial recognition, maintain law enforcement connections, and phase REAL ID compliance"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization preserves 40 years of driving record logic while enabling cloud security, RDS Custom maintains Oracle procedures critical for DMV transactions, IoT Core handles photo capture devices at scale, Rekognition provides facial recognition with law enforcement integration capabilities, B2B Data Interchange manages law enforcement system protocols, and Audit Manager automates REAL ID federal compliance requirements.",
        whyWrong: {
            1: "Aurora migration could break DMV transaction procedures and third-party recognition lacks law enforcement integration",
            2: "Complete replacement risks public safety and loses decades of driving record history",
            3: "Hybrid approach doesn't achieve full security compliance and maintains legacy vulnerabilities"
        },
        examStrategy: "For government DMV migration: preserve critical record systems, Rekognition provides facial recognition with law enforcement features. B2B Data Interchange handles law enforcement protocols. Audit Manager automates federal compliance."
    }
},
{
    id: 'sap_174',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A pharmaceutical research consortium operates 150 AWS accounts across member companies conducting collaborative drug research. Requirements include: complete intellectual property isolation between competing projects, secure data sharing for approved collaborations, unified clinical trial recruitment, regulatory reporting coordination, and cost sharing based on research participation levels.",
    question: "Which governance model enables pharmaceutical collaboration while protecting competitive interests?",
    options: [
        "Single Organization with IAM policies, S3 bucket sharing, manual trial coordination, individual regulatory reports, and spreadsheet cost allocation",
        "Separate Organizations per company, isolated research environments, email-based collaboration, individual compliance processes, and manual cost tracking",
        "AWS Organizations with project-specific OUs for IP isolation, AWS Clean Rooms for secure approved collaborations without exposing proprietary data, AWS HealthLake for unified clinical trial management, AWS Audit Manager for coordinated regulatory reporting, and AWS Cost Categories for participation-based cost allocation",
        "Hub-and-spoke architecture, centralized research databases, API-based sharing, shared compliance systems, and automated billing allocation"
    ],
    correct: 2,
    explanation: {
        correct: "Project-specific OUs ensure complete IP isolation between competing research, Clean Rooms enables approved collaborations while protecting proprietary drug research data, HealthLake provides unified clinical trial recruitment across consortium members, Audit Manager coordinates regulatory reporting without exposing sensitive research, and Cost Categories enable precise participation-based cost sharing.",
        whyWrong: {
            0: "Basic IAM policies don't provide IP protection needed for competitive pharmaceutical research",
            1: "Separate Organizations eliminate collaboration benefits and increase operational complexity",
            3: "Centralized databases violate IP isolation requirements for competing drug development projects"
        },
        examStrategy: "For pharmaceutical consortiums: Clean Rooms enables collaboration while protecting IP, HealthLake provides clinical trial coordination, project-specific OUs ensure IP isolation. Cost Categories enable complex cost sharing models."
    }
},
{
    id: 'sap_175',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart city initiative is building an integrated urban management platform for a metropolitan area with 5 million residents. Requirements include: traffic optimization across 10,000 intersections, air quality monitoring with 1,000 sensors, smart parking with dynamic pricing, emergency response coordination, energy grid optimization, and citizen engagement through mobile applications.",
    question: "Which architecture provides comprehensive smart city capabilities at metropolitan scale?",
    options: [
        "EC2 for traffic control, IoT Core for sensors, Lambda for parking logic, CloudWatch for emergency monitoring, basic energy tracking, and mobile APIs",
        "Custom traffic systems, DynamoDB for sensor data, serverless parking applications, manual emergency coordination, spreadsheet energy management, and native mobile apps",
        "Kubernetes for traffic processing, Timestream for sensor storage, microservices for parking, Step Functions for emergency workflows, energy monitoring dashboards, and API Gateway for mobile",
        "AWS Panorama for computer vision traffic optimization, AWS IoT SiteWise for air quality monitoring, Amazon Location Service for smart parking optimization, Amazon Connect for emergency response coordination, AWS IoT Analytics for energy grid optimization, and AWS AppSync for real-time citizen mobile engagement"
    ],
    correct: 3,
    explanation: {
        correct: "Panorama provides edge computer vision for 10,000 intersections without bandwidth costs, IoT SiteWise handles industrial-grade air quality monitoring, Location Service optimizes parking with dynamic pricing algorithms, Amazon Connect scales emergency response coordination, IoT Analytics processes energy grid data for optimization, and AppSync provides real-time mobile engagement with offline sync for citizens.",
        whyWrong: {
            0: "Generic EC2 lacks computer vision capabilities for traffic optimization and basic tracking doesn't provide energy grid optimization",
            1: "Custom systems require extensive development and manual coordination doesn't scale to metropolitan emergency response",
            2: "Kubernetes adds operational complexity and doesn't provide smart city-specific optimization features"
        },
        examStrategy: "For smart cities: Panorama for edge computer vision, IoT SiteWise for industrial monitoring, Location Service for spatial optimization. AppSync provides real-time mobile engagement. Use purpose-built services vs generic compute."
    }
},
{
    id: 'sap_176',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global gaming platform supports 100 million players with monthly costs of $3M: EC2 for game servers ($1.2M), DynamoDB for player data ($800K), ElastiCache for leaderboards ($500K), CloudFront for content delivery ($300K), S3 for game assets ($200K). Issues include: 30-second matchmaking during peak hours, game server scaling taking 10 minutes, leaderboard inconsistencies across regions, content download delays for new releases, and massive over-provisioning for weekend peak traffic.",
    question: "Which optimization strategy provides the GREATEST improvement in player experience and cost efficiency?",
    options: [
        "Add more game servers, increase DynamoDB capacity, expand cache clusters, optimize CloudFront configurations, and implement S3 Transfer Acceleration for better performance",
        "Migrate to Amazon GameLift for intelligent game server management with Spot Fleet integration, implement DynamoDB Global Tables for consistent leaderboards, use ElastiCache Global Datastore for real-time synchronization, add CloudFront Origin Shield for new release optimization, and implement GameLift FlexMatch for sub-5-second matchmaking",
        "Containerize game servers on EKS, add DynamoDB auto-scaling, implement Redis clustering, increase CloudFront edge locations, and use S3 lifecycle policies",
        "Pre-scale for weekends, optimize DynamoDB queries, implement local caching, add CDN compression, and purchase Reserved Instances for cost savings"
    ],
    correct: 1,
    explanation: {
        correct: "GameLift with Spot Fleet provides intelligent server management reducing costs 60% while eliminating 10-minute scaling delays, DynamoDB Global Tables ensure leaderboard consistency across regions, ElastiCache Global Datastore provides real-time synchronization, Origin Shield optimizes new release content delivery reducing origin load 85%, and FlexMatch reduces matchmaking from 30 seconds to <5 seconds.",
        whyWrong: {
            0: "Adding more resources increases costs without solving fundamental scaling and consistency issues",
            2: "EKS adds complexity and doesn't provide gaming-specific optimizations like intelligent matchmaking",
            3: "Pre-scaling maintains expensive over-provisioning and doesn't solve consistency or matchmaking issues"
        },
        examStrategy: "For gaming optimization: GameLift provides purpose-built game server management, FlexMatch optimizes matchmaking. Global Tables and Global Datastore solve multi-region consistency. Origin Shield optimizes content delivery costs."
    }
},
{
    id: 'sap_177',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A county government must migrate property tax assessment systems to AWS within 20 months for compliance with state modernization requirements. Current systems include: mainframe for property valuations with 50 years of assessment data, Oracle databases for tax calculations, integration with 100 local municipalities, GIS mapping for property boundaries, and citizen portal for tax payments.",
    question: "Which migration approach maintains tax collection continuity while meeting modernization requirements?",
    options: [
        "AWS Mainframe Modernization for property valuation logic preservation, Amazon Location Service for GIS property mapping, AWS Application Migration Service for tax calculation systems, AWS B2B Data Interchange for municipal integration, Amazon API Gateway for citizen portal modernization, and AWS Audit Manager for state compliance automation",
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom municipal APIs, third-party GIS solutions, and basic citizen web portal",
        "Replace assessment system with SaaS, PostgreSQL for calculations, REST APIs for municipalities, cloud GIS platform, and modern citizen applications",
        "Keep mainframe hybrid, gradual Oracle migration, maintain municipal connections, upgrade GIS systems, and enhance citizen portal"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization preserves 50 years of property valuation logic while enabling cloud benefits, Location Service provides managed GIS capabilities for property mapping, MGN ensures seamless tax calculation migration, B2B Data Interchange handles diverse municipal data formats, API Gateway modernizes citizen portal with scalability, and Audit Manager automates state compliance requirements.",
        whyWrong: {
            1: "Aurora migration could break tax calculation procedures and third-party GIS adds cost and complexity",
            2: "SaaS replacement doesn't work for government with extensive customization and 50 years of data",
            3: "Hybrid approach doesn't achieve full modernization requirements and maintains legacy limitations"
        },
        examStrategy: "For government property tax migration: preserve valuation algorithms, Location Service provides managed GIS. B2B Data Interchange handles municipal integrations. Audit Manager automates government compliance."
    }
},
{
    id: 'sap_178',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global mining consortium operates 300 AWS accounts across 200 mines in 40 countries with harsh environmental conditions. Requirements include: environmental compliance monitoring for multiple governments, secure geological data sharing between partner companies, real-time safety monitoring with emergency response, equipment predictive maintenance, operational cost optimization, and protection of proprietary mining techniques.",
    question: "Which governance architecture addresses mining industry challenges while enabling international operations?",
    options: [
        "Basic Organizations structure, standard monitoring, manual environmental reporting, email-based data sharing, traditional safety systems, and spreadsheet cost tracking",
        "Separate Organizations per country, isolated monitoring systems, individual compliance processes, restricted data access, manual safety procedures, and project-based billing",
        "AWS Organizations with mining-specific environmental guardrails, AWS Clean Rooms for IP-protected geological data collaboration, AWS IoT SiteWise Edge for safety monitoring with offline resilience, Amazon Lookout for Equipment for predictive maintenance, AWS Cost Categories for mine-level optimization, and AWS B2B Data Interchange for automated government environmental reporting",
        "Hub-and-spoke architecture, centralized databases, shared monitoring systems, open data collaboration, unified safety platform, and consolidated billing"
    ],
    correct: 2,
    explanation: {
        correct: "Mining-specific environmental guardrails ensure automated compliance across 40 countries, Clean Rooms enables geological collaboration while protecting proprietary mining techniques, IoT SiteWise Edge provides safety monitoring with offline operation for remote mines, Lookout for Equipment predicts failures in harsh mining conditions, Cost Categories enable mine-level operational optimization, and B2B Data Interchange automates environmental reporting to multiple governments.",
        whyWrong: {
            0: "Standard controls don't address harsh mining environments and environmental compliance complexity",
            1: "Separate Organizations break collaboration and operational efficiency across the consortium",
            3: "Centralized databases violate IP protection and open collaboration exposes proprietary mining techniques"
        },
        examStrategy: "For mining operations: IoT SiteWise Edge handles harsh environments, Lookout for Equipment predicts industrial failures, Clean Rooms protects mining IP. Environmental guardrails ensure automated compliance."
    }
},
{
    id: 'sap_179',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous shipping company is developing a fleet management platform for 1,000 cargo vessels operating globally. Requirements include: autonomous navigation with collision avoidance, weather routing optimization, cargo monitoring for hazardous materials, port coordination for 500+ harbors, crew health monitoring, maritime regulatory compliance, and integration with global supply chains.",
    question: "Which architecture enables comprehensive autonomous maritime operations?",
    options: [
        "EC2 for navigation, Lambda for routing, IoT Core for cargo sensors, API Gateway for ports, CloudWatch for crew monitoring, and manual compliance tracking",
        "Custom navigation software, third-party weather services, DynamoDB for cargo data, Direct Connect to ports, wearable device integration, and basic regulatory processes",
        "Kubernetes for ship systems, machine learning on SageMaker, Timestream for sensor data, VPN to harbors, mobile apps for crew, and Config for compliance",
        "AWS Ground Station for satellite navigation, AWS IoT FleetWise for ship systems integration, Amazon Forecast with Weather Index for route optimization, AWS IoT Core for hazmat cargo monitoring, AWS Supply Chain for port coordination, AWS HealthLake for crew medical monitoring, and AWS B2B Data Interchange for maritime regulatory compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Ground Station provides global satellite coverage for autonomous navigation, IoT FleetWise manages ship systems with maritime protocols, Forecast with Weather Index optimizes routes considering ocean conditions, IoT Core monitors hazardous cargo with compliance tracking, Supply Chain coordinates with 500+ ports globally, HealthLake manages crew health with maritime medical standards, and B2B Data Interchange handles international maritime regulations.",
        whyWrong: {
            0: "Generic EC2 navigation lacks autonomous ship capabilities and manual compliance doesn't scale globally",
            1: "Custom navigation requires extensive development and third-party weather lacks AWS integration",
            2: "Kubernetes adds complexity for ship operations and VPN doesn't provide reliable port connectivity"
        },
        examStrategy: "For autonomous maritime: Ground Station for satellite navigation, IoT FleetWise for ship systems, Supply Chain for port coordination. Weather Index optimizes maritime routing with ocean conditions."
    }
},
{
    id: 'sap_180',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A B2B marketplace connects 100,000 suppliers with 50,000 buyers using: RDS PostgreSQL cluster ($300K/month), 500 EC2 instances for order processing ($400K/month), ElastiCache for catalog search ($150K/month), S3 for product catalogs ($100K/month). Issues include: catalog search taking 8+ seconds, order processing delays during business hours, database connection exhaustion during month-end, cache memory pressure during promotional periods, and 50% infrastructure idle during nights/weekends.",
    question: "Which optimization strategy provides the BEST performance improvement and cost reduction for B2B marketplace operations?",
    options: [
        "Add more RDS read replicas, scale EC2 instances, increase cache cluster size, optimize S3 configurations, and implement Reserved Instances for cost savings",
        "Migrate to Amazon OpenSearch Serverless for sub-second catalog search, implement Lambda with SQS for elastic order processing, upgrade to Aurora Serverless v2 for automatic scaling, use MemoryDB for promotional cache performance, and implement S3 Intelligent-Tiering for catalog optimization",
        "Containerize on EKS with autoscaling, add Aurora Global Database, implement ElastiCache auto-scaling, enable S3 lifecycle policies, and use Spot instances for development",
        "Optimize database queries, implement EC2 Auto Scaling, add Redis clustering, compress S3 objects, and purchase Compute Savings Plans"
    ],
    correct: 1,
    explanation: {
        correct: "OpenSearch Serverless eliminates search infrastructure management while providing sub-second catalog search with ML relevance, Lambda with SQS enables elastic order processing eliminating 50% idle costs, Aurora Serverless v2 prevents connection exhaustion through automatic scaling, MemoryDB handles promotional traffic with clustering and durability, and S3 Intelligent-Tiering optimizes catalog storage costs automatically.",
        whyWrong: {
            0: "Adding more resources increases costs without solving fundamental search performance and idle capacity issues",
            2: "EKS adds operational complexity and Aurora Global Database is expensive overkill for single-region marketplace",
            3: "Optimizing current architecture doesn't solve 8-second search times or eliminate 50% idle infrastructure"
        },
        examStrategy: "For B2B marketplace optimization: OpenSearch Serverless provides managed search with ML, Lambda eliminates idle costs, Aurora Serverless v2 handles business hour spikes. MemoryDB provides durable cache performance."
    }
},

// Questions 181-200 with planned distribution: Domain 1(5), Domain 2(6), Domain 3(5), Domain 4(4)
// Answer distribution: A(5), B(5), C(5), D(5)
{
    id: 'sap_181',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment firm manages $5 trillion in assets across 1,000 AWS accounts spanning hedge funds, private equity, real estate, and institutional clients. Requirements include: complete fund isolation for regulatory compliance, real-time risk monitoring across all portfolios, automated compliance with SEC, FINRA, FCA, and 25+ international regulations, secure collaboration with external auditors, and precise cost allocation to individual investment strategies.",
    question: "Which governance architecture provides comprehensive financial services compliance while enabling efficient operations?",
    options: [
        "AWS Organizations with investment-specific OUs and automated regulatory guardrails, Amazon FinSpace for financial data management with built-in risk analytics, AWS Clean Rooms for auditor collaboration without exposing trading strategies, AWS Cost Categories with strategy-level allocation, AWS Audit Manager for multi-regulatory compliance frameworks, and AWS Security Lake for unified threat detection across all funds while maintaining isolation",
        "Separate Organizations per fund type, manual risk calculations, individual auditor access, spreadsheet cost tracking, separate compliance processes, and isolated security monitoring",
        "Single Organization with IAM policies, CloudWatch for basic monitoring, S3 for data sharing, tagging for costs, manual compliance tracking, and GuardDuty for security",
        "Hub-and-spoke architecture, centralized risk databases, API-based auditor access, automated billing, shared compliance systems, and Security Hub federation"
    ],
    correct: 0,
    explanation: {
        correct: "Investment-specific OUs with regulatory guardrails ensure automated compliance across 25+ jurisdictions, FinSpace provides purpose-built financial analytics with real-time risk monitoring, Clean Rooms enables auditor collaboration while protecting proprietary strategies, Cost Categories automate complex strategy-level allocation, Audit Manager handles multiple regulatory frameworks, and Security Lake provides unified monitoring without compromising fund isolation.",
        whyWrong: {
            1: "Separate Organizations break operational efficiency and cost consolidation for $5T asset management",
            2: "Basic controls don't meet sophisticated financial compliance requirements for international investment management",
            3: "Centralized databases violate regulatory fund isolation requirements"
        },
        examStrategy: "For investment management: FinSpace provides financial industry features, Clean Rooms protects trading strategies, Audit Manager handles multiple regulatory frameworks. Fund isolation is critical for financial compliance."
    }
},
{
    id: 'sap_182',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A space exploration company is building mission control for deep space missions to Mars and beyond. Requirements include: 20-minute communication delays, autonomous spacecraft operation during blackouts, processing 500TB of scientific data daily, 3D planetary mapping, integration with international space agencies, real-time collaboration between global research teams, and 50-year mission data archival.",
    question: "Which architecture handles deep space mission requirements with extreme communication constraints?",
    options: [
        "EC2 for mission control, S3 for data storage, Lambda for processing, Direct Connect to space agencies, and basic collaboration tools",
        "AWS Ground Station for deep space communication with delay handling, AWS IoT Greengrass for autonomous spacecraft logic during blackouts, Amazon ParallelCluster for scientific computing, Amazon FSx for Lustre for high-performance data processing, AWS B2B Data Interchange for space agency coordination, and S3 Intelligent-Tiering with Glacier Deep Archive for 50-year retention",
        "Custom ground stations, Kubernetes for autonomy, EMR for data processing, EFS for storage, VPN to agencies, and manual collaboration",
        "Satellite modems on EC2, containers for spacecraft logic, Redshift for analytics, NFS for data, API Gateway for agencies, and tape storage"
    ],
    correct: 1,
    explanation: {
        correct: "Ground Station handles deep space protocols with 20-minute delay buffering, Greengrass enables autonomous spacecraft operation during communication blackouts, ParallelCluster provides HPC for 500TB daily scientific processing, FSx for Lustre delivers high-performance computing storage, B2B Data Interchange manages international space agency protocols, and S3 Intelligent-Tiering with Deep Archive optimizes 50-year storage costs.",
        whyWrong: {
            0: "Generic EC2 lacks space communication protocols and Direct Connect doesn't handle space agency networks",
            2: "Custom ground stations lack global redundancy and Kubernetes isn't optimized for spacecraft autonomy",
            3: "Satellite modems don't provide deep space capabilities and tape storage lacks accessibility for long missions"
        },
        examStrategy: "For space missions: Ground Station provides specialized communication with delay handling, Greengrass enables autonomous edge operation. ParallelCluster for scientific HPC, FSx for Lustre for high-performance data processing."
    }
},
{
    id: 'sap_183',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global e-commerce platform serves 500 million customers with monthly costs of $12M: CloudFront ($4M), EC2 for microservices ($3M), OpenSearch cluster ($2M), RDS PostgreSQL ($2M), S3 storage ($1M). Issues include: search response times >10 seconds during sales events, microservice scaling taking 20 minutes, database connection exhaustion during Black Friday, CloudFront cache misses for personalized content, and massive infrastructure over-provisioning for flash sales.",
    question: "Which comprehensive optimization provides the GREATEST performance improvement and cost reduction?",
    options: [
        "Add more OpenSearch nodes, pre-scale EC2 instances, increase RDS connections, optimize CloudFront configurations, and implement S3 lifecycle policies",
        "Containerize on EKS with KEDA, add OpenSearch auto-scaling, implement Aurora Global Database, use CloudFront behaviors, and purchase Reserved Instances",
        "Migrate to OpenSearch Serverless for automatic scaling with ML-powered search relevance, implement Lambda with container image support for instant microservice scaling, upgrade to Aurora Serverless v2 for automatic database scaling, add CloudFront with Lambda@Edge for personalized content optimization, and eliminate over-provisioning through serverless architecture achieving 60% cost reduction",
        "Optimize current configurations, implement caching layers, add connection pooling, use CDN compression, and negotiate volume discounts"
    ],
    correct: 2,
    explanation: {
        correct: "OpenSearch Serverless eliminates cluster management while ML search reduces response time to <1 second during sales, Lambda containers provide instant microservice scaling eliminating 20-minute delays, Aurora Serverless v2 prevents Black Friday connection exhaustion through auto-scaling, Lambda@Edge enables personalized content at edge, and serverless architecture eliminates over-provisioning achieving 60% cost reduction.",
        whyWrong: {
            0: "Adding more resources increases costs without solving fundamental scaling and personalization issues",
            1: "EKS adds complexity and Aurora Global Database is expensive overkill for single-region e-commerce",
            3: "Optimizing current architecture doesn't solve fundamental over-provisioning and scaling delay issues"
        },
        examStrategy: "For e-commerce optimization: OpenSearch Serverless with ML improves search performance, Lambda containers enable instant scaling. Aurora Serverless v2 handles traffic spikes, Lambda@Edge personalizes at edge."
    }
},
{
    id: 'sap_184',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national railway system must migrate train control and passenger systems to AWS within 22 months for safety compliance. Current systems include: mainframe COBOL for train scheduling with 60 years of route optimization, Oracle databases for ticket sales, SCADA for signal control, integration with 500 stations, and compliance with federal transportation safety regulations. Zero service disruption is mandatory.",
    question: "Which migration approach maintains railway safety while achieving modernization compliance?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, IoT Core for signals, custom station integration, and basic safety monitoring",
        "Replace railway system with modern platform, PostgreSQL for all data, cloud signal control, API Gateway for stations, and automated safety compliance",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain SCADA on-premises, upgrade station systems, and manual compliance tracking",
        "AWS Mainframe Modernization for train scheduling logic preservation, AWS IoT SiteWise Edge for local signal control ensuring safety during outages, Amazon RDS Custom for Oracle maintaining railway procedures, AWS B2B Data Interchange for station integration, AWS IoT Analytics for safety monitoring, and AWS Audit Manager for federal transportation compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 60 years of proven route optimization while enabling cloud benefits, IoT SiteWise Edge maintains local signal control for safety during connectivity failures, RDS Custom preserves Oracle railway procedures, B2B Data Interchange handles diverse station protocols, IoT Analytics provides real-time safety monitoring, and Audit Manager automates federal transportation compliance.",
        whyWrong: {
            0: "Aurora migration could break railway procedures and IoT Core signal migration risks public safety",
            1: "Complete replacement of 60-year railway system is impossible in 22 months and risks safety",
            2: "Hybrid approach doesn't achieve modernization compliance and maintains legacy vulnerabilities"
        },
        examStrategy: "For railway migration: preserve proven scheduling algorithms, IoT SiteWise Edge maintains local safety control. Never migrate safety-critical systems without local backup. B2B Data Interchange handles transportation standards."
    }
},
{
    id: 'sap_185',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global pharmaceutical company conducts clinical trials across 80 countries with 200 AWS accounts. Requirements include: complete trial data isolation between competing drugs, secure collaboration with 1,000 research institutions, automated compliance with FDA, EMA, and local health authorities, patient privacy protection across jurisdictions, and unified safety reporting for adverse events.",
    question: "Which governance model enables global pharmaceutical research while protecting competitive interests?",
    options: [
        "AWS Organizations with drug-specific OUs for complete trial isolation, AWS Clean Rooms for IP-protected research collaboration with institutions, AWS HealthLake for HIPAA/GDPR-compliant patient data management, Amazon Monitron for real-time adverse event detection across trials, AWS Audit Manager for multi-country health authority compliance, and AWS DataZone for governed safety reporting with data lineage",
        "Separate Organizations per therapeutic area, manual institutional collaboration, custom clinical databases, traditional safety monitoring, individual compliance processes, and isolated reporting systems",
        "Single Organization with IAM policies, S3 bucket sharing for research, RDS for patient data, CloudWatch for safety monitoring, manual compliance tracking, and basic reporting tools",
        "Hub-and-spoke architecture, centralized trial databases, API-based collaboration, shared safety monitoring, unified compliance platform, and automated reporting"
    ],
    correct: 0,
    explanation: {
        correct: "Drug-specific OUs ensure complete competitive trial isolation, Clean Rooms enables research collaboration while protecting competing drug IP, HealthLake provides multi-jurisdictional patient privacy compliance, Monitron detects adverse events in real-time across all trials, Audit Manager automates health authority compliance across 80 countries, and DataZone provides governed safety reporting with full lineage.",
        whyWrong: {
            1: "Separate Organizations break collaboration capabilities and multiply compliance management overhead",
            2: "Basic controls don't provide trial isolation required for competing drug development",
            3: "Centralized databases violate competitive trial isolation and expose proprietary research"
        },
        examStrategy: "For pharmaceutical trials: Clean Rooms protects competitive IP during collaboration, HealthLake provides multi-jurisdictional compliance, Monitron enables real-time safety monitoring. DataZone ensures data governance for regulatory reporting."
    }
},
{
    id: 'sap_186',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A renewable energy consortium is building a carbon credit trading platform connecting 50,000 renewable energy producers, 10,000 corporations buying credits, and integration with global carbon markets. Requirements include: real-time carbon credit pricing, blockchain-based settlement for transparency, automated verification of renewable energy production, compliance with international carbon standards, and fraud detection for credit authenticity.",
    question: "Which architecture enables transparent carbon credit trading at global scale?",
    options: [
        "DynamoDB for pricing, custom blockchain on EC2, Lambda for verification, API Gateway for markets, and manual fraud detection",
        "Amazon MemoryDB for real-time carbon credit pricing, Amazon Managed Blockchain for transparent settlement with regulatory audit trails, AWS IoT SiteWise for automated renewable energy verification, AWS B2B Data Interchange for global carbon market integration, Amazon Fraud Detector for credit authenticity validation, and AWS Audit Manager for international carbon standard compliance",
        "ElastiCache for pricing, Ethereum on containers, SageMaker for verification, Direct Connect to markets, and custom fraud algorithms",
        "RDS for credit data, third-party blockchain, Batch for verification, VPN to markets, and rule-based fraud detection"
    ],
    correct: 1,
    explanation: {
        correct: "MemoryDB provides consistent real-time pricing for 50,000+ producers, Managed Blockchain offers enterprise-grade transparency with regulatory audit capabilities, IoT SiteWise automates renewable energy production verification with industrial protocols, B2B Data Interchange handles diverse global carbon market standards, Fraud Detector uses ML to validate credit authenticity, and Audit Manager automates international carbon compliance.",
        whyWrong: {
            0: "Custom blockchain lacks enterprise features needed for regulatory compliance and global markets",
            2: "Ethereum containers require significant management and ElastiCache alone can't handle global pricing complexity",
            3: "Third-party blockchain adds complexity and VPN doesn't scale to global carbon market integration"
        },
        examStrategy: "For carbon trading: MemoryDB provides real-time pricing at scale, Managed Blockchain offers enterprise features vs custom. IoT SiteWise verifies renewable production, Fraud Detector validates credit authenticity with ML."
    }
},
{
    id: 'sap_187',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A video streaming service with 50 million subscribers uses: EC2 for transcoding ($600K/month), S3 for video storage ($400K/month), CloudFront for delivery ($300K/month), RDS for metadata ($100K/month). Issues include: transcoding queues backing up 72+ hours during releases, 98% of content never watched after 6 months, CloudFront costs spiking 400% during premieres, database timeouts during concurrent viewing peaks, and scaling delays causing viewer frustration.",
    question: "Which optimization strategy provides the MOST comprehensive cost reduction and performance improvement?",
    options: [
        "Add more EC2 transcoding capacity, implement S3 lifecycle policies, optimize CloudFront configurations, scale RDS instances, and implement caching layers",
        "Containerize transcoding on EKS, manually move old content to Glacier, add CloudFront behaviors, implement Aurora read replicas, and use Spot instances",
        "Migrate to AWS Elemental MediaConvert for serverless parallel transcoding eliminating queues, implement S3 Intelligent-Tiering moving 98% content to Archive tiers, add CloudFront Origin Shield reducing premiere costs 85%, upgrade to Aurora Serverless v2 for automatic scaling, and implement Lambda@Edge for viewer-optimized content delivery",
        "Optimize transcoding settings, compress video files, increase CloudFront TTLs, add connection pooling, and purchase Reserved Instances"
    ],
    correct: 2,
    explanation: {
        correct: "MediaConvert provides serverless parallel transcoding eliminating 72-hour queues, S3 Intelligent-Tiering automatically moves 98% to cheaper Archive tiers saving $390K annually, Origin Shield reduces premiere costs 85% by eliminating duplicate requests, Aurora Serverless v2 prevents viewing peak timeouts through auto-scaling, and Lambda@Edge optimizes delivery reducing transcoding load.",
        whyWrong: {
            0: "Adding more EC2 increases costs without solving fundamental queuing and scaling architecture issues",
            1: "Manual Glacier movement is error-prone and EKS adds complexity without solving transcoding queue bottlenecks",
            3: "Optimizing current architecture doesn't solve 72-hour queues or eliminate massive cost spikes during premieres"
        },
        examStrategy: "For video streaming: MediaConvert eliminates transcoding queues through parallel processing, S3 Intelligent-Tiering automates massive storage savings. Origin Shield prevents premiere cost spikes."
    }
},
{
    id: 'sap_188',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A regional credit union must migrate core banking systems to AWS within 15 months to reduce costs 50%. Current systems include: IBM AS/400 with member account management, Oracle database for loan processing, Windows teller applications, ATM network integration, and Federal Reserve wire transfer systems. Members cannot experience any service disruption.",
    question: "Which migration strategy achieves cost reduction while maintaining banking service continuity?",
    options: [
        "Lift-and-shift AS/400 to EC2, Oracle to Aurora migration, containerize teller apps, cloud ATM management, and custom Federal Reserve integration",
        "Replace core banking with SaaS solution, PostgreSQL for loans, web-based teller systems, IoT for ATMs, and modern payment APIs",
        "Keep AS/400 hybrid, gradual Oracle migration, modernize Windows apps, upgrade ATM systems, and maintain Federal Reserve connections",
        "AWS Mainframe Modernization for AS/400 member management with 50% cost reduction, Amazon RDS Custom for Oracle maintaining loan procedures, AWS Application Migration Service for teller applications, AWS IoT Device Management for ATM fleet, and AWS B2B Data Interchange for Federal Reserve wire integration"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization provides 50% cost reduction while preserving AS/400 member management logic, RDS Custom maintains Oracle loan procedures critical for credit union operations, MGN ensures zero-downtime teller migration, IoT Device Management modernizes ATM fleet monitoring, and B2B Data Interchange handles Federal Reserve wire protocols natively.",
        whyWrong: {
            0: "Lift-and-shift doesn't achieve 50% cost reduction and Aurora migration risks breaking loan calculations",
            1: "SaaS replacement isn't viable for credit union with 15-month timeline and member service requirements",
            2: "Hybrid approach doesn't achieve full cost reduction goals and maintains data center expenses"
        },
        examStrategy: "For credit union cost reduction: Mainframe Modernization provides significant savings while preserving banking logic. RDS Custom maintains database compatibility. B2B Data Interchange handles banking industry protocols."
    }
},
{
    id: 'sap_189',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global telecommunications operator with 800 AWS accounts serves 1 billion customers across 120 countries. Requirements include: data sovereignty compliance with each country's telecom laws, real-time network optimization across all regions, unified billing for international roaming, secure sharing of network performance data with regulators, prevention of customer data crossing borders, and 99.999% availability for emergency services.",
    question: "Which governance architecture provides global telecom operations while ensuring regulatory compliance?",
    options: [
        "AWS Organizations with country-specific OUs and data residency guardrails, Amazon CloudWatch cross-region dashboards for network monitoring, AWS Clean Rooms for regulatory data sharing without exposing customer information, AWS Cost Categories for international roaming billing, AWS Local Zones for emergency services latency, and AWS B2B Data Interchange for automated regulatory reporting",
        "Separate Organizations per region, manual network monitoring, individual billing systems, direct regulator access, basic emergency systems, and manual compliance reporting",
        "Single Organization with regional VPCs, centralized monitoring, consolidated billing, S3 for data sharing, Multi-AZ deployments, and quarterly compliance reports",
        "Hub-and-spoke architecture, unified databases, shared monitoring, automated billing, distributed emergency systems, and API-based regulatory integration"
    ],
    correct: 0,
    explanation: {
        correct: "Country-specific OUs with data residency guardrails ensure automatic telecom law compliance, CloudWatch cross-region provides unified monitoring without data movement, Clean Rooms enables regulatory sharing while protecting customer privacy, Cost Categories handle complex international roaming billing, Local Zones provide <10ms emergency services latency, and B2B Data Interchange automates regulatory reporting across 120 countries.",
        whyWrong: {
            1: "Separate Organizations break roaming coordination and create massive management complexity for 800 accounts",
            2: "Centralized approach violates data sovereignty requirements for telecommunications",
            3: "Unified databases violate customer data isolation requirements across country borders"
        },
        examStrategy: "For global telecom: data sovereignty is paramount, Clean Rooms enables regulatory compliance without data exposure. Local Zones provide emergency services latency. B2B Data Interchange handles telecom regulatory standards."
    }
},
{
    id: 'sap_190',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart agriculture company is developing an AI-powered farming platform for 500,000 farms globally. Requirements include: satellite imagery analysis for crop health monitoring, soil sensor data from 100 million IoT devices, drone-based pest detection, weather-driven irrigation optimization, supply chain coordination with 5,000 food processors, and compliance with organic certification in 80 countries.",
    question: "Which architecture provides comprehensive precision agriculture at massive scale?",
    options: [
        "EC2 for imagery processing, Kinesis for sensor data, Lambda for pest detection, custom irrigation logic, API Gateway for supply chain, and manual certification tracking",
        "Amazon SageMaker Geospatial for satellite crop analysis with agricultural ML models, AWS IoT Core with Basic Ingest for cost-effective sensor processing, Amazon Rekognition Custom Labels for drone pest detection, Amazon Forecast with Weather Index for irrigation optimization, AWS Supply Chain for food processor coordination, and AWS Audit Manager for organic certification compliance across 80 countries",
        "Custom ML models on GPU clusters, DynamoDB for sensor storage, computer vision on EC2, manual irrigation scheduling, Direct Connect to processors, and third-party certification platforms",
        "Batch processing for satellite data, Timestream for sensors, generic ML on SageMaker, rule-based irrigation, MSK for supply chain events, and Config for compliance"
    ],
    correct: 1,
    explanation: {
        correct: "SageMaker Geospatial provides purpose-built satellite imagery analysis with agricultural ML models, IoT Core Basic Ingest reduces costs for 100M sensors while maintaining scale, Rekognition Custom Labels enables training farm-specific pest detection models, Forecast with Weather Index optimizes irrigation using integrated weather data, Supply Chain provides end-to-end food processor coordination, and Audit Manager automates organic certification across 80 countries.",
        whyWrong: {
            0: "Generic EC2 processing lacks agricultural-specific ML capabilities and manual certification doesn't scale globally",
            2: "Custom ML requires extensive expertise and DynamoDB is expensive for 100M device time-series data",
            3: "Batch processing can't provide real-time irrigation optimization and generic ML lacks agricultural specificity"
        },
        examStrategy: "For precision agriculture: SageMaker Geospatial for satellite analysis with agricultural ML, IoT Core Basic Ingest for massive IoT cost optimization. Weather Index provides agricultural weather integration."
    }
},
{
    id: 'sap_191',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A cryptocurrency exchange handles 50 million trades daily with costs of $2.5M/month: DynamoDB ($800K), Lambda ($600K), API Gateway ($400K), CloudFront ($300K), S3 ($200K), ElastiCache ($200K). Issues include: DynamoDB throttling during market crashes, Lambda timeouts during high volatility, API Gateway 5xx errors during flash trading, CloudFront cache misses for real-time prices, and exponential cost growth with trading volume.",
    question: "Which optimization provides the BEST performance and cost control for extreme trading volumes?",
    options: [
        "Increase DynamoDB provisioned capacity, use Lambda reserved concurrency, raise API Gateway limits, optimize CloudFront behaviors, and implement S3 lifecycle policies",
        "Add Aurora for consistency, containerize Lambda functions, implement API caching, add CloudFront distributions, and use ElastiCache clustering",
        "Implement DynamoDB on-demand with DAX for microsecond latency, migrate to Lambda with provisioned concurrency for consistent performance, add API Gateway regional endpoints with edge caching, implement CloudFront with Lambda@Edge for real-time price optimization, and use ElastiCache Global Datastore for multi-region consistency",
        "Switch to Aurora Global Database, implement Step Functions for orchestration, use Application Load Balancer, deploy multiple CDNs, and add Redis clustering"
    ],
    correct: 2,
    explanation: {
        correct: "DynamoDB on-demand with DAX provides automatic scaling with microsecond latency for market crashes, Lambda provisioned concurrency eliminates timeouts during volatility, API Gateway regional endpoints with edge caching handle flash trading spikes, Lambda@Edge processes real-time pricing at edge reducing costs, and ElastiCache Global Datastore ensures price consistency across regions.",
        whyWrong: {
            0: "Pre-provisioning leads to over-provisioning costs and doesn't handle unpredictable crypto market spikes",
            1: "Aurora adds complexity for trading systems optimized for DynamoDB's performance characteristics",
            3: "Step Functions add latency inappropriate for high-frequency trading and multiple CDNs add complexity"
        },
        examStrategy: "For crypto trading: DynamoDB on-demand handles volatility cost-effectively, DAX provides microsecond latency. Lambda provisioned concurrency prevents timeouts. Lambda@Edge optimizes real-time pricing."
    }
},
{
    id: 'sap_192',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state department of health must migrate disease surveillance systems to AWS within 18 months for pandemic preparedness. Current systems include: mainframe for disease reporting with 30 years of epidemiological data, Oracle databases for laboratory results, integration with 500 hospitals, real-time contact tracing capabilities, and compliance with CDC reporting requirements.",
    question: "Which migration approach maintains public health capabilities while enabling pandemic response?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom hospital APIs, manual contact tracing, and basic CDC reporting",
        "Replace health system with modern platform, PostgreSQL for all data, API Gateway for hospitals, automated contact tracing, and cloud compliance tools",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain hospital connections, upgrade tracing systems, and manual CDC compliance",
        "AWS Mainframe Modernization for disease reporting logic preservation, AWS HealthLake for HIPAA-compliant laboratory data with FHIR support, AWS B2B Data Interchange for hospital integration, Amazon Location Service for contact tracing optimization, and AWS Audit Manager for CDC compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 30 years of epidemiological algorithms while enabling cloud scalability, HealthLake provides HIPAA-compliant laboratory data management with FHIR interoperability, B2B Data Interchange handles diverse hospital HL7 formats, Location Service optimizes contact tracing with privacy protection, and Audit Manager automates CDC reporting compliance.",
        whyWrong: {
            0: "Aurora migration could break epidemiological calculations and manual tracing doesn't scale for pandemics",
            1: "Complete replacement risks public health operations and loses 30 years of disease surveillance data",
            2: "Hybrid approach doesn't enable pandemic scale capabilities and maintains legacy limitations"
        },
        examStrategy: "For public health migration: preserve epidemiological algorithms, HealthLake provides healthcare compliance with FHIR. Location Service enables privacy-preserving contact tracing. B2B Data Interchange handles healthcare standards."
    }
},
{
    id: 'sap_193',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational manufacturing company operates 800 factories across 60 countries with 300 AWS accounts. Requirements include: environmental compliance reporting to multiple governments, secure sharing of production data with suppliers, real-time quality monitoring across all facilities, cost allocation to individual product lines, and protection of proprietary manufacturing processes.",
    question: "Which governance architecture addresses manufacturing industry requirements while enabling global operations?",
    options: [
        "AWS Organizations with product-line OUs and environmental guardrails, AWS Clean Rooms for IP-protected supplier collaboration, AWS IoT SiteWise for industrial quality monitoring, AWS Cost Categories for product-line allocation, AWS DataZone for governed production data sharing, and AWS B2B Data Interchange for automated government environmental reporting",
        "Separate Organizations per country, manual environmental reporting, basic supplier portals, individual quality systems, spreadsheet cost tracking, and isolated data management",
        "Single Organization with basic IAM, S3 for data sharing, CloudWatch monitoring, tagging for costs, manual data governance, and quarterly environmental reports",
        "Hub-and-spoke architecture, centralized production databases, API-based supplier access, shared monitoring, automated billing, and unified environmental platform"
    ],
    correct: 0,
    explanation: {
        correct: "Product-line OUs with environmental guardrails ensure automated compliance across 60 countries, Clean Rooms enables supplier collaboration while protecting manufacturing IP, IoT SiteWise provides industrial-grade quality monitoring for 800 factories, Cost Categories enable precise product-line allocation, DataZone governs production data sharing with lineage, and B2B Data Interchange automates environmental reporting to multiple governments.",
        whyWrong: {
            1: "Separate Organizations break supplier coordination and create massive environmental compliance overhead",
            2: "Basic controls don't provide manufacturing industry compliance and IP protection requirements",
            3: "Centralized databases violate IP protection between competing product lines and proprietary processes"
        },
        examStrategy: "For global manufacturing: IoT SiteWise monitors industrial operations, Clean Rooms protects manufacturing IP in supplier collaboration. DataZone provides data governance for production sharing."
    }
},
{
    id: 'sap_194',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national emergency response agency is building a disaster coordination platform managing 50,000 first responders across natural disasters. Requirements include: real-time personnel tracking during emergencies, satellite communication backup during infrastructure failures, AI-powered resource allocation, integration with 1,000 local emergency services, predictive disaster impact modeling, and compliance with federal emergency management standards.",
    question: "Which architecture provides comprehensive disaster response coordination capabilities?",
    options: [
        "IoT Core for responder tracking, Direct Connect for reliability, Lambda for resource allocation, API Gateway for local services, SageMaker for predictions, and Config for compliance",
        "AWS IoT Core for emergency device tracking with priority protocols, AWS Ground Station for satellite backup during infrastructure outages, AWS Supply Chain for emergency resource optimization, Amazon Forecast for disaster impact prediction, AWS B2B Data Interchange for automated local service coordination, and AWS Audit Manager for federal emergency compliance",
        "Custom tracking systems, satellite modems, manual resource coordination, point-to-point integration, historical analysis, and manual compliance processes",
        "EC2 for coordination, GPS mobile apps, VPN communication, spreadsheet allocation, basic forecasting, and quarterly compliance reviews"
    ],
    correct: 1,
    explanation: {
        correct: "IoT Core handles 50,000 responder devices with emergency communication priorities, Ground Station provides satellite backup when terrestrial infrastructure fails during disasters, Supply Chain optimizes emergency resource allocation across affected areas, Forecast predicts disaster impact using weather and historical data, B2B Data Interchange automates coordination with 1,000 local services, and Audit Manager ensures federal emergency compliance.",
        whyWrong: {
            0: "Direct Connect fails during disasters and doesn't provide satellite backup capabilities",
            2: "Custom systems require extensive development and manual coordination doesn't scale to 50,000 responders",
            3: "GPS apps fail during infrastructure outages and manual processes can't handle large-scale emergency coordination"
        },
        examStrategy: "For emergency response: Ground Station provides disaster-resilient satellite backup, Supply Chain optimizes resource allocation at scale. B2B Data Interchange handles emergency service standards."
    }
},
{
    id: 'sap_195',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A food delivery platform handles 30 million orders daily using: 2,000 EC2 instances ($1.2M/month), DynamoDB ($500K/month), ElastiCache ($300K/month), API Gateway ($200K/month). Issues include: order processing delays during meal peaks, driver location inconsistencies causing poor matching, cache memory pressure during promotional campaigns, API throttling during flash sales, and 65% infrastructure idle during overnight hours.",
    question: "Which optimization strategy provides the BEST performance improvement and cost reduction for variable food delivery traffic?",
    options: [
        "Implement Auto Scaling with predictive scaling, add DynamoDB auto-scaling, use ElastiCache clustering, optimize API configurations, and purchase Reserved Instances",
        "Containerize on EKS with Karpenter, add DynamoDB global tables, implement Redis read replicas, increase API Gateway quotas, and use Spot instances",
        "Migrate to ECS Fargate Spot for elastic order processing eliminating idle costs, implement DynamoDB Global Tables for consistent driver locations, upgrade to MemoryDB with clustering for promotional traffic, add API Gateway regional endpoints with caching, and achieve 65% cost reduction through serverless-first architecture",
        "Keep current setup but optimize configurations, add connection pooling, implement application caching, use CDN for mobile apps, and negotiate enterprise pricing"
    ],
    correct: 2,
    explanation: {
        correct: "Fargate Spot eliminates 65% idle costs while providing instant scaling for meal peaks, DynamoDB Global Tables ensure driver location consistency preventing poor matching, MemoryDB clustering scales memory for promotional campaigns with durability, API Gateway regional endpoints with caching handle flash sales efficiently, and serverless-first architecture eliminates overnight idle infrastructure.",
        whyWrong: {
            0: "Predictive scaling still maintains some idle capacity and doesn't solve location consistency issues",
            1: "EKS adds operational complexity and doesn't achieve the cost optimization of true serverless",
            3: "Optimizing current architecture doesn't eliminate 65% idle capacity or solve fundamental scaling issues"
        },
        examStrategy: "For variable food delivery: Fargate Spot eliminates idle costs, DynamoDB Global Tables solve location consistency. MemoryDB provides durable performance for campaigns. Serverless-first maximizes cost efficiency."
    }
},
{
    id: 'sap_196',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state motor vehicle department must migrate driver licensing systems to AWS within 16 months for REAL ID compliance. Current systems include: mainframe for license validation with 50 years of driving records, Oracle databases for DMV transactions, photo capture systems with facial recognition, integration with law enforcement databases, and federal background check systems.",
    question: "Which migration approach maintains public services while achieving REAL ID compliance?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, cloud photo processing, REST APIs for law enforcement, and manual background checks",
        "Replace DMV system with modern platform, PostgreSQL for all data, mobile photo capture, API Gateway for integrations, and automated background processing",
        "Keep mainframe with hybrid connectivity, gradual Oracle migration, upgrade photo systems, maintain law enforcement connections, and enhance background systems",
        "AWS Mainframe Modernization for license validation logic preservation, Amazon RDS Custom for Oracle maintaining DMV procedures, AWS IoT Core for photo capture device management, Amazon Rekognition for facial recognition with law enforcement integration, and AWS B2B Data Interchange for federal background check connectivity"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 50 years of driving record logic while enabling REAL ID compliance, RDS Custom maintains Oracle DMV procedures, IoT Core manages photo capture devices at scale, Rekognition provides facial recognition with law enforcement database integration, and B2B Data Interchange handles federal background check protocols natively.",
        whyWrong: {
            0: "Aurora migration could break DMV transaction procedures and manual background checks don't meet REAL ID requirements",
            1: "Complete replacement risks public safety and loses decades of driving record validation logic",
            2: "Hybrid approach doesn't achieve full REAL ID compliance and maintains legacy security vulnerabilities"
        },
        examStrategy: "For DMV REAL ID migration: preserve validation logic with modernization, Rekognition provides law enforcement facial recognition integration. B2B Data Interchange handles federal background check protocols."
    }
},
{
    id: 'sap_197',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global aerospace consortium develops aircraft with 500 AWS accounts across 40 partner companies in 25 countries. Requirements include: complete IP protection between competing partners, secure collaboration on approved joint projects, export control compliance (ITAR/EAR), supply chain traceability for safety regulations, and cost allocation to individual aircraft programs across partners.",
    question: "Which governance model enables aerospace collaboration while maintaining competitive protection?",
    options: [
        "AWS Organizations with partner-specific OUs and ITAR-compliant guardrails, AWS Clean Rooms for IP-protected project collaboration, AWS Supply Chain for component traceability, AWS DataZone for governed aerospace data sharing, AWS Cost Categories for cross-partner program allocation, and AWS Audit Manager for export control compliance automation",
        "Separate Organizations per partner, manual collaboration processes, individual supply chain tracking, isolated data systems, project-based billing, and manual export control monitoring",
        "Single Organization with IAM policies, S3 bucket sharing, manual supply chain management, basic data governance, spreadsheet cost allocation, and quarterly ITAR reviews",
        "Hub-and-spoke architecture, centralized project databases, API-based collaboration, shared supply chain systems, automated billing, and unified export control platform"
    ],
    correct: 0,
    explanation: {
        correct: "Partner-specific OUs with ITAR guardrails ensure automated export control compliance, Clean Rooms enables joint project collaboration while protecting competing partner IP, Supply Chain provides end-to-end component traceability for aviation safety, DataZone governs sensitive aerospace data sharing with lineage, Cost Categories enable complex cross-partner allocation, and Audit Manager automates ITAR/EAR compliance.",
        whyWrong: {
            1: "Separate Organizations eliminate collaboration benefits and create massive complexity for 500 accounts",
            2: "Basic controls don't meet ITAR export control requirements and lack IP protection for aerospace competition",
            3: "Centralized databases violate ITAR requirements and expose competing partner intellectual property"
        },
        examStrategy: "For aerospace consortiums: ITAR compliance is critical, Clean Rooms protects IP in joint development, Supply Chain provides safety traceability. DataZone ensures governance for sensitive aerospace data."
    }
},
{
    id: 'sap_198',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A precision medicine company is developing personalized cancer treatment using multi-omics data. Requirements include: analysis of genomics, proteomics, and metabolomics for 100,000 patients, AI-driven drug combination predictions, real-time biomarker monitoring, clinical trial matching, integration with 300 cancer centers, and FDA compliance for clinical decision support systems.",
    question: "Which architecture enables comprehensive precision oncology while maintaining regulatory compliance?",
    options: [
        "Custom bioinformatics on GPU clusters, RDS for patient data, Lambda for drug predictions, API Gateway for cancer centers, and manual FDA compliance tracking",
        "Amazon Omics for multi-omics analysis workflows optimized for cancer research, AWS HealthLake for HIPAA-compliant patient data with oncology FHIR profiles, Amazon SageMaker with healthcare compliance for AI drug combination predictions, AWS IoT Core for real-time biomarker device integration, AWS Clean Rooms for cancer center collaboration without patient data exposure, and AWS Audit Manager for FDA clinical decision support compliance",
        "S3 data lake for omics data, EMR for processing, generic SageMaker for AI, Direct Connect to cancer centers, and Config for regulatory monitoring",
        "Redshift for multi-omics storage, Aurora for clinical data, Batch for drug screening, VPN to cancer centers, and third-party compliance platforms"
    ],
    correct: 1,
    explanation: {
        correct: "Amazon Omics provides purpose-built multi-omics analysis optimized for cancer genomics research, HealthLake ensures HIPAA compliance with oncology-specific FHIR profiles, SageMaker with healthcare features enables compliant AI drug predictions, IoT Core handles real-time biomarker monitoring devices, Clean Rooms enables cancer center collaboration without exposing patient data, and Audit Manager automates FDA clinical decision support compliance.",
        whyWrong: {
            0: "Custom GPU bioinformatics lacks optimized multi-omics capabilities and manual FDA compliance doesn't scale",
            2: "Generic EMR doesn't provide specialized cancer omics analysis and lacks healthcare compliance features",
            3: "Redshift isn't optimized for genomics data and third-party compliance adds complexity for FDA requirements"
        },
        examStrategy: "For precision oncology: Amazon Omics for cancer-optimized multi-omics analysis, HealthLake for oncology FHIR compliance. Clean Rooms enables healthcare collaboration while protecting patient data."
    }
},
{
    id: 'sap_199',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global gaming platform supports 200 million players with monthly costs of $6M: EC2 game servers ($2.5M), DynamoDB player data ($1.5M), ElastiCache leaderboards ($1M), CloudFront content delivery ($500K), S3 game assets ($500K). Issues include: 45-second matchmaking during peak hours, game server scaling taking 15 minutes causing disconnections, leaderboard lag across regions, content download delays for new releases, and massive over-provisioning for weekend traffic spikes.",
    question: "Which optimization strategy provides the GREATEST improvement in player experience and operational efficiency?",
    options: [
        "Pre-scale game servers for weekends, increase DynamoDB capacity, expand cache clusters, optimize CloudFront configurations, and implement S3 Transfer Acceleration",
        "Containerize game servers on EKS with autoscaling, add DynamoDB global tables, implement ElastiCache clustering, increase CloudFront edge locations, and use Reserved Instances",
        "Migrate to Amazon GameLift with FlexMatch for <5-second matchmaking, implement GameLift Spot Fleet for 70% server cost reduction, use DynamoDB Global Tables with DAX for real-time leaderboards, add CloudFront Origin Shield for new release optimization, and implement GameLift automatic scaling eliminating disconnections",
        "Optimize current game servers, implement DynamoDB auto-scaling, add Redis read replicas, use CloudFront compression, and purchase Compute Savings Plans"
    ],
    correct: 2,
    explanation: {
        correct: "GameLift with FlexMatch reduces matchmaking from 45 seconds to <5 seconds using intelligent algorithms, Spot Fleet integration provides 70% server cost reduction while maintaining availability, DynamoDB Global Tables with DAX ensure real-time leaderboard consistency across regions, Origin Shield optimizes new release content delivery reducing costs 85%, and GameLift automatic scaling eliminates 15-minute delays preventing player disconnections.",
        whyWrong: {
            0: "Pre-scaling maintains expensive over-provisioning and doesn't solve fundamental matchmaking or scaling issues",
            1: "EKS adds complexity and doesn't provide gaming-specific optimizations like intelligent matchmaking and game server management",
            3: "Optimizing current architecture doesn't solve 45-second matchmaking or 15-minute scaling delays affecting player experience"
        },
        examStrategy: "For gaming optimization: GameLift provides purpose-built game management with FlexMatch for rapid matchmaking. Spot Fleet reduces costs while maintaining availability. Global Tables with DAX solve leaderboard consistency."
    }
},
{
    id: 'sap_200',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart grid company is building an energy management platform for 100 utility companies managing 50 million smart meters. Requirements include: real-time energy consumption monitoring, automated demand response during peak periods, integration with renewable energy sources, predictive maintenance for grid equipment, cybersecurity for critical infrastructure, and compliance with NERC reliability standards.",
    question: "Which architecture provides comprehensive smart grid management capabilities at utility scale?",
    options: [
        "IoT Core for smart meters, Lambda for demand response, CloudWatch for monitoring, API Gateway for renewables, GuardDuty for security, and Config for compliance",
        "Custom meter management, EC2 for processing, DynamoDB for consumption data, manual demand response, third-party renewable integration, and basic security monitoring",
        "Kinesis for meter data, Batch for analytics, RDS for grid information, automated response systems, Direct Connect to renewables, and manual compliance tracking",
        "AWS IoT Core with device fleet management for 50 million smart meters, AWS IoT Analytics for real-time consumption analysis, Amazon Forecast for demand prediction, AWS IoT SiteWise for renewable energy integration, Amazon Lookout for Equipment for grid predictive maintenance, AWS Security Hub for critical infrastructure protection, and AWS Audit Manager for NERC compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core with fleet management scales to 50 million smart meters with utility-grade reliability, IoT Analytics processes real-time consumption data for demand response, Forecast predicts energy demand patterns for grid optimization, IoT SiteWise integrates renewable energy sources with grid protocols, Lookout for Equipment provides predictive maintenance for critical grid equipment, Security Hub protects critical infrastructure with specialized threat detection, and Audit Manager automates NERC compliance evidence collection.",
        whyWrong: {
            0: "Generic IoT Core lacks utility fleet management features and GuardDuty alone doesn't provide critical infrastructure security",
            1: "Custom meter management doesn't scale to 50 million devices and manual processes can't handle utility-scale operations",
            2: "Kinesis alone lacks device management features and manual compliance doesn't meet NERC requirements"
        },
        examStrategy: "For smart grid/utilities: IoT Core with fleet management scales to millions of devices, IoT SiteWise integrates renewable energy, Lookout for Equipment predicts infrastructure failures. Security Hub provides critical infrastructure protection."
    }
},	

// Questions 201-250 with planned distribution: Domain 1(13), Domain 2(15), Domain 3(12), Domain 4(10)
// Answer distribution: A(12), B(13), C(13), D(12)
{
    id: 'sap_201',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A biotech consortium is developing a personalized vaccine platform for emerging pandemics. Requirements include: analysis of viral genome sequences from global surveillance, AI-driven vaccine design predictions, clinical trial coordination across 200 research sites, real-time efficacy monitoring from 10 million vaccine recipients, integration with WHO surveillance systems, and compliance with FDA emergency use authorization protocols.",
    question: "Which architecture enables rapid pandemic vaccine development while maintaining regulatory compliance?",
    options: [
        "Amazon Omics for viral genome analysis with pandemic-specific workflows, Amazon SageMaker with healthcare compliance for AI vaccine design, AWS HealthLake for clinical trial coordination with FHIR support, AWS IoT Core for real-time efficacy monitoring devices, AWS B2B Data Interchange for WHO surveillance integration, and AWS Audit Manager for FDA emergency authorization compliance",
        "Custom bioinformatics on GPU clusters, RDS for clinical data, Lambda for vaccine predictions, API Gateway for research sites, and manual WHO reporting",
        "S3 data lake for genome storage, EMR for sequence analysis, generic ML models, Direct Connect to research sites, and basic compliance tracking",
        "Redshift for viral data, Aurora for trial information, Batch for vaccine modeling, VPN to WHO systems, and third-party regulatory platforms"
    ],
    correct: 0,
    explanation: {
        correct: "Amazon Omics provides purpose-built viral genome analysis optimized for pandemic surveillance, SageMaker with healthcare features enables compliant AI vaccine design, HealthLake coordinates clinical trials with FHIR interoperability, IoT Core handles real-time efficacy monitoring from 10M recipients, B2B Data Interchange manages WHO surveillance protocols, and Audit Manager automates FDA emergency authorization compliance.",
        whyWrong: {
            1: "Custom GPU bioinformatics lacks pandemic-specific optimizations and manual WHO reporting doesn't scale globally",
            2: "Generic EMR doesn't provide specialized viral genome analysis and lacks healthcare compliance features",
            3: "Redshift isn't optimized for genomics data and third-party platforms add complexity for emergency protocols"
        },
        examStrategy: "For pandemic response: Amazon Omics for viral genome analysis, HealthLake for clinical coordination, B2B Data Interchange for WHO integration. Emergency protocols require specialized compliance automation."
    }
},
{
    id: 'sap_202',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global banking consortium with 2,000 AWS accounts processes $50 trillion in transactions annually across 150 countries. Requirements include: complete financial isolation between competing banks, secure collaboration for anti-money laundering (AML), automated compliance with Basel III, SWIFT, and local banking regulations, real-time fraud detection across the consortium, and precise cost allocation to individual banks and transaction types.",
    question: "Which governance architecture provides banking consortium operations while maintaining competitive isolation?",
    options: [
        "Basic Organizations structure, manual AML collaboration, individual compliance processes, separate fraud systems, and spreadsheet cost allocation",
        "AWS Organizations with bank-specific OUs and automated regulatory guardrails, AWS Clean Rooms for AML collaboration without exposing transaction data, Amazon Fraud Detector for consortium-wide fraud detection, AWS Cost Categories for bank and transaction-type allocation, AWS Audit Manager for Basel III and SWIFT compliance, and AWS FinSpace for secure financial data analysis",
        "Separate Organizations per bank, isolated AML systems, manual compliance tracking, individual fraud detection, and project-based billing",
        "Hub-and-spoke architecture, centralized transaction databases, API-based AML sharing, unified fraud platform, and automated billing"
    ],
    correct: 1,
    explanation: {
        correct: "Bank-specific OUs with regulatory guardrails ensure automated compliance across 150 countries, Clean Rooms enables AML collaboration while protecting competitive transaction data, Fraud Detector provides consortium-wide protection without data sharing, Cost Categories handle complex bank and transaction-type allocation, Audit Manager automates Basel III and SWIFT compliance, and FinSpace provides secure financial analytics for the consortium.",
        whyWrong: {
            0: "Basic structure doesn't meet banking regulatory requirements and manual processes don't scale to $50T annually",
            2: "Separate Organizations eliminate AML collaboration benefits and create massive compliance overhead",
            3: "Centralized databases violate competitive banking isolation and expose confidential transaction data"
        },
        examStrategy: "For banking consortiums: Clean Rooms enables AML collaboration while protecting competitive data, FinSpace provides banking-specific analytics. Regulatory guardrails ensure automated compliance across jurisdictions."
    }
},
{
    id: 'sap_203',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global streaming platform serves 800 million subscribers with monthly costs of $20M: CloudFront ($8M), EC2 transcoding ($5M), S3 storage ($4M), DynamoDB user data ($2M), RDS content metadata ($1M). Issues include: transcoding queues backing up 96+ hours during major releases, 99% of content never watched after 3 months, CloudFront costs spiking 500% during premieres, database connection exhaustion during simultaneous streaming peaks, and content discovery taking 15+ seconds.",
    question: "Which comprehensive optimization provides the GREATEST performance improvement and cost reduction?",
    options: [
        "Add more transcoding capacity, implement S3 lifecycle policies, optimize CloudFront configurations, scale database instances, and add search infrastructure",
        "Containerize transcoding on EKS, manually archive old content, add CloudFront behaviors, implement Aurora Global Database, and build custom recommendation engine",
        "Migrate to AWS Elemental MediaConvert for serverless parallel transcoding eliminating queues, implement S3 Intelligent-Tiering moving 99% content to Archive tiers saving $3.96M annually, add CloudFront Origin Shield reducing premiere costs 85%, upgrade to Aurora Serverless v2 for automatic scaling, and implement OpenSearch Serverless with ML-powered content discovery reducing search time to <1 second",
        "Optimize transcoding parameters, compress video files, increase CloudFront TTLs, add connection pooling, and implement basic search indexing"
    ],
    correct: 2,
    explanation: {
        correct: "MediaConvert provides serverless parallel transcoding eliminating 96-hour queues through unlimited scalability, S3 Intelligent-Tiering automatically moves 99% to Archive tiers saving $3.96M annually, Origin Shield reduces premiere costs 85% by eliminating duplicate origin requests, Aurora Serverless v2 prevents connection exhaustion through auto-scaling, and OpenSearch Serverless with ML reduces content discovery to <1 second while eliminating infrastructure management.",
        whyWrong: {
            0: "Adding more capacity increases costs without solving fundamental architecture bottlenecks",
            1: "Manual archiving is error-prone and EKS adds complexity without solving transcoding queue issues",
            3: "Optimizing current architecture doesn't solve 96-hour queues or achieve massive storage cost savings"
        },
        examStrategy: "For streaming optimization: MediaConvert eliminates transcoding bottlenecks, S3 Intelligent-Tiering provides massive storage savings. Origin Shield prevents premiere cost spikes. OpenSearch Serverless with ML optimizes content discovery."
    }
},
{
    id: 'sap_204',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A federal government agency must migrate classified intelligence systems to AWS GovCloud within 20 months for cybersecurity compliance. Current systems include: mainframe analysis with 40 years of intelligence algorithms, classified databases with sources and methods, real-time threat assessment, integration with allied intelligence services, and compliance with Intelligence Community Directive (ICD) 503 and FedRAMP High. Zero intelligence compromise is acceptable.",
    question: "Which migration approach maintains national security while achieving cybersecurity modernization?",
    options: [
        "Lift-and-shift mainframe to EC2, classified databases to RDS, custom threat systems, VPN to allies, and manual compliance tracking",
        "Replace intelligence systems with modern platform, PostgreSQL for classified data, cloud threat detection, API Gateway for allies, and automated compliance",
        "Keep classified systems on-premises, migrate support functions, implement hybrid connectivity, maintain allied connections, and gradual compliance upgrade",
        "AWS Mainframe Modernization with dual-run approach for intelligence algorithm preservation, AWS DataSync with encryption for classified database migration, AWS Security Lake for threat assessment aggregation, AWS B2B Data Interchange for allied intelligence integration, AWS GovCloud with ICD 503 compliance controls, and AWS CloudHSM for cryptographic intelligence requirements"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization with dual-run preserves 40 years of proven intelligence algorithms while maintaining security, DataSync with encryption ensures classified data protection during migration, Security Lake aggregates threat data without compromising sources, B2B Data Interchange handles allied intelligence protocols securely, GovCloud provides ICD 503 compliance infrastructure, and CloudHSM meets intelligence community cryptographic requirements.",
        whyWrong: {
            0: "Simple lift-and-shift doesn't meet intelligence community security requirements and risks algorithm compromise",
            1: "Complete replacement risks national security and loses decades of intelligence analysis capabilities",
            2: "Hybrid approach doesn't achieve cybersecurity modernization goals and maintains legacy vulnerabilities"
        },
        examStrategy: "For classified intelligence migration: dual-run maintains security during transition, Security Lake aggregates without compromising sources. CloudHSM provides intelligence-grade cryptography. Never compromise security for modernization speed."
    }
},
{
    id: 'sap_205',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A quantum computing research network is building a distributed quantum-classical platform connecting 50 quantum computers across universities and companies. Requirements include: quantum circuit optimization across different hardware types, classical preprocessing requiring 100,000 GPU hours monthly, secure quantum algorithm sharing with IP protection, real-time collaboration between 500 researchers, quantum error correction analysis, and 20-year research data retention for patent applications.",
    question: "Which architecture enables advanced quantum research collaboration while protecting intellectual property?",
    options: [
        "Custom quantum interfaces, GPU clusters for classical compute, file sharing for algorithms, video conferencing for collaboration, manual error analysis, and basic data storage",
        "Amazon Braket for multi-vendor quantum access with native AWS integration, AWS ParallelCluster with P4d instances for massive classical preprocessing, AWS Clean Rooms for IP-protected quantum algorithm collaboration, AWS CodeCommit with fine-grained IAM for secure quantum code versioning, Amazon SageMaker for quantum error correction analysis, and S3 Intelligent-Tiering with Glacier Deep Archive for 20-year patent data retention",
        "Third-party quantum platforms, EMR for GPU processing, private repositories for code, Slack for communication, spreadsheet error tracking, and tape storage for archives",
        "Direct quantum hardware access, EC2 for classical compute, SVN for version control, email for collaboration, manual error correction, and EBS for data storage"
    ],
    correct: 1,
    explanation: {
        correct: "Braket provides unified access to 50 different quantum computers with circuit optimization, ParallelCluster with P4d delivers 100,000 GPU hours for classical preprocessing, Clean Rooms enables quantum algorithm collaboration while protecting IP, CodeCommit with IAM provides secure versioning for quantum code, SageMaker analyzes quantum error correction patterns, and S3 Intelligent-Tiering with Deep Archive optimizes 20-year patent storage costs.",
        whyWrong: {
            0: "Custom quantum interfaces don't scale to 50 different hardware types and lack optimization capabilities",
            2: "Third-party platforms don't integrate well for hybrid quantum-classical workflows and lack IP protection",
            3: "Direct hardware access is complex to manage and doesn't provide unified quantum programming interface"
        },
        examStrategy: "For quantum computing: Braket provides multi-vendor quantum access with optimization, Clean Rooms protects quantum IP during collaboration. ParallelCluster scales classical preprocessing for hybrid workflows."
    }
},
{
    id: 'sap_206',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global logistics consortium operates 1,500 AWS accounts across shipping companies, airlines, and trucking firms. Requirements include: shipment tracking visibility across all modes, secure data sharing for supply chain optimization, automated customs clearance, real-time capacity coordination, cost allocation to individual shipments, and compliance with international trade regulations.",
    question: "Which governance model enables logistics consortium coordination while maintaining competitive protection?",
    options: [
        "AWS Organizations with transport-mode OUs for operational coordination, AWS Clean Rooms for competitive data sharing without exposing pricing, AWS Supply Chain for end-to-end shipment visibility, AWS B2B Data Interchange for automated customs clearance, AWS Cost Categories for shipment-level allocation, and AWS Audit Manager for international trade compliance",
        "Separate Organizations per company type, manual tracking coordination, email-based data sharing, individual customs processes, spreadsheet cost tracking, and manual trade compliance",
        "Single Organization with basic IAM, S3 for shipment data, manual supply chain coordination, API Gateway for customs, tagging for costs, and quarterly compliance reviews",
        "Hub-and-spoke architecture, centralized shipment databases, open data sharing, unified customs platform, automated billing, and single trade compliance system"
    ],
    correct: 0,
    explanation: {
        correct: "Transport-mode OUs provide operational coordination while maintaining competitive boundaries, Clean Rooms enables supply chain optimization without exposing competitive pricing data, Supply Chain provides end-to-end visibility across modes, B2B Data Interchange automates customs with international standards, Cost Categories enable precise shipment allocation, and Audit Manager automates trade compliance across jurisdictions.",
        whyWrong: {
            1: "Separate Organizations eliminate consortium benefits and create customs clearance inefficiencies",
            2: "Basic controls don't provide supply chain visibility and lack international trade compliance features",
            3: "Centralized databases expose competitive pricing and shipping capacity information"
        },
        examStrategy: "For logistics consortiums: Clean Rooms protects competitive data while enabling collaboration, Supply Chain provides end-to-end visibility. B2B Data Interchange handles international customs standards."
    }
},
{
    id: 'sap_207',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A cryptocurrency trading platform handles 100 million transactions daily with costs of $5M/month: DynamoDB ($2M), Lambda ($1.5M), API Gateway ($800K), ElastiCache ($500K), CloudFront ($200K). Issues include: DynamoDB throttling during market crashes, Lambda cold starts during volatility spikes, API Gateway timeouts during flash trading, cache evictions during price movements, and exponential cost scaling with trading volume.",
    question: "Which optimization strategy provides the BEST performance and cost control for extreme cryptocurrency trading volumes?",
    options: [
        "Increase DynamoDB provisioned capacity, use Lambda reserved concurrency, raise API Gateway limits, expand cache clusters, and implement compression",
        "Add Aurora for transaction consistency, containerize Lambda functions, implement API caching, use Redis clustering, and purchase Reserved Instances",
        "Implement DynamoDB on-demand with DAX for microsecond trading latency, migrate to Lambda with provisioned concurrency eliminating cold starts, add API Gateway regional endpoints with edge optimization, use ElastiCache Global Datastore for price consistency, and implement CloudFront with Lambda@Edge for trading interface optimization",
        "Switch to RDS for ACID compliance, implement Step Functions for orchestration, use Application Load Balancer, deploy multiple cache layers, and add CDN compression"
    ],
    correct: 2,
    explanation: {
        correct: "DynamoDB on-demand with DAX provides automatic scaling with microsecond latency for market crashes, Lambda provisioned concurrency eliminates cold starts during volatility, API Gateway regional endpoints with edge optimization handle flash trading, ElastiCache Global Datastore ensures price consistency across regions, and Lambda@Edge optimizes trading interfaces reducing latency and costs.",
        whyWrong: {
            0: "Pre-provisioning leads to massive over-provisioning costs and doesn't handle unpredictable crypto volatility",
            1: "Aurora adds complexity for trading systems optimized for DynamoDB's performance characteristics",
            3: "RDS doesn't scale to 100M daily transactions and Step Functions add latency inappropriate for trading"
        },
        examStrategy: "For crypto trading: DynamoDB on-demand handles volatility cost-effectively, DAX provides microsecond latency. Provisioned concurrency prevents Lambda cold starts. Global Datastore ensures price consistency."
    }
},
{
    id: 'sap_208',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state environmental agency must migrate pollution monitoring systems to AWS within 18 months for real-time compliance. Current systems include: mainframe for emissions data with 35 years of environmental records, Oracle databases for permit tracking, integration with 1,000 industrial facilities, air quality sensor networks, and compliance with EPA regulations.",
    question: "Which migration approach maintains environmental protection while enabling real-time monitoring?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom facility APIs, IoT Core for sensors, and manual EPA compliance",
        "Replace environmental system with modern platform, PostgreSQL for permits, API Gateway for facilities, cloud sensors, and automated compliance",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain facility connections, upgrade sensor networks, and manual EPA tracking",
        "AWS Mainframe Modernization for emissions data preservation, AWS HealthLake for environmental health data with FHIR support, AWS B2B Data Interchange for facility integration, AWS IoT Analytics for air quality monitoring, and AWS Audit Manager for EPA compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 35 years of emissions data and algorithms while enabling real-time capabilities, HealthLake manages environmental health data with FHIR interoperability, B2B Data Interchange handles diverse facility reporting formats, IoT Analytics processes air quality sensor data for real-time monitoring, and Audit Manager automates EPA compliance evidence collection.",
        whyWrong: {
            0: "Aurora migration could break environmental tracking procedures and manual compliance doesn't meet EPA requirements",
            1: "Complete replacement risks environmental protection and loses decades of emissions baseline data",
            2: "Hybrid approach doesn't enable real-time monitoring capabilities required for environmental compliance"
        },
        examStrategy: "For environmental migration: preserve historical data for compliance baselines, IoT Analytics for real-time monitoring. B2B Data Interchange handles industrial facility standards. Audit Manager automates EPA compliance."
    }
},
{
    id: 'sap_209',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A precision agriculture company is building an AI-powered crop optimization platform for 1 million farms globally. Requirements include: satellite imagery analysis for field health monitoring, soil sensor data from 500 million IoT devices, drone-based pesticide application optimization, weather-driven irrigation scheduling, supply chain coordination with 10,000 food distributors, and compliance with organic farming standards in 100 countries.",
    question: "Which architecture provides comprehensive precision agriculture at unprecedented scale?",
    options: [
        "EC2 for image processing, Kinesis for sensor data, Lambda for drone control, custom irrigation logic, API Gateway for supply chain, and manual organic certification",
        "Amazon SageMaker Geospatial for satellite crop analysis with agricultural ML models, AWS IoT Core with Basic Ingest for cost-effective processing of 500M sensors, Amazon Location Service for precision drone application mapping, Amazon Forecast with Weather Index for irrigation optimization, AWS Supply Chain for food distributor coordination, and AWS Audit Manager for organic certification compliance across 100 countries",
        "Custom ML models on GPU farms, DynamoDB for sensor storage, computer vision on EC2, manual drone operations, Direct Connect to distributors, and third-party organic tracking",
        "Batch processing for satellite data, Timestream for sensors, generic SageMaker models, rule-based irrigation, MSK for supply chain events, and Config for organic compliance"
    ],
    correct: 1,
    explanation: {
        correct: "SageMaker Geospatial provides purpose-built satellite analysis with agricultural ML, IoT Core Basic Ingest reduces costs for 500M sensors while maintaining reliability, Location Service enables precision drone mapping for targeted pesticide application, Forecast with Weather Index optimizes irrigation with integrated weather data, Supply Chain coordinates with 10,000 distributors, and Audit Manager automates organic certification across 100 countries.",
        whyWrong: {
            0: "Generic EC2 lacks agricultural-specific capabilities and manual certification doesn't scale to 100 countries",
            2: "Custom GPU farms are expensive and DynamoDB costs would be prohibitive for 500M device data",
            3: "Batch processing can't provide real-time irrigation optimization and generic models lack agricultural specificity"
        },
        examStrategy: "For precision agriculture at scale: SageMaker Geospatial for agricultural satellite analysis, IoT Core Basic Ingest for massive sensor cost optimization. Location Service for precision mapping applications."
    }
},
{
    id: 'sap_210',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global energy consortium operates 2,000 AWS accounts across oil, gas, renewable, and nuclear companies in 80 countries. Requirements include: environmental impact monitoring for all operations, secure sharing of energy production data, automated compliance with international energy regulations, carbon footprint tracking across the consortium, real-time grid stability coordination, and cost allocation to individual energy projects.",
    question: "Which governance architecture addresses energy industry complexity while enabling global coordination?",
    options: [
        "Basic Organizations structure, manual environmental monitoring, individual compliance processes, separate grid systems, and spreadsheet cost tracking",
        "Separate Organizations per energy type, isolated monitoring systems, manual compliance tracking, individual grid operations, and project-based billing",
        "Single Organization with IAM policies, CloudWatch for basic monitoring, S3 for data sharing, manual compliance tracking, and tagging for costs",
        "AWS Organizations with energy-type OUs and environmental guardrails, AWS Clean Rooms for competitive data sharing without exposing pricing, AWS IoT SiteWise for energy production monitoring, AWS Carbon Footprint Tool for consortium-wide tracking, AWS B2B Data Interchange for international energy regulation compliance, and AWS Cost Categories for project-level allocation"
    ],
    correct: 3,
    explanation: {
        correct: "Energy-type OUs with environmental guardrails ensure automated compliance across 80 countries, Clean Rooms enables energy data sharing while protecting competitive pricing, IoT SiteWise monitors production across oil/gas/renewable/nuclear, Carbon Footprint Tool tracks consortium-wide emissions, B2B Data Interchange automates international energy compliance, and Cost Categories enable precise project allocation across energy types.",
        whyWrong: {
            0: "Basic structure doesn't meet international energy regulatory requirements and lacks environmental monitoring",
            1: "Separate Organizations eliminate coordination benefits and create massive compliance overhead",
            2: "Basic controls don't provide energy industry compliance and lack carbon footprint tracking capabilities"
        },
        examStrategy: "For energy consortiums: IoT SiteWise monitors energy production, Clean Rooms protects competitive data. Carbon Footprint Tool provides consortium-wide tracking. Environmental guardrails ensure automated compliance."
    }
},
{
    id: 'sap_211',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global social media platform supports 2 billion users with monthly costs of $50M: EC2 for content processing ($20M), OpenSearch for content discovery ($12M), DynamoDB for user data ($10M), CloudFront for delivery ($5M), S3 for media storage ($3M). Issues include: content processing backlogs during viral events, search response times >20 seconds, user data inconsistencies across regions, CDN costs spiking 1000% during major events, and massive infrastructure over-provisioning for unpredictable viral content.",
    question: "Which optimization strategy provides the GREATEST performance improvement and cost reduction for viral social media traffic?",
    options: [
        "Pre-scale infrastructure for viral events, add more OpenSearch nodes, implement DynamoDB global tables, optimize CloudFront configurations, and purchase Reserved Instances",
        "Migrate to Lambda with SQS for elastic content processing eliminating backlogs, implement OpenSearch Serverless with ML-powered content discovery reducing search to <2 seconds, use DynamoDB Global Tables for user consistency, add CloudFront Origin Shield reducing event costs 90%, implement S3 Intelligent-Tiering for automatic media optimization, and achieve 60% cost reduction through serverless-first architecture",
        "Containerize on EKS with KEDA autoscaling, add OpenSearch auto-scaling, implement DynamoDB on-demand, increase CloudFront distributions, and use Spot instances for processing",
        "Optimize current processing algorithms, tune OpenSearch configurations, add DynamoDB caching, implement CDN compression, and negotiate volume discounts"
    ],
    correct: 1,
    explanation: {
        correct: "Lambda with SQS provides infinite scaling for viral content processing without backlogs, OpenSearch Serverless with ML reduces search time to <2 seconds while eliminating cluster management, DynamoDB Global Tables ensure user consistency across regions, Origin Shield reduces viral event costs 90% by eliminating duplicate requests, S3 Intelligent-Tiering optimizes media storage automatically, and serverless-first architecture eliminates over-provisioning achieving 60% cost reduction.",
        whyWrong: {
            0: "Pre-scaling maintains expensive over-provisioning and doesn't solve fundamental architecture bottlenecks",
            2: "EKS adds complexity and doesn't provide the viral scalability benefits of true serverless architecture",
            3: "Optimizing current architecture doesn't solve viral traffic backlogs or eliminate massive over-provisioning costs"
        },
        examStrategy: "For viral social media: Lambda with SQS provides infinite viral scaling, OpenSearch Serverless with ML improves search. Origin Shield prevents viral cost spikes. Serverless-first eliminates over-provisioning."
    }
},
{
    id: 'sap_212',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A municipal water utility must migrate infrastructure management systems to AWS within 20 months for cyber resilience. Current systems include: SCADA for water treatment with 40 years of process control algorithms, Oracle databases for customer billing, integration with 500 water quality sensors, emergency response coordination, and compliance with Safe Drinking Water Act regulations.",
    question: "Which migration approach maintains water safety while achieving cyber resilience?",
    options: [
        "AWS IoT SiteWise Edge for local SCADA control ensuring water safety during connectivity issues, Amazon RDS Custom for Oracle preserving billing procedures, AWS IoT Analytics for water quality monitoring, Amazon Connect for emergency coordination, and AWS Audit Manager for Safe Drinking Water Act compliance automation",
        "Migrate SCADA to IoT Core, Oracle to Aurora migration, cloud sensor platform, manual emergency response, and basic compliance tracking",
        "Replace water systems with modern platform, PostgreSQL for billing, API Gateway for sensors, automated emergency systems, and cloud compliance tools",
        "Keep SCADA on-premises, gradual database migration, upgrade sensor networks, enhance emergency systems, and manual compliance tracking"
    ],
    correct: 0,
    explanation: {
        correct: "IoT SiteWise Edge maintains local SCADA control for water safety during cyber attacks, RDS Custom preserves Oracle billing procedures critical for utility operations, IoT Analytics processes water quality data for compliance monitoring, Amazon Connect provides scalable emergency coordination, and Audit Manager automates Safe Drinking Water Act compliance evidence collection.",
        whyWrong: {
            1: "Direct IoT Core SCADA migration creates single point of failure risking water safety",
            2: "Complete replacement of water treatment systems is too risky for public safety in 20 months",
            3: "Keeping SCADA on-premises doesn't achieve cyber resilience goals"
        },
        examStrategy: "For water utility migration: maintain local control for safety-critical water treatment, IoT SiteWise Edge enables cyber-resilient operation. Never migrate safety systems without local backup."
    }
},
{
    id: 'sap_213',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous vehicle consortium is developing a fleet coordination platform for 500,000 self-driving vehicles across smart cities. Requirements include: real-time traffic optimization with vehicle-to-infrastructure communication, predictive maintenance using sensor data from all vehicles, autonomous parking coordination, emergency response integration, energy optimization for electric fleets, and compliance with automotive safety standards.",
    question: "Which architecture enables intelligent autonomous vehicle coordination at smart city scale?",
    options: [
        "EC2 for traffic processing, IoT Core for vehicle communication, Lambda for maintenance, manual parking coordination, CloudWatch for emergencies, and basic energy tracking",
        "Custom vehicle management, DynamoDB for traffic data, SageMaker for predictions, API Gateway for parking, SNS for emergency alerts, and spreadsheet energy tracking",
        "Kubernetes for vehicle processing, Kinesis for communication data, Batch for maintenance analysis, automated parking algorithms, Step Functions for emergency workflows, and custom energy optimization",
        "AWS IoT FleetWise for vehicle systems integration and OTA updates, Amazon Location Service for traffic optimization with real-time routing, Amazon Lookout for Equipment for predictive maintenance across 500K vehicles, AWS IoT SiteWise for autonomous parking infrastructure, Amazon Connect for emergency response coordination, and AWS IoT Analytics for electric fleet energy optimization"
    ],
    correct: 3,
    explanation: {
        correct: "IoT FleetWise manages 500K vehicles with OTA updates and vehicle-to-infrastructure protocols, Location Service optimizes traffic with real-time routing and smart city integration, Lookout for Equipment provides predictive maintenance across the entire fleet, IoT SiteWise coordinates parking infrastructure with autonomous vehicles, Amazon Connect handles emergency response coordination, and IoT Analytics optimizes electric fleet energy consumption patterns.",
        whyWrong: {
            0: "Generic IoT Core lacks autonomous vehicle fleet management and manual parking doesn't scale to smart cities",
            1: "Custom vehicle management doesn't provide automotive industry protocols and spreadsheet energy tracking is inadequate",
            2: "Kubernetes adds complexity and doesn't provide vehicle-specific optimization capabilities"
        },
        examStrategy: "For autonomous vehicles: IoT FleetWise for vehicle fleet management with OTA, Location Service for smart city traffic optimization. Lookout for Equipment for fleet-wide predictive maintenance."
    }
},
{
    id: 'sap_214',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A healthcare research network operates 500 AWS accounts across hospitals, universities, and biotech companies. Requirements include: patient data privacy protection between institutions, secure collaboration for clinical research, automated compliance with HIPAA, GDPR, and local health regulations, unified biobank coordination, and cost allocation to individual research grants.",
    question: "Which governance model enables healthcare research collaboration while protecting patient privacy?",
    options: [
        "AWS Organizations with institution-type OUs for research coordination, AWS Clean Rooms for patient privacy-protected research collaboration, AWS HealthLake for clinical data management with FHIR compliance, AWS DataZone for governed biobank data sharing, AWS Cost Categories for grant-level allocation, and AWS Audit Manager for multi-regulatory healthcare compliance",
        "Separate Organizations per institution, manual research coordination, individual patient databases, isolated biobank systems, and spreadsheet grant tracking",
        "Single Organization with basic IAM, S3 for research data, manual privacy protection, individual biobank management, and tagging for costs",
        "Hub-and-spoke architecture, centralized patient databases, API-based research sharing, unified biobank platform, and automated billing"
    ],
    correct: 0,
    explanation: {
        correct: "Institution-type OUs provide research coordination while maintaining privacy boundaries, Clean Rooms enables clinical research collaboration without exposing patient data, HealthLake manages clinical data with FHIR compliance across regulations, DataZone governs biobank sharing with lineage tracking, Cost Categories enable precise grant allocation, and Audit Manager automates HIPAA/GDPR compliance across jurisdictions.",
        whyWrong: {
            1: "Separate Organizations eliminate research collaboration benefits and create compliance overhead",
            2: "Basic controls don't provide patient privacy protection required for healthcare research",
            3: "Centralized patient databases violate privacy regulations and expose sensitive health information"
        },
        examStrategy: "For healthcare research: Clean Rooms protects patient privacy during collaboration, HealthLake provides healthcare compliance. DataZone enables governed biobank sharing with privacy protection."
    }
},
{
    id: 'sap_215',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A fintech lending platform processes 50 million loan applications annually using: 1,000 EC2 instances for underwriting ($1M/month), RDS PostgreSQL for applications ($400K/month), SageMaker for credit scoring ($300K/month), S3 for documents ($200K/month), ElastiCache for real-time decisions ($100K/month). Issues include: 48-hour application processing time, database timeouts during month-end, ML model retraining taking 12 hours, document retrieval delays, and 75% infrastructure idle overnight.",
    question: "Which optimization strategy provides the MOST significant improvement in processing speed and cost efficiency?",
    options: [
        "Add more EC2 instances, increase RDS capacity, optimize SageMaker training, implement S3 Transfer Acceleration, and purchase Reserved Instances",
        "Migrate to Step Functions with Lambda for parallel loan processing reducing time to 4 hours, implement Aurora Serverless v2 for automatic month-end scaling, use SageMaker Serverless Inference for cost-effective real-time scoring, implement S3 Intelligent-Tiering with CloudFront for document optimization, and eliminate 75% idle costs through serverless architecture",
        "Containerize on EKS with autoscaling, add Aurora read replicas, implement SageMaker multi-model endpoints, enable S3 lifecycle policies, and use Spot instances",
        "Optimize current configurations, implement connection pooling, schedule model training off-hours, compress documents, and negotiate volume pricing"
    ],
    correct: 1,
    explanation: {
        correct: "Step Functions with Lambda enables parallel processing reducing 48-hour time to 4 hours, Aurora Serverless v2 eliminates month-end timeouts through automatic scaling, SageMaker Serverless Inference provides cost-effective real-time scoring without idle costs, S3 Intelligent-Tiering with CloudFront improves document access speed, and serverless architecture eliminates 75% idle infrastructure costs.",
        whyWrong: {
            0: "Adding more resources increases costs without solving fundamental processing bottlenecks and idle capacity",
            2: "EKS adds complexity and doesn't provide the processing speed improvements of parallel serverless workflows",
            3: "Optimizing current architecture doesn't solve 48-hour processing time or eliminate massive idle capacity"
        },
        examStrategy: "For lending optimization: Step Functions enables parallel workflows dramatically reducing processing time. Aurora Serverless v2 handles variable database loads. Serverless eliminates idle costs."
    }
},
{
    id: 'sap_216',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A regional public transit authority must migrate fare collection and route management systems to AWS within 14 months for contactless payment compliance. Current systems include: mainframe for fare calculations with 25 years of transit data, Oracle databases for route scheduling, integration with 500 buses and rail cars, mobile ticketing applications, and compliance with federal transportation accessibility requirements.",
    question: "Which migration approach maintains transit operations while enabling contactless payment modernization?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, IoT Core for vehicle tracking, custom mobile apps, and basic accessibility compliance",
        "Replace transit system with modern platform, PostgreSQL for scheduling, API Gateway for vehicles, cloud mobile platform, and automated accessibility tools",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain vehicle connections, upgrade mobile systems, and manual accessibility tracking",
        "AWS Mainframe Modernization for fare calculation preservation, Amazon Location Service for route optimization, AWS IoT Device Management for transit vehicle fleet, Amazon API Gateway for contactless payment integration, and AWS Audit Manager for transportation accessibility compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 25 years of fare calculation logic while enabling contactless payment capabilities, Location Service optimizes route planning with real-time traffic data, IoT Device Management handles 500 buses and rail cars, API Gateway provides secure contactless payment integration, and Audit Manager automates federal accessibility compliance.",
        whyWrong: {
            0: "Aurora migration could break fare calculation procedures and basic compliance doesn't meet federal requirements",
            1: "Complete replacement risks transit operations and loses decades of route optimization data",
            2: "Hybrid approach doesn't enable contactless payment modernization required for compliance"
        },
        examStrategy: "For transit migration: preserve fare calculation logic, Location Service optimizes routes. IoT Device Management handles vehicle fleets. API Gateway enables contactless payment integration."
    }
},
{
    id: 'sap_217',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A carbon capture consortium is building a global monitoring platform for 10,000 carbon capture facilities across industrial sites. Requirements include: real-time CO2 capture rate monitoring, predictive maintenance for capture equipment, optimization of carbon storage locations, integration with carbon credit markets, verification of capture claims for regulatory compliance, and coordination with 50 industrial partners.",
    question: "Which architecture provides comprehensive carbon capture monitoring and verification at global scale?",
    options: [
        "IoT Core for facility monitoring, Lambda for maintenance predictions, manual storage optimization, API Gateway for markets, and basic compliance tracking",
        "AWS IoT SiteWise for industrial carbon capture monitoring, Amazon Lookout for Equipment for capture equipment predictive maintenance, Amazon Location Service for carbon storage optimization, AWS B2B Data Interchange for carbon credit market integration, Amazon QLDB for immutable capture verification records, and AWS Clean Rooms for industrial partner collaboration without exposing proprietary data",
        "Custom monitoring systems, SageMaker for equipment analysis, spreadsheet storage planning, Direct Connect to markets, and manual verification processes",
        "Kinesis for capture data, Batch for maintenance analysis, third-party storage solutions, VPN to markets, and Config for regulatory compliance"
    ],
    correct: 1,
    explanation: {
        correct: "IoT SiteWise provides industrial-grade monitoring for 10,000 capture facilities, Lookout for Equipment predicts failures in carbon capture equipment, Location Service optimizes storage locations considering geography and capacity, B2B Data Interchange integrates with carbon credit markets, QLDB provides immutable verification records for regulatory compliance, and Clean Rooms enables partner collaboration while protecting proprietary capture technologies.",
        whyWrong: {
            0: "Generic IoT Core lacks industrial monitoring features and manual processes don't scale to 10,000 facilities",
            2: "Custom systems require extensive development and spreadsheet planning is inadequate for global operations",
            3: "Third-party storage solutions lack AWS integration and VPN doesn't scale to global market integration"
        },
        examStrategy: "For carbon capture: IoT SiteWise for industrial monitoring, Lookout for Equipment for predictive maintenance. QLDB provides immutable verification for compliance. Clean Rooms protects proprietary technologies."
    }
},
{
    id: 'sap_218',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global defense alliance operates 3,000 AWS accounts across member nations for joint military operations. Requirements include: complete operational security between classified levels, secure intelligence sharing for approved operations, automated compliance with NATO security standards, real-time threat assessment coordination, weapon systems integration testing, and cost allocation to individual defense programs across nations.",
    question: "Which governance architecture provides military alliance coordination while maintaining operational security?",
    options: [
        "Basic Organizations structure, manual intelligence sharing, individual compliance processes, separate threat systems, and project-based billing",
        "Separate Organizations per nation, isolated intelligence systems, manual compliance tracking, individual threat assessment, and national billing",
        "Single Organization with classification-based IAM, S3 for intelligence data, manual compliance tracking, shared threat platform, and unified billing",
        "AWS Organizations with classification-level OUs and automated security guardrails, AWS Clean Rooms for intelligence sharing without exposing sources and methods, AWS Security Lake for unified threat assessment, AWS B2B Data Interchange for NATO standard compliance, AWS IoT SiteWise for weapon systems integration testing, and AWS Cost Categories for cross-nation program allocation"
    ],
    correct: 3,
    explanation: {
        correct: "Classification-level OUs with security guardrails ensure automated NATO compliance, Clean Rooms enables intelligence sharing while protecting sources and methods, Security Lake provides unified threat assessment without compromising classified data, B2B Data Interchange automates NATO standard compliance, IoT SiteWise handles weapon systems integration testing, and Cost Categories enable complex cross-nation program allocation.",
        whyWrong: {
            0: "Basic structure doesn't meet military classification requirements and manual processes don't scale to alliance operations",
            1: "Separate Organizations eliminate joint operation benefits and create massive coordination overhead",
            2: "Single Organization with basic IAM doesn't provide classification-level isolation required for military operations"
        },
        examStrategy: "For military alliances: classification-level isolation is critical, Clean Rooms protects intelligence sources while enabling sharing. Security Lake aggregates threats without compromising classified data."
    }
},
{
    id: 'sap_219',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global marketplace connects 5 million sellers with 500 million buyers using: 3,000 EC2 instances for order processing ($2M/month), OpenSearch cluster for product search ($800K/month), DynamoDB for user data ($600K/month), CloudFront for content delivery ($400K/month), S3 for product images ($200K/month). Issues include: order processing delays during flash sales, search response times >15 seconds, user session inconsistencies, CDN costs spiking during major shopping events, and 60% infrastructure idle during off-peak hours.",
    question: "Which optimization strategy provides the BEST performance improvement and cost reduction for variable marketplace traffic?",
    options: [
        "Pre-scale for flash sales, add more OpenSearch nodes, implement DynamoDB global tables, optimize CloudFront configurations, and purchase Reserved Instances",
        "Containerize on EKS with KEDA, add OpenSearch auto-scaling, implement DynamoDB on-demand, increase CloudFront distributions, and use Spot instances",
        "Migrate to ECS Fargate Spot for elastic order processing eliminating idle costs, implement OpenSearch Serverless with ML-powered search reducing response time to <2 seconds, use DynamoDB Global Tables for session consistency, add CloudFront Origin Shield reducing event costs 85%, and achieve 60% cost reduction through optimized serverless architecture",
        "Optimize current processing, tune search configurations, add DynamoDB caching, implement CDN compression, and negotiate enterprise pricing"
    ],
    correct: 2,
    explanation: {
        correct: "Fargate Spot eliminates 60% idle costs while providing instant scaling for flash sales, OpenSearch Serverless with ML reduces search time to <2 seconds while eliminating cluster management, DynamoDB Global Tables ensure session consistency across regions, Origin Shield reduces shopping event costs 85% by eliminating duplicate requests, and serverless architecture achieves 60% cost reduction by eliminating idle infrastructure.",
        whyWrong: {
            0: "Pre-scaling maintains expensive over-provisioning and doesn't solve fundamental search performance issues",
            1: "EKS adds operational complexity and doesn't provide the cost optimization benefits of serverless",
            3: "Optimizing current architecture doesn't solve flash sale scalability or eliminate 60% idle capacity"
        },
        examStrategy: "For marketplace optimization: Fargate Spot eliminates idle costs during off-peak, OpenSearch Serverless with ML improves search. Origin Shield prevents event cost spikes."
    }
},
{
    id: 'sap_220',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national postal service must migrate package tracking and logistics systems to AWS within 16 months for competitive modernization. Current systems include: mainframe for package routing with 50 years of postal optimization algorithms, Oracle databases for customer services, integration with international postal networks, handheld scanner fleet management, and compliance with Universal Postal Union regulations.",
    question: "Which migration approach maintains postal operations while enabling competitive capabilities?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom international integration, IoT Core for scanners, and manual UPU compliance",
        "Replace postal system with modern platform, PostgreSQL for customer data, API Gateway for international networks, cloud scanner management, and automated compliance",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain international connections, upgrade scanner systems, and manual compliance tracking",
        "AWS Mainframe Modernization for postal routing logic preservation, Amazon Location Service for delivery optimization, AWS IoT Device Management for scanner fleet, AWS B2B Data Interchange for international postal network integration, and AWS Audit Manager for Universal Postal Union compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 50 years of postal routing optimization while enabling competitive capabilities, Location Service provides modern delivery route optimization, IoT Device Management handles scanner fleet at scale, B2B Data Interchange manages international postal network protocols, and Audit Manager automates Universal Postal Union compliance.",
        whyWrong: {
            0: "Aurora migration could break postal procedures and manual compliance doesn't meet international standards",
            1: "Complete replacement risks postal operations and loses decades of routing optimization knowledge",
            2: "Hybrid approach doesn't achieve competitive modernization goals and maintains legacy limitations"
        },
        examStrategy: "For postal modernization: preserve routing optimization algorithms, Location Service enhances delivery efficiency. B2B Data Interchange handles international postal standards."
    }
},
{
    id: 'sap_221',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart manufacturing consortium is building an Industry 5.0 platform for 5,000 factories with human-robot collaboration. Requirements include: real-time safety monitoring for worker-robot interaction, predictive quality control using computer vision, supply chain optimization across 50,000 suppliers, energy consumption minimization, collaborative robot programming, and compliance with international manufacturing safety standards.",
    question: "Which architecture enables human-robot collaborative manufacturing at consortium scale?",
    options: [
        "EC2 for safety monitoring, Lambda for quality control, custom supply chain software, CloudWatch for energy tracking, manual robot programming, and basic safety compliance",
        "AWS Panorama for edge computer vision safety monitoring, Amazon Lookout for Equipment for predictive quality control, AWS Supply Chain for supplier optimization, AWS IoT Analytics for energy minimization, AWS RoboMaker for collaborative robot development, and AWS Audit Manager for international manufacturing safety compliance",
        "Custom safety systems, computer vision on GPU clusters, third-party supply chain platforms, manual energy tracking, traditional robot programming, and quarterly safety reviews",
        "IoT Core for safety sensors, SageMaker for quality predictions, DynamoDB for supplier data, Timestream for energy monitoring, custom robot software, and Config for compliance"
    ],
    correct: 1,
    explanation: {
        correct: "Panorama provides edge computer vision for real-time worker-robot safety monitoring, Lookout for Equipment predicts quality issues before they occur, Supply Chain optimizes 50,000 supplier relationships, IoT Analytics minimizes energy consumption across 5,000 factories, RoboMaker enables collaborative robot programming and simulation, and Audit Manager automates international manufacturing safety compliance.",
        whyWrong: {
            0: "Generic EC2 safety monitoring lacks real-time computer vision capabilities for human-robot interaction",
            2: "Custom safety systems don't provide the real-time capabilities needed for worker protection",
            3: "IoT Core alone doesn't provide computer vision safety monitoring and custom robot software lacks collaboration features"
        },
        examStrategy: "For human-robot collaboration: Panorama for real-time safety monitoring with computer vision, RoboMaker for collaborative robot development. Supply Chain optimizes complex supplier networks."
    }
},
{
    id: 'sap_222',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global food safety consortium operates 1,200 AWS accounts across food producers, processors, and retailers. Requirements include: contamination source tracing across the supply chain, secure sharing of safety data without exposing proprietary processes, automated compliance with FDA, USDA, and international food safety regulations, real-time outbreak response coordination, and cost allocation to individual product recalls.",
    question: "Which governance model enables food safety coordination while protecting competitive information?",
    options: [
        "AWS Organizations with supply-chain OUs for safety coordination, AWS Clean Rooms for contamination tracing without exposing proprietary data, AWS IoT Analytics for real-time safety monitoring, AWS B2B Data Interchange for automated regulatory compliance, Amazon Connect for outbreak response coordination, and AWS Cost Categories for recall-specific allocation",
        "Separate Organizations per company type, manual safety coordination, individual contamination tracking, separate compliance processes, and project-based billing",
        "Single Organization with basic IAM, S3 for safety data, manual contamination tracing, individual compliance tracking, and tagging for costs",
        "Hub-and-spoke architecture, centralized safety databases, API-based data sharing, unified compliance platform, and automated billing"
    ],
    correct: 0,
    explanation: {
        correct: "Supply-chain OUs provide safety coordination while maintaining competitive boundaries, Clean Rooms enables contamination tracing without exposing proprietary processes, IoT Analytics monitors food safety in real-time, B2B Data Interchange automates FDA/USDA compliance, Amazon Connect coordinates outbreak response, and Cost Categories enable precise recall allocation.",
        whyWrong: {
            1: "Separate Organizations eliminate contamination tracing capabilities and create food safety gaps",
            2: "Basic controls don't provide food safety coordination and lack contamination source tracking",
            3: "Centralized databases expose proprietary food processing methods and competitive information"
        },
        examStrategy: "For food safety: Clean Rooms enables contamination tracing while protecting proprietary processes, IoT Analytics provides real-time safety monitoring. Amazon Connect coordinates emergency response."
    }
},
{
    id: 'sap_223',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global gaming platform supports 1 billion players with monthly costs of $30M: EC2 game servers ($12M), DynamoDB player data ($8M), ElastiCache leaderboards ($5M), CloudFront content delivery ($3M), S3 game assets ($2M). Issues include: 60-second matchmaking during peak hours, game server scaling taking 20 minutes causing player disconnections, leaderboard inconsistencies across 15 regions, content download delays for 100GB+ game updates, and massive weekend over-provisioning costs.",
    question: "Which optimization strategy provides the GREATEST improvement in player experience and operational cost efficiency?",
    options: [
        "Pre-scale game servers for weekends, increase DynamoDB capacity, expand cache clusters, optimize CloudFront configurations, and implement Reserved Instances",
        "Migrate to Amazon GameLift with FlexMatch for <3-second intelligent matchmaking, implement GameLift Spot Fleet for 80% server cost reduction, use DynamoDB Global Tables with DAX for consistent real-time leaderboards, add CloudFront Origin Shield for game update optimization, implement GameLift automatic scaling eliminating player disconnections, and achieve 65% total cost reduction",
        "Containerize game servers on EKS with autoscaling, add DynamoDB global tables, implement ElastiCache Global Datastore, increase CloudFront edge locations, and use Compute Savings Plans",
        "Optimize current game server algorithms, implement DynamoDB auto-scaling, add Redis clustering, enable CloudFront compression, and negotiate enterprise pricing"
    ],
    correct: 1,
    explanation: {
        correct: "GameLift with FlexMatch reduces matchmaking from 60 seconds to <3 seconds using intelligent player matching, Spot Fleet provides 80% server cost reduction while maintaining availability, DynamoDB Global Tables with DAX ensure leaderboard consistency across 15 regions with microsecond latency, Origin Shield optimizes 100GB+ game update delivery, automatic scaling eliminates 20-minute delays preventing disconnections, achieving 65% total cost reduction.",
        whyWrong: {
            0: "Pre-scaling maintains expensive weekend over-provisioning and doesn't solve fundamental matchmaking delays",
            2: "EKS adds complexity and doesn't provide gaming-specific optimizations like intelligent matchmaking",
            3: "Optimizing current architecture doesn't solve 60-second matchmaking or 20-minute scaling delays"
        },
        examStrategy: "For gaming at scale: GameLift provides purpose-built game management with intelligent matchmaking. Spot Fleet dramatically reduces costs. Global Tables with DAX solve multi-region consistency."
    }
},
{
    id: 'sap_224',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state agriculture department must migrate crop monitoring and subsidy systems to AWS within 17 months for precision agriculture support. Current systems include: mainframe for subsidy calculations with 30 years of farm data, Oracle databases for crop insurance, integration with 10,000 farms, weather monitoring networks, and compliance with USDA agricultural regulations.",
    question: "Which migration approach maintains agricultural services while enabling precision agriculture capabilities?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, IoT Core for farms, custom weather systems, and manual USDA compliance",
        "Replace agriculture system with modern platform, PostgreSQL for insurance, API Gateway for farms, cloud weather monitoring, and automated compliance",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain farm connections, upgrade weather systems, and manual compliance tracking",
        "AWS Mainframe Modernization for subsidy calculation preservation, Amazon Location Service for precision agriculture field mapping, AWS IoT Analytics for farm monitoring, Amazon Forecast with Weather Index for crop predictions, and AWS Audit Manager for USDA compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 30 years of subsidy calculation logic while enabling precision agriculture, Location Service provides field mapping for precision agriculture support, IoT Analytics processes farm monitoring data, Forecast with Weather Index predicts crop conditions, and Audit Manager automates USDA regulatory compliance.",
        whyWrong: {
            0: "Aurora migration could break subsidy calculations and custom weather systems lack agricultural optimization",
            1: "Complete replacement risks agricultural services and loses decades of farm subsidy data",
            2: "Hybrid approach doesn't enable precision agriculture capabilities required for modernization"
        },
        examStrategy: "For agriculture migration: preserve subsidy calculation logic, Location Service enables precision agriculture. Weather Index provides agricultural weather integration. IoT Analytics processes farm data."
    }
},
{
    id: 'sap_225',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A renewable energy trading consortium is building a peer-to-peer platform for 100,000 solar/wind producers and 50,000 energy buyers. Requirements include: real-time energy price matching with <50ms latency, blockchain-based settlement for transparency, weather-based production forecasting, grid stability monitoring, automated demand response, and compliance with energy market regulations in 30 countries.",
    question: "Which architecture enables transparent renewable energy trading at consortium scale?",
    options: [
        "DynamoDB for pricing, custom blockchain on EC2, Lambda for forecasting, CloudWatch for grid monitoring, manual demand response, and basic compliance tracking",
        "ElastiCache for energy pricing, Ethereum on containers, SageMaker for weather predictions, IoT Core for grid data, automated response systems, and Config for compliance",
        "RDS for trading data, third-party blockchain, Batch for forecasting, Kinesis for grid streams, Step Functions for demand response, and manual regulatory compliance",
        "Amazon MemoryDB for ultra-low latency energy price matching, Amazon Managed Blockchain for transparent settlement with regulatory audit capabilities, Amazon Forecast with Weather Index for renewable production prediction, AWS IoT SiteWise for grid stability monitoring, AWS IoT Analytics for automated demand response, and AWS B2B Data Interchange for multi-country energy regulation compliance"
    ],
    correct: 3,
    explanation: {
        correct: "MemoryDB provides consistent <50ms latency for 150,000 participants, Managed Blockchain offers enterprise-grade transparency with regulatory audit trails, Forecast with Weather Index predicts renewable production accurately, IoT SiteWise monitors grid stability with industrial protocols, IoT Analytics enables automated demand response, and B2B Data Interchange handles energy regulations across 30 countries.",
        whyWrong: {
            0: "Custom blockchain lacks enterprise features needed for regulatory compliance and manual response doesn't scale",
            1: "Ethereum containers require significant management and ElastiCache alone can't handle 150,000 participants",
            2: "Third-party blockchain adds complexity and Step Functions add latency inappropriate for real-time trading"
        },
        examStrategy: "For energy trading: MemoryDB provides ultra-low latency at scale, Managed Blockchain offers enterprise features. Weather Index optimizes renewable forecasting. IoT SiteWise monitors industrial grid infrastructure."
    }
},
{
    id: 'sap_226',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global semiconductor consortium operates 1,000 AWS accounts across chip designers, foundries, and equipment manufacturers in 20 countries. Requirements include: intellectual property protection for chip designs, secure collaboration on joint research projects, automated compliance with export control regulations (ITAR/EAR), supply chain traceability for critical components, and cost allocation to individual chip development programs.",
    question: "Which governance architecture addresses semiconductor industry complexity while enabling innovation collaboration?",
    options: [
        "Basic Organizations structure, manual IP protection, individual research projects, separate compliance processes, and spreadsheet cost tracking",
        "Separate Organizations per company type, isolated IP systems, manual research coordination, individual export control, and project-based billing",
        "Single Organization with IAM policies, S3 for design data, manual collaboration, basic export compliance, and tagging for costs",
        "AWS Organizations with technology-node OUs and export control guardrails, AWS Clean Rooms for IP-protected chip design collaboration, AWS Supply Chain for component traceability, AWS DataZone for governed research data sharing, AWS Cost Categories for chip program allocation, and AWS Audit Manager for ITAR/EAR compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Technology-node OUs with export control guardrails ensure automated ITAR/EAR compliance, Clean Rooms enables chip design collaboration while protecting competitive IP, Supply Chain provides critical component traceability for semiconductor security, DataZone governs research data sharing with lineage tracking, Cost Categories enable complex chip program allocation, and Audit Manager automates export control compliance.",
        whyWrong: {
            0: "Basic structure doesn't meet export control requirements and lacks IP protection for semiconductor designs",
            1: "Separate Organizations eliminate joint research benefits and create massive compliance overhead",
            2: "Basic controls don't provide IP protection and export compliance required for semiconductor industry"
        },
        examStrategy: "For semiconductor industry: export control compliance is critical, Clean Rooms protects chip design IP. Supply Chain provides component traceability for security. DataZone governs sensitive research data."
    }
},
{
    id: 'sap_227',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global logistics platform tracks 500 million packages daily using: 5,000 EC2 instances for tracking updates ($3M/month), DynamoDB for package data ($1.5M/month), ElastiCache for real-time status ($800K/month), API Gateway for customer queries ($500K/month), S3 for shipping documents ($200K/month). Issues include: tracking update delays during peak shipping seasons, package location inconsistencies across regions, cache memory pressure during holiday rushes, API timeouts during Black Friday, and 70% infrastructure idle during off-peak periods.",
    question: "Which optimization strategy provides the BEST performance improvement and cost reduction for variable logistics traffic?",
    options: [
        "Pre-scale infrastructure for peak seasons, increase DynamoDB capacity, expand cache clusters, raise API Gateway limits, and purchase Reserved Instances",
        "Containerize on EKS with KEDA autoscaling, add DynamoDB global tables, implement ElastiCache clustering, optimize API configurations, and use Spot instances",
        "Migrate to ECS Fargate Spot for elastic tracking processing eliminating idle costs, implement DynamoDB Global Tables for package consistency, upgrade to MemoryDB with clustering for holiday traffic, add API Gateway regional endpoints with caching, and achieve 70% cost reduction through serverless-optimized architecture",
        "Optimize current tracking algorithms, implement DynamoDB auto-scaling, add Redis read replicas, enable API compression, and negotiate enterprise pricing"
    ],
    correct: 2,
    explanation: {
        correct: "Fargate Spot eliminates 70% idle costs while providing instant scaling for peak seasons, DynamoDB Global Tables ensure package location consistency across regions, MemoryDB clustering scales memory for holiday rushes with durability, API Gateway regional endpoints with caching handle Black Friday traffic efficiently, and serverless-optimized architecture achieves 70% cost reduction by eliminating off-peak idle infrastructure.",
        whyWrong: {
            0: "Pre-scaling maintains expensive over-provisioning and doesn't solve consistency issues across regions",
            1: "EKS adds operational complexity and doesn't achieve the cost optimization benefits of pure serverless",
            3: "Optimizing current architecture doesn't solve seasonal scalability or eliminate 70% idle capacity"
        },
        examStrategy: "For logistics optimization: Fargate Spot eliminates seasonal idle costs, DynamoDB Global Tables solve package consistency. MemoryDB handles holiday traffic spikes with durability."
    }
},
{
    id: 'sap_228',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A county election office must migrate voter registration and election management systems to AWS within 12 months for cybersecurity compliance. Current systems include: mainframe for voter registration with 40 years of electoral data, Oracle databases for ballot management, integration with 500 polling locations, voter check-in systems, and compliance with federal election security requirements.",
    question: "Which migration approach maintains election integrity while achieving cybersecurity compliance?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, IoT Core for polling locations, custom check-in systems, and basic security monitoring",
        "Replace election system with modern platform, PostgreSQL for ballot data, API Gateway for polling sites, cloud check-in applications, and automated security tools",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain polling connections, upgrade check-in systems, and manual security compliance",
        "AWS Mainframe Modernization for voter registration preservation, Amazon RDS Custom for Oracle maintaining ballot procedures, AWS IoT Device Management for polling location equipment, AWS Security Hub for election cybersecurity monitoring, and AWS Audit Manager for federal election security compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 40 years of voter registration data while enabling cybersecurity compliance, RDS Custom maintains Oracle ballot procedures critical for election integrity, IoT Device Management secures polling location equipment, Security Hub provides specialized election cybersecurity monitoring, and Audit Manager automates federal election security compliance.",
        whyWrong: {
            0: "Aurora migration could break ballot management procedures and basic monitoring doesn't meet election security requirements",
            1: "Complete replacement risks election integrity and loses decades of voter registration history",
            2: "Hybrid approach doesn't achieve cybersecurity compliance required for election systems"
        },
        examStrategy: "For election migration: preserve voter registration data integrity, Security Hub provides cybersecurity monitoring. Never risk election integrity for modernization speed. Audit Manager automates compliance."
    }
},
{
    id: 'sap_229',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national disaster response network is building a coordination platform for 100,000 emergency responders across natural disasters. Requirements include: real-time personnel tracking during disasters, satellite communication backup when infrastructure fails, AI-powered resource allocation optimization, integration with 2,000 emergency service agencies, predictive disaster impact modeling, and compliance with federal emergency management standards.",
    question: "Which architecture provides comprehensive disaster response coordination at national scale?",
    options: [
        "IoT Core for responder tracking, Direct Connect for reliability, Lambda for resource allocation, API Gateway for agencies, SageMaker for predictions, and Config for compliance",
        "AWS IoT Core for emergency device tracking with disaster-priority protocols, AWS Ground Station for satellite backup communication, AWS Supply Chain for emergency resource optimization, Amazon Forecast for disaster impact prediction, AWS B2B Data Interchange for emergency service coordination, and AWS Audit Manager for federal emergency compliance",
        "Custom tracking systems, satellite modems, manual resource coordination, point-to-point agency integration, historical disaster analysis, and manual compliance processes",
        "EC2 for coordination, GPS mobile apps, VPN communication, spreadsheet resource allocation, basic weather forecasting, and quarterly compliance reviews"
    ],
    correct: 1,
    explanation: {
        correct: "IoT Core handles 100,000 responder devices with disaster-priority communication protocols, Ground Station provides satellite backup when terrestrial infrastructure fails during disasters, Supply Chain optimizes emergency resource allocation across affected areas, Forecast predicts disaster impact using weather and historical data, B2B Data Interchange automates coordination with 2,000 agencies, and Audit Manager ensures federal emergency compliance.",
        whyWrong: {
            0: "Direct Connect fails during disasters and doesn't provide satellite backup capabilities",
            2: "Custom systems require extensive development and manual coordination doesn't scale to 100,000 responders",
            3: "GPS apps fail during infrastructure damage and manual processes can't handle national-scale coordination"
        },
        examStrategy: "For disaster response: Ground Station provides disaster-resilient satellite backup, Supply Chain optimizes resource allocation at scale. B2B Data Interchange handles emergency service standards."
    }
},
{
    id: 'sap_230',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global space industry consortium operates 600 AWS accounts across launch providers, satellite manufacturers, and ground station operators. Requirements include: mission-critical data protection for space operations, secure collaboration on joint missions, automated compliance with international space law, orbital debris tracking coordination, and cost allocation to individual space missions across partners.",
    question: "Which governance model enables space industry collaboration while maintaining mission-critical security?",
    options: [
        "AWS Organizations with mission-type OUs for space operations coordination, AWS Clean Rooms for mission data sharing without exposing proprietary technologies, AWS Ground Station for unified satellite communication, AWS B2B Data Interchange for international space law compliance, AWS Cost Categories for mission-specific allocation, and AWS Security Hub for space industry threat detection",
        "Separate Organizations per company type, manual mission coordination, individual satellite systems, separate compliance processes, and project-based billing",
        "Single Organization with basic IAM, S3 for mission data, manual space collaboration, individual compliance tracking, and tagging for costs",
        "Hub-and-spoke architecture, centralized mission databases, API-based collaboration, shared satellite systems, and automated billing"
    ],
    correct: 0,
    explanation: {
        correct: "Mission-type OUs provide space operations coordination while maintaining security boundaries, Clean Rooms enables mission collaboration without exposing proprietary space technologies, Ground Station provides unified satellite communication infrastructure, B2B Data Interchange automates international space law compliance, Cost Categories enable complex mission allocation, and Security Hub provides space industry-specific threat detection.",
        whyWrong: {
            1: "Separate Organizations eliminate joint mission benefits and create orbital coordination challenges",
            2: "Basic controls don't provide mission-critical security required for space operations",
            3: "Centralized databases expose proprietary space technologies and mission-critical information"
        },
        examStrategy: "For space industry: mission-critical security is paramount, Clean Rooms protects space technology IP. Ground Station provides space communications infrastructure. Security Hub provides industry-specific protection."
    }
},
{
    id: 'sap_231',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global ride-sharing platform serves 200 million users with monthly costs of $15M: EC2 for matching engine ($6M), DynamoDB for driver/rider data ($4M), ElastiCache for real-time tracking ($2.5M), API Gateway for mobile apps ($1.5M), CloudFront for maps ($1M). Issues include: 30-second driver matching during rush hours, driver location lag causing poor matches, memory pressure during surge pricing events, API throttling during promotional campaigns, and massive over-provisioning for 4-hour daily peaks.",
    question: "Which optimization strategy provides the GREATEST improvement in user experience and cost efficiency?",
    options: [
        "Pre-scale matching engine for rush hours, increase DynamoDB capacity, expand cache clusters, raise API Gateway limits, and purchase Reserved Instances",
        "Migrate to ECS Fargate Spot for elastic matching eliminating over-provisioning costs, implement DynamoDB Global Tables for consistent driver locations, upgrade to MemoryDB for real-time tracking with clustering, add API Gateway regional endpoints with intelligent caching, implement CloudFront with Lambda@Edge for map optimization, and achieve 65% cost reduction through serverless-first architecture",
        "Containerize on EKS with HPA, add DynamoDB auto-scaling, implement ElastiCache clustering, optimize API configurations, and use Compute Savings Plans",
        "Optimize matching algorithms, implement DynamoDB on-demand, add Redis read replicas, enable API compression, and negotiate volume discounts"
    ],
    correct: 1,
    explanation: {
        correct: "Fargate Spot eliminates over-provisioning costs for 4-hour peaks while providing instant scaling, DynamoDB Global Tables ensure driver location consistency preventing poor matches, MemoryDB with clustering provides real-time tracking performance for surge events, API Gateway regional endpoints with caching handle promotional campaigns, Lambda@Edge optimizes map delivery, and serverless-first architecture achieves 65% cost reduction.",
        whyWrong: {
            0: "Pre-scaling maintains expensive over-provisioning and doesn't solve driver location consistency issues",
            2: "EKS adds complexity and doesn't provide the cost optimization benefits of true serverless architecture",
            3: "Optimizing current architecture doesn't eliminate massive over-provisioning or solve regional consistency"
        },
        examStrategy: "For ride-sharing optimization: Fargate Spot eliminates peak over-provisioning, DynamoDB Global Tables solve location consistency. MemoryDB provides real-time performance with durability."
    }
},
{
    id: 'sap_232',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A federal customs agency must migrate trade processing systems to AWS within 18 months for international trade compliance. Current systems include: mainframe for tariff calculations with 60 years of trade data, Oracle databases for cargo manifests, integration with global shipping lines, automated cargo scanning systems, and compliance with World Trade Organization regulations.",
    question: "Which migration approach maintains trade operations while achieving international compliance modernization?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom shipping integrations, cloud scanning systems, and manual WTO compliance",
        "Replace customs system with modern platform, PostgreSQL for cargo data, API Gateway for shipping lines, automated scanning platforms, and cloud compliance tools",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain shipping connections, upgrade scanning systems, and manual compliance tracking",
        "AWS Mainframe Modernization for tariff calculation preservation, Amazon Location Service for cargo tracking optimization, AWS IoT Analytics for automated cargo scanning, AWS B2B Data Interchange for global shipping line integration, and AWS Audit Manager for World Trade Organization compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 60 years of tariff calculation logic while enabling modern compliance, Location Service optimizes cargo tracking with global port integration, IoT Analytics processes automated cargo scanning data, B2B Data Interchange handles diverse global shipping protocols, and Audit Manager automates World Trade Organization compliance.",
        whyWrong: {
            0: "Aurora migration could break tariff calculations and custom shipping integrations are complex and error-prone",
            1: "Complete replacement risks international trade operations and loses decades of tariff knowledge",
            2: "Hybrid approach doesn't achieve international compliance modernization required for WTO standards"
        },
        examStrategy: "For customs migration: preserve tariff calculation logic, Location Service optimizes cargo tracking. B2B Data Interchange handles international shipping standards. IoT Analytics processes cargo scanning data."
    }
},
{
    id: 'sap_233',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global precision medicine consortium is developing a personalized treatment platform analyzing multi-omics data for 10 million patients. Requirements include: genomics, proteomics, and metabolomics integration, AI-driven treatment recommendations, real-time biomarker monitoring, clinical trial matching across 1,000 research sites, secure collaboration between competing pharmaceutical companies, and FDA compliance for clinical decision support.",
    question: "Which architecture enables precision medicine at population scale while maintaining regulatory compliance?",
    options: [
        "Custom bioinformatics on GPU clusters, RDS for patient data, Lambda for treatment algorithms, API Gateway for research sites, and manual FDA compliance",
        "Amazon Omics for multi-omics analysis workflows optimized for precision medicine, AWS HealthLake for HIPAA-compliant patient data with personalized medicine FHIR profiles, Amazon SageMaker with healthcare compliance for AI treatment recommendations, AWS Clean Rooms for pharmaceutical collaboration without exposing proprietary data, AWS IoT Core for real-time biomarker monitoring, and AWS Audit Manager for FDA clinical decision support compliance",
        "S3 data lake for omics storage, EMR for analysis, generic ML models, Direct Connect to research sites, and Config for regulatory compliance",
        "Redshift for patient data, Aurora for clinical information, Batch for treatment predictions, VPN to pharmaceutical companies, and third-party compliance platforms"
    ],
    correct: 1,
    explanation: {
        correct: "Amazon Omics provides purpose-built multi-omics analysis optimized for precision medicine at population scale, HealthLake ensures HIPAA compliance with personalized medicine FHIR profiles, SageMaker with healthcare features enables compliant AI treatment recommendations, Clean Rooms enables pharmaceutical collaboration without exposing proprietary drug data, IoT Core handles real-time biomarker monitoring devices, and Audit Manager automates FDA clinical decision support compliance.",
        whyWrong: {
            0: "Custom GPU bioinformatics lacks precision medicine optimization and manual FDA compliance doesn't scale to 10M patients",
            2: "Generic EMR doesn't provide specialized multi-omics analysis and lacks healthcare compliance features",
            3: "Redshift isn't optimized for genomics data and VPN doesn't provide secure pharmaceutical collaboration"
        },
        examStrategy: "For precision medicine at scale: Amazon Omics for multi-omics analysis, HealthLake for personalized medicine compliance. Clean Rooms enables pharmaceutical collaboration while protecting IP."
    }
},
{
    id: 'sap_234',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global mining consortium operates 2,500 AWS accounts across 500 mines in 50 countries with extreme environmental conditions. Requirements include: environmental impact monitoring for multiple governments, secure geological data sharing between competing partners, real-time safety monitoring with emergency response, equipment predictive maintenance in harsh conditions, operational cost optimization per mine, and protection of proprietary extraction techniques.",
    question: "Which governance architecture addresses mining industry challenges while enabling global operations?",
    options: [
        "Basic Organizations structure, manual environmental reporting, individual geological databases, traditional safety systems, and spreadsheet cost tracking",
        "Separate Organizations per country, isolated monitoring systems, manual safety procedures, individual compliance processes, and project-based billing",
        "Single Organization with IAM policies, S3 for geological data, CloudWatch monitoring, tagging for costs, and quarterly environmental reports",
        "AWS Organizations with mining-operation OUs and environmental guardrails, AWS Clean Rooms for geological data sharing without exposing extraction techniques, AWS IoT SiteWise Edge for safety monitoring with offline resilience, Amazon Lookout for Equipment for predictive maintenance in harsh conditions, AWS Cost Categories for mine-level optimization, and AWS B2B Data Interchange for automated government environmental reporting"
    ],
    correct: 3,
    explanation: {
        correct: "Mining-operation OUs with environmental guardrails ensure automated compliance across 50 countries, Clean Rooms enables geological collaboration while protecting proprietary extraction techniques, IoT SiteWise Edge provides safety monitoring with offline operation for remote mines, Lookout for Equipment predicts failures in harsh mining environments, Cost Categories enable mine-level optimization, and B2B Data Interchange automates environmental reporting to multiple governments.",
        whyWrong: {
            0: "Basic structure doesn't address harsh mining environments and environmental compliance complexity",
            1: "Separate Organizations break collaboration and create massive environmental compliance overhead",
            2: "Basic controls don't provide mining industry safety requirements and IP protection for extraction techniques"
        },
        examStrategy: "For mining operations: IoT SiteWise Edge handles harsh environments with offline capability, Clean Rooms protects extraction IP. Lookout for Equipment provides predictive maintenance for extreme conditions."
    }
},
{
    id: 'sap_235',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global e-learning platform serves 100 million students with monthly costs of $8M: EC2 for video processing ($3M), S3 for course content ($2M), DynamoDB for student progress ($1.5M), CloudFront for delivery ($1M), RDS for metadata ($500K). Issues include: video processing queues backing up 48+ hours during course launches, 96% of content never accessed after 6 months, student progress inconsistencies across regions, content delivery delays during exam periods, and 80% infrastructure idle during summer breaks.",
    question: "Which optimization strategy provides the MOST significant improvement in learning experience and cost efficiency?",
    options: [
        "Add more EC2 video processing capacity, implement S3 lifecycle policies, increase DynamoDB capacity, optimize CloudFront configurations, and purchase Reserved Instances",
        "Migrate to AWS Elemental MediaConvert for serverless video processing eliminating queues, implement S3 Intelligent-Tiering moving 96% content to Archive tiers saving $1.92M annually, use DynamoDB Global Tables for student progress consistency, add CloudFront Origin Shield for exam period optimization, and eliminate 80% idle costs through serverless architecture achieving 70% total cost reduction",
        "Containerize video processing on EKS, manually archive old content, add DynamoDB global tables, increase CloudFront edge locations, and use Spot instances",
        "Optimize video encoding parameters, compress course files, implement DynamoDB auto-scaling, enable CloudFront compression, and negotiate volume discounts"
    ],
    correct: 1,
    explanation: {
        correct: "MediaConvert provides serverless video processing eliminating 48-hour queues through parallel processing, S3 Intelligent-Tiering automatically moves 96% to Archive tiers saving $1.92M annually, DynamoDB Global Tables ensure student progress consistency across regions, Origin Shield optimizes content delivery during exam periods, and serverless architecture eliminates 80% summer idle costs achieving 70% total cost reduction.",
        whyWrong: {
            0: "Adding more EC2 increases costs without solving fundamental queuing and seasonal idle issues",
            2: "Manual archiving is error-prone and EKS doesn't solve video processing queue bottlenecks",
            3: "Optimizing current architecture doesn't solve 48-hour queues or eliminate massive seasonal idle costs"
        },
        examStrategy: "For e-learning optimization: MediaConvert eliminates video processing bottlenecks, S3 Intelligent-Tiering provides massive storage savings. Serverless architecture eliminates seasonal idle costs."
    }
},
{
    id: 'sap_236',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A regional hospital network must migrate patient care systems to AWS within 15 months for interoperability compliance. Current systems include: Epic EHR with 25 years of patient records, Oracle databases for medical billing, integration with 200 medical devices, laboratory information systems, and compliance with HIPAA and HITECH regulations.",
    question: "Which migration approach maintains patient care continuity while achieving interoperability modernization?",
    options: [
        "Lift-and-shift Epic to EC2, Oracle to Aurora migration, IoT Core for devices, custom lab systems, and manual HIPAA compliance",
        "Replace EHR with cloud-native system, PostgreSQL for billing, API Gateway for devices, cloud lab platform, and automated compliance tools",
        "Keep Epic on-premises, gradual database migration, maintain device connections, upgrade lab systems, and manual compliance tracking",
        "AWS HealthLake for EHR data aggregation with FHIR interoperability, Amazon RDS Custom for Oracle preserving billing procedures, AWS IoT Core for medical device integration, AWS Application Migration Service for laboratory systems, and AWS Audit Manager for HIPAA/HITECH compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "HealthLake aggregates Epic data with FHIR interoperability enabling modern healthcare standards, RDS Custom preserves Oracle billing procedures critical for hospital operations, IoT Core integrates 200 medical devices with healthcare protocols, MGN ensures seamless laboratory system migration, and Audit Manager automates HIPAA/HITECH compliance evidence collection.",
        whyWrong: {
            0: "Aurora migration could break medical billing procedures and manual compliance doesn't meet healthcare requirements",
            1: "EHR replacement risks patient care and loses 25 years of medical history",
            2: "Keeping Epic on-premises doesn't achieve interoperability modernization required for compliance"
        },
        examStrategy: "For healthcare migration: HealthLake provides FHIR interoperability, preserve billing procedures with RDS Custom. IoT Core handles medical device protocols. Never risk patient care for modernization."
    }
},
{
    id: 'sap_237',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart city initiative is building an integrated urban management platform for a metropolitan area with 15 million residents. Requirements include: traffic optimization across 50,000 intersections, air quality monitoring with 5,000 sensors, smart parking with dynamic pricing for 1 million spaces, emergency response coordination, energy grid optimization, and citizen engagement through mobile applications with real-time city services.",
    question: "Which architecture provides comprehensive smart city management at metropolitan scale?",
    options: [
        "EC2 for traffic control, IoT Core for sensors, Lambda for parking logic, CloudWatch for emergency monitoring, basic energy tracking, and mobile APIs",
        "AWS Panorama for computer vision traffic optimization, AWS IoT SiteWise for air quality monitoring, Amazon Location Service for smart parking optimization with dynamic pricing, Amazon Connect for emergency response coordination, AWS IoT Analytics for energy grid optimization, and AWS AppSync for real-time citizen mobile engagement with offline synchronization",
        "Custom traffic systems, DynamoDB for sensor data, manual parking management, traditional emergency systems, spreadsheet energy tracking, and native mobile apps",
        "Kubernetes for traffic processing, Timestream for sensor storage, microservices for parking, Step Functions for emergency workflows, CloudWatch for energy monitoring, and API Gateway for mobile"
    ],
    correct: 1,
    explanation: {
        correct: "Panorama provides computer vision for 50,000 intersections without bandwidth costs, IoT SiteWise handles industrial-grade air quality monitoring, Location Service optimizes 1M parking spaces with dynamic pricing, Amazon Connect scales emergency coordination for 15M residents, IoT Analytics optimizes energy grid operations, and AppSync provides real-time citizen engagement with offline sync for mobile reliability.",
        whyWrong: {
            0: "Generic EC2 lacks computer vision capabilities for traffic optimization and basic tracking doesn't optimize energy grids",
            2: "Custom systems require extensive development and manual parking doesn't scale to 1M spaces",
            3: "Kubernetes adds complexity and doesn't provide smart city-specific optimization features"
        },
        examStrategy: "For smart cities: Panorama for computer vision traffic optimization, IoT SiteWise for industrial monitoring. Location Service for spatial optimization. AppSync provides real-time citizen engagement."
    }
},
{
    id: 'sap_238',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global pharmaceutical research alliance operates 800 AWS accounts across universities, biotech companies, and pharmaceutical corporations. Requirements include: intellectual property protection for drug discoveries, secure collaboration on approved research projects, automated compliance with FDA, EMA, and international drug regulations, clinical trial coordination, and cost allocation to individual research grants across institutions.",
    question: "Which governance model enables pharmaceutical research collaboration while protecting competitive interests?",
    options: [
        "AWS Organizations with research-type OUs for collaboration coordination, AWS Clean Rooms for IP-protected drug research without exposing proprietary compounds, AWS HealthLake for clinical trial coordination with regulatory compliance, AWS DataZone for governed research data sharing, AWS Cost Categories for grant-level allocation, and AWS Audit Manager for multi-regulatory pharmaceutical compliance",
        "Separate Organizations per institution type, manual research coordination, individual drug databases, isolated trial systems, and spreadsheet grant tracking",
        "Single Organization with basic IAM, S3 for research data, manual IP protection, individual trial management, and tagging for costs",
        "Hub-and-spoke architecture, centralized drug databases, API-based research sharing, unified trial platform, and automated billing"
    ],
    correct: 0,
    explanation: {
        correct: "Research-type OUs provide collaboration while maintaining competitive boundaries, Clean Rooms enables drug research collaboration without exposing proprietary compounds, HealthLake coordinates clinical trials with regulatory compliance, DataZone governs research data sharing with lineage tracking, Cost Categories enable complex grant allocation, and Audit Manager automates FDA/EMA compliance across jurisdictions.",
        whyWrong: {
            1: "Separate Organizations eliminate research collaboration benefits and create massive regulatory overhead",
            2: "Basic controls don't provide IP protection required for pharmaceutical drug discovery",
            3: "Centralized databases expose proprietary drug compounds and competitive research information"
        },
        examStrategy: "For pharmaceutical research: Clean Rooms protects drug discovery IP during collaboration, HealthLake provides clinical trial compliance. DataZone governs sensitive research data sharing."
    }
},
{
    id: 'sap_239',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global video conferencing platform supports 500 million daily users with monthly costs of $25M: EC2 for media processing ($10M), DynamoDB for session data ($6M), ElastiCache for real-time features ($4M), CloudFront for content delivery ($3M), S3 for recordings ($2M). Issues include: media processing delays during peak hours, session data inconsistencies across regions, memory pressure during large meetings, content delivery delays for high-resolution video, and massive over-provisioning for unpredictable usage spikes.",
    question: "Which optimization strategy provides the GREATEST improvement in user experience and cost efficiency?",
    options: [
        "Pre-scale media processing for peak hours, increase DynamoDB capacity, expand cache clusters, optimize CloudFront configurations, and purchase Reserved Instances",
        "Migrate to AWS Elemental MediaLive for real-time media processing, implement DynamoDB Global Tables for session consistency, upgrade to MemoryDB with clustering for large meetings, add CloudFront Origin Shield for high-resolution video optimization, implement S3 Intelligent-Tiering for recording optimization, and achieve 60% cost reduction through managed services optimization",
        "Containerize on EKS with KEDA autoscaling, add DynamoDB auto-scaling, implement ElastiCache clustering, increase CloudFront distributions, and use Spot instances",
        "Optimize media algorithms, implement DynamoDB on-demand, add Redis read replicas, enable CloudFront compression, and negotiate enterprise pricing"
    ],
    correct: 1,
    explanation: {
        correct: "MediaLive provides real-time media processing eliminating peak hour delays through managed scaling, DynamoDB Global Tables ensure session consistency across regions, MemoryDB clustering scales memory for large meetings with durability, Origin Shield optimizes high-resolution video delivery reducing bandwidth costs, S3 Intelligent-Tiering optimizes recording storage automatically, and managed services achieve 60% cost reduction by eliminating over-provisioning.",
        whyWrong: {
            0: "Pre-scaling maintains expensive over-provisioning and doesn't solve session consistency issues",
            2: "EKS adds complexity and doesn't provide the media processing optimization of purpose-built services",
            3: "Optimizing current architecture doesn't solve media processing delays or eliminate over-provisioning"
        },
        examStrategy: "For video conferencing: MediaLive provides real-time media processing, DynamoDB Global Tables solve session consistency. MemoryDB handles large meeting memory demands with durability."
    }
},
{
    id: 'sap_240',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national tax collection agency must migrate taxpayer systems to AWS within 20 months for digital transformation. Current systems include: mainframe for tax processing with 50 years of tax code implementation, Oracle databases for taxpayer records, integration with 10,000 tax preparers, audit case management systems, and compliance with federal tax regulations and privacy laws.",
    question: "Which migration approach maintains tax collection operations while enabling digital taxpayer services?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom preparer APIs, cloud audit systems, and manual compliance tracking",
        "Replace tax system with modern platform, PostgreSQL for taxpayer data, API Gateway for preparers, automated audit tools, and cloud compliance frameworks",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain preparer connections, upgrade audit systems, and manual privacy compliance",
        "AWS Mainframe Modernization for tax code implementation preservation, Amazon RDS Custom for Oracle maintaining taxpayer procedures, AWS B2B Data Interchange for tax preparer integration, AWS Application Migration Service for audit systems, and AWS Audit Manager for federal tax regulation compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 50 years of tax code implementation while enabling digital services, RDS Custom maintains Oracle taxpayer procedures critical for tax operations, B2B Data Interchange handles diverse tax preparer protocols and standards, MGN ensures seamless audit system migration, and Audit Manager automates federal tax regulation compliance.",
        whyWrong: {
            0: "Aurora migration could break taxpayer procedures and custom preparer APIs are complex for 10,000 integrations",
            1: "Complete replacement risks tax collection and loses decades of tax code implementation",
            2: "Hybrid approach doesn't enable digital transformation required for taxpayer services"
        },
        examStrategy: "For tax agency migration: preserve tax code implementation logic, B2B Data Interchange handles tax preparer standards. RDS Custom maintains taxpayer data procedures. Never risk tax collection integrity."
    }
},
{
    id: 'sap_241',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global logistics consortium is building an autonomous shipping platform coordinating 10,000 cargo vessels with AI-powered route optimization. Requirements include: real-time weather routing across all oceans, autonomous collision avoidance using maritime AI, port coordination for 2,000 harbors worldwide, cargo monitoring for hazardous materials, crew health monitoring, and compliance with international maritime regulations.",
    question: "Which architecture enables intelligent autonomous shipping coordination at global scale?",
    options: [
        "EC2 for route processing, Lambda for collision avoidance, API Gateway for ports, IoT Core for cargo monitoring, basic crew tracking, and manual maritime compliance",
        "AWS IoT FleetWise for vessel systems integration, Amazon Location Service for weather-optimized route planning, Amazon SageMaker for maritime AI collision avoidance, AWS Supply Chain for port coordination, AWS IoT Core for hazmat cargo monitoring, AWS HealthLake for crew medical monitoring, and AWS B2B Data Interchange for international maritime regulation compliance",
        "Custom ship management systems, third-party weather services, manual collision prevention, Direct Connect to ports, basic cargo tracking, and quarterly maritime compliance",
        "Kubernetes for vessel processing, SageMaker for route optimization, custom AI models, VPN to harbors, DynamoDB for cargo data, and Config for compliance"
    ],
    correct: 1,
    explanation: {
        correct: "IoT FleetWise manages 10,000 vessels with maritime protocols and OTA updates, Location Service provides weather-optimized routing across all oceans, SageMaker enables maritime AI for collision avoidance, Supply Chain coordinates with 2,000 ports globally, IoT Core monitors hazmat cargo with compliance tracking, HealthLake manages crew medical data with maritime health standards, and B2B Data Interchange handles international maritime regulations.",
        whyWrong: {
            0: "Generic EC2 route processing lacks maritime optimization and manual compliance doesn't scale globally",
            2: "Custom ship management doesn't provide autonomous capabilities and manual collision prevention is unsafe",
            3: "Kubernetes adds complexity and VPN doesn't scale to 2,000 global ports efficiently"
        },
        examStrategy: "For autonomous shipping: IoT FleetWise for vessel fleet management, Location Service for maritime route optimization. Supply Chain for global port coordination. HealthLake for crew medical monitoring."
    }
},
{
    id: 'sap_242',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global aerospace defense consortium operates 1,500 AWS accounts across prime contractors, subcontractors, and allied nations for joint fighter aircraft development. Requirements include: complete program isolation for classified projects, secure collaboration on approved joint programs, automated compliance with ITAR, EAR, and allied nation security standards, supply chain security for critical components, and cost allocation to individual aircraft programs across international partners.",
    question: "Which governance architecture addresses aerospace defense complexity while enabling international collaboration?",
    options: [
        "Basic Organizations structure, manual program isolation, individual collaboration tools, separate compliance processes, and spreadsheet cost tracking",
        "Separate Organizations per nation, isolated program databases, manual collaboration coordination, individual security standards, and project-based billing",
        "Single Organization with classification IAM, S3 for program data, manual collaboration, basic export compliance, and tagging for costs",
        "AWS Organizations with program-classification OUs and automated security guardrails, AWS Clean Rooms for joint program collaboration without exposing classified data, AWS Supply Chain for component security traceability, AWS DataZone for governed aerospace data sharing, AWS Cost Categories for international program allocation, and AWS Audit Manager for ITAR/EAR/allied compliance automation"
    ],
    correct: 3,
    explanation: {
        correct: "Program-classification OUs with security guardrails ensure automated ITAR/EAR compliance, Clean Rooms enables joint program collaboration while protecting classified aerospace data, Supply Chain provides security traceability for critical components, DataZone governs sensitive aerospace data sharing with lineage, Cost Categories enable complex international program allocation, and Audit Manager automates compliance across multiple security standards.",
        whyWrong: {
            0: "Basic structure doesn't meet defense classification requirements and lacks supply chain security",
            1: "Separate Organizations eliminate joint program benefits and create massive compliance overhead",
            2: "Basic controls don't provide classification isolation required for defense aerospace programs"
        },
        examStrategy: "For aerospace defense: classification-level isolation is critical, Clean Rooms protects classified data during collaboration. Supply Chain provides component security traceability. ITAR/EAR compliance automation is essential."
    }
},
{
    id: 'sap_243',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online education platform serves 200 million students with monthly costs of $12M: EC2 for content processing ($5M), S3 for course materials ($3M), DynamoDB for student data ($2M), CloudFront for delivery ($1.5M), RDS for course metadata ($500K). Issues include: content processing backlogs during enrollment periods, 97% of materials never accessed after graduation, student progress inconsistencies across regions, content delivery delays during exam periods, and 85% infrastructure idle during academic breaks.",
    question: "Which optimization strategy provides the MOST comprehensive improvement in educational experience and cost efficiency?",
    options: [
        "Add more EC2 processing capacity, implement S3 lifecycle policies, increase DynamoDB capacity, optimize CloudFront configurations, and purchase Reserved Instances",
        "Migrate to Lambda with SQS for elastic content processing eliminating enrollment backlogs, implement S3 Intelligent-Tiering moving 97% materials to Archive tiers saving $2.91M annually, use DynamoDB Global Tables for student progress consistency, add CloudFront Origin Shield for exam period optimization, and eliminate 85% idle costs through serverless architecture achieving 75% total cost reduction",
        "Containerize content processing on EKS, manually archive old materials, add DynamoDB global tables, increase CloudFront edge locations, and use Spot instances",
        "Optimize content algorithms, compress course files, implement DynamoDB auto-scaling, enable CloudFront compression, and negotiate academic pricing"
    ],
    correct: 1,
    explanation: {
        correct: "Lambda with SQS provides infinite scaling for enrollment periods eliminating backlogs, S3 Intelligent-Tiering automatically moves 97% to Archive tiers saving $2.91M annually, DynamoDB Global Tables ensure student progress consistency across regions, Origin Shield optimizes content delivery during exam periods, and serverless architecture eliminates 85% academic break idle costs achieving 75% total cost reduction.",
        whyWrong: {
            0: "Adding more EC2 increases costs without solving fundamental enrollment scaling and seasonal idle issues",
            2: "Manual archiving is error-prone and EKS doesn't solve content processing backlogs during enrollment",
            3: "Optimizing current architecture doesn't solve enrollment backlogs or eliminate massive seasonal idle costs"
        },
        examStrategy: "For education optimization: Lambda with SQS eliminates enrollment backlogs, S3 Intelligent-Tiering provides massive storage savings. Serverless architecture eliminates academic break idle costs."
    }
},
{
    id: 'sap_244',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state unemployment agency must migrate benefit processing systems to AWS within 16 months for fraud prevention modernization. Current systems include: mainframe for benefit calculations with 40 years of unemployment law implementation, Oracle databases for claimant records, integration with 500 employers, call center systems, and compliance with federal unemployment regulations.",
    question: "Which migration approach maintains benefit processing while enabling fraud prevention capabilities?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom employer APIs, cloud call center, and manual fraud detection",
        "Replace unemployment system with modern platform, PostgreSQL for claimant data, API Gateway for employers, automated call systems, and AI fraud detection",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain employer connections, upgrade call center, and basic fraud tools",
        "AWS Mainframe Modernization for benefit calculation preservation, Amazon Fraud Detector for ML-powered benefit fraud detection, AWS B2B Data Interchange for employer integration, Amazon Connect for call center modernization, and AWS Audit Manager for federal unemployment regulation compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 40 years of unemployment law implementation while enabling fraud prevention, Fraud Detector provides ML-powered benefit fraud detection, B2B Data Interchange handles diverse employer reporting formats, Amazon Connect modernizes call center operations, and Audit Manager automates federal unemployment regulation compliance.",
        whyWrong: {
            0: "Aurora migration could break benefit calculations and manual fraud detection doesn't provide modern capabilities",
            1: "Complete replacement risks benefit processing and loses decades of unemployment law implementation",
            2: "Hybrid approach doesn't enable fraud prevention modernization required for system security"
        },
        examStrategy: "For unemployment agency migration: preserve benefit calculation logic, Fraud Detector provides ML-powered fraud prevention. B2B Data Interchange handles employer reporting standards."
    }
},
{
    id: 'sap_245',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A renewable energy grid consortium is building a carbon-neutral energy trading platform connecting 100,000 renewable producers, 50,000 energy storage systems, and 10,000 corporate buyers. Requirements include: real-time carbon intensity tracking, blockchain-based renewable energy certificates, predictive grid balancing, automated demand response, energy storage optimization, and compliance with carbon neutrality standards in 40 countries.",
    question: "Which architecture enables comprehensive carbon-neutral energy trading at consortium scale?",
    options: [
        "DynamoDB for trading data, custom blockchain on EC2, Lambda for grid balancing, manual demand response, basic storage tracking, and manual carbon compliance",
        "Amazon MemoryDB for real-time carbon intensity tracking, Amazon Managed Blockchain for renewable energy certificate transparency, AWS IoT Analytics for predictive grid balancing, AWS Supply Chain for automated demand response coordination, Amazon Lookout for Equipment for energy storage optimization, and AWS Audit Manager for carbon neutrality compliance across 40 countries",
        "ElastiCache for energy data, Ethereum on containers, SageMaker for predictions, API Gateway for demand response, CloudWatch for storage monitoring, and Config for compliance",
        "RDS for carbon tracking, third-party blockchain, Batch for grid analysis, Step Functions for automation, custom storage algorithms, and manual regulatory compliance"
    ],
    correct: 1,
    explanation: {
        correct: "MemoryDB provides real-time carbon intensity tracking for 160,000 participants, Managed Blockchain ensures renewable certificate transparency with regulatory audit capabilities, IoT Analytics enables predictive grid balancing with renewable intermittency, Supply Chain coordinates automated demand response across buyers, Lookout for Equipment optimizes energy storage performance, and Audit Manager automates carbon neutrality compliance across 40 countries.",
        whyWrong: {
            0: "Custom blockchain lacks enterprise features for regulatory compliance and manual processes don't scale to consortium level",
            2: "Ethereum containers require management overhead and CloudWatch doesn't provide energy storage optimization",
            3: "Third-party blockchain adds complexity and Step Functions don't provide real-time energy trading capabilities"
        },
        examStrategy: "For carbon-neutral energy: MemoryDB for real-time carbon tracking, Managed Blockchain for certificate transparency. Supply Chain coordinates demand response. Lookout for Equipment optimizes storage systems."
    }
},
{
    id: 'sap_246',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global automotive consortium develops electric vehicles with 1,000 AWS accounts across automakers, battery manufacturers, and charging network operators. Requirements include: battery technology IP protection, secure collaboration on charging standards, automated compliance with automotive safety regulations, supply chain transparency for battery materials, and cost allocation to individual EV development programs.",
    question: "Which governance model enables automotive collaboration while protecting competitive battery technologies?",
    options: [
        "AWS Organizations with automotive-segment OUs for collaboration coordination, AWS Clean Rooms for battery technology sharing without exposing proprietary chemistry, AWS Supply Chain for battery material traceability, AWS IoT SiteWise for charging network monitoring, AWS Cost Categories for EV program allocation, and AWS Audit Manager for automotive safety compliance",
        "Separate Organizations per company type, manual technology sharing, individual supply chain tracking, isolated charging networks, and spreadsheet program tracking",
        "Single Organization with basic IAM, S3 for battery data, manual collaboration, individual charging systems, and tagging for costs",
        "Hub-and-spoke architecture, centralized battery databases, API-based collaboration, shared charging platforms, and automated billing"
    ],
    correct: 0,
    explanation: {
        correct: "Automotive-segment OUs provide collaboration while maintaining competitive boundaries, Clean Rooms enables battery technology sharing without exposing proprietary chemistry, Supply Chain provides battery material traceability for sustainability, IoT SiteWise monitors charging network performance, Cost Categories enable complex EV program allocation, and Audit Manager automates automotive safety compliance.",
        whyWrong: {
            1: "Separate Organizations eliminate charging standard collaboration and create supply chain visibility gaps",
            2: "Basic controls don't provide battery IP protection required for competitive automotive development",
            3: "Centralized databases expose proprietary battery chemistry and competitive technology information"
        },
        examStrategy: "For automotive consortiums: Clean Rooms protects battery IP while enabling collaboration, Supply Chain provides material traceability. IoT SiteWise monitors charging infrastructure performance."
    }
},
{
    id: 'sap_247',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global fintech platform processes 1 billion payment transactions daily with monthly costs of $40M: DynamoDB ($15M), Lambda ($10M), API Gateway ($8M), ElastiCache ($4M), CloudFront ($3M). Issues include: DynamoDB throttling during payment surges, Lambda cold starts during flash sales, API Gateway timeouts during market volatility, cache evictions during promotional events, and exponential cost scaling with transaction volume growth.",
    question: "Which optimization strategy provides the BEST performance and cost control for extreme payment transaction volumes?",
    options: [
        "Increase DynamoDB provisioned capacity, use Lambda reserved concurrency, raise API Gateway limits, expand cache clusters, and implement data compression",
        "Add Aurora for transaction consistency, containerize Lambda functions, implement API caching, use Redis clustering, and purchase Reserved Instances",
        "Switch to RDS for ACID compliance, implement Step Functions for orchestration, use Application Load Balancer, deploy multiple cache layers, and add CDN optimization",
        "Implement DynamoDB on-demand with DAX for microsecond payment latency, migrate to Lambda with provisioned concurrency eliminating cold starts, add API Gateway regional endpoints with edge caching, use ElastiCache Global Datastore for payment consistency, and implement CloudFront with Lambda@Edge for payment interface optimization achieving predictable cost scaling"
    ],
    correct: 3,
    explanation: {
        correct: "DynamoDB on-demand with DAX provides automatic scaling with microsecond latency for payment surges, Lambda provisioned concurrency eliminates cold starts during flash sales, API Gateway regional endpoints with edge caching handle market volatility, ElastiCache Global Datastore ensures payment consistency across regions, Lambda@Edge optimizes payment interfaces, and this architecture provides predictable cost scaling vs exponential growth.",
        whyWrong: {
            0: "Pre-provisioning leads to massive over-provisioning costs and doesn't handle unpredictable payment surges",
            1: "Aurora adds complexity for payment systems optimized for DynamoDB's performance characteristics",
            2: "RDS can't scale to 1 billion daily transactions and Step Functions add latency inappropriate for payments"
        },
        examStrategy: "For payment processing at scale: DynamoDB on-demand with DAX handles extreme volumes, Lambda provisioned concurrency prevents cold starts. Global Datastore ensures payment consistency."
    }
},
{
    id: 'sap_248',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A state lottery commission must migrate gaming systems to AWS within 14 months for security modernization. Current systems include: mainframe for number generation with 35 years of randomization algorithms, Oracle databases for ticket sales, integration with 10,000 retail locations, prize payout systems, and compliance with gaming regulations and security standards.",
    question: "Which migration approach maintains lottery integrity while achieving security modernization?",
    options: [
        "Lift-and-shift mainframe to EC2, Oracle to Aurora migration, custom retail APIs, cloud payout systems, and basic security monitoring",
        "Replace lottery system with modern platform, PostgreSQL for ticket data, API Gateway for retailers, automated payouts, and cloud security tools",
        "Keep mainframe with hybrid connectivity, gradual database migration, maintain retail connections, upgrade payout systems, and manual security compliance",
        "AWS Mainframe Modernization for random number generation preservation, AWS CloudHSM for cryptographic lottery security, AWS B2B Data Interchange for retail location integration, Amazon RDS Custom for Oracle maintaining payout procedures, and AWS Audit Manager for gaming regulation compliance"
    ],
    correct: 3,
    explanation: {
        correct: "Mainframe Modernization preserves 35 years of proven randomization algorithms critical for lottery integrity, CloudHSM provides FIPS 140-2 Level 3 cryptographic security for gaming operations, B2B Data Interchange handles diverse retail location protocols, RDS Custom maintains Oracle payout procedures, and Audit Manager automates gaming regulation compliance.",
        whyWrong: {
            0: "Aurora migration could break payout procedures and basic monitoring doesn't meet gaming security requirements",
            1: "Complete replacement risks lottery integrity and loses decades of randomization algorithms",
            2: "Hybrid approach doesn't achieve security modernization required for gaming compliance"
        },
        examStrategy: "For lottery migration: preserve randomization algorithms for integrity, CloudHSM provides gaming-grade cryptographic security. B2B Data Interchange handles retail location standards."
    }
},
{
    id: 'sap_249',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global space exploration consortium is building a mission coordination platform for Mars colonization involving 50 space agencies and private companies. Requirements include: deep space communication with 40-minute delays, autonomous spacecraft operation during communication blackouts, resource allocation for multi-year missions, crew health monitoring for long-duration spaceflight, coordination with Earth-based mission control centers, and compliance with international space law.",
    question: "Which architecture enables comprehensive Mars colonization mission coordination at consortium scale?",
    options: [
        "EC2 for mission control, S3 for space data, Lambda for resource allocation, CloudWatch for crew monitoring, Direct Connect to agencies, and manual space law compliance",
        "AWS Ground Station for deep space communication with delay buffering, AWS IoT Greengrass for autonomous spacecraft operation during blackouts, AWS Supply Chain for multi-year resource allocation, AWS HealthLake for crew medical monitoring, AWS B2B Data Interchange for space agency coordination, and AWS Audit Manager for international space law compliance",
        "Custom ground stations, Kubernetes for spacecraft autonomy, DynamoDB for resource data, RDS for crew health, VPN to mission control, and quarterly space compliance",
        "Satellite communication on EC2, containers for mission logic, EFS for resource storage, Aurora for health data, API Gateway for agencies, and Config for compliance"
    ],
    correct: 1,
    explanation: {
        correct: "Ground Station handles deep space protocols with 40-minute delay buffering and global coverage, Greengrass enables autonomous spacecraft operation during communication blackouts, Supply Chain manages complex multi-year resource allocation, HealthLake monitors crew health with long-duration spaceflight medical standards, B2B Data Interchange coordinates 50 agencies and companies, and Audit Manager automates international space law compliance.",
        whyWrong: {
            0: "Generic EC2 lacks deep space communication protocols and Direct Connect doesn't work for space agencies",
            2: "Custom ground stations lack redundancy and Kubernetes isn't optimized for spacecraft autonomy",
            3: "Satellite communication on EC2 doesn't provide deep space capabilities and VPN doesn't scale globally"
        },
        examStrategy: "For space exploration: Ground Station provides deep space communication with delay handling, Greengrass enables autonomous operation during blackouts. Supply Chain manages complex multi-year logistics."
    }
},
{
    id: 'sap_250',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial services consortium operates 5,000 AWS accounts across banks, insurance companies, and investment firms managing $100 trillion in assets across 200 countries. Requirements include: complete financial isolation between competing institutions, secure collaboration for systemic risk monitoring, automated compliance with Basel III, Solvency II, and 200+ country-specific regulations, unified fraud detection across the consortium, and precise cost allocation to individual financial products and jurisdictions.",
    question: "Which governance architecture addresses financial services complexity while enabling systemic risk coordination?",
    options: [
        "Basic Organizations structure, manual risk coordination, individual compliance processes, separate fraud systems, and spreadsheet cost allocation",
        "Separate Organizations per institution type, isolated risk monitoring, manual compliance tracking, individual fraud detection, and project-based billing",
        "Single Organization with financial IAM, S3 for risk data, manual collaboration, shared fraud platform, and unified billing",
        "AWS Organizations with institution-type OUs and automated regulatory guardrails, AWS Clean Rooms for systemic risk monitoring without exposing proprietary trading data, Amazon FinSpace for consortium-wide financial analytics, Amazon Fraud Detector for unified fraud detection, AWS Cost Categories for product and jurisdiction allocation, and AWS Audit Manager for multi-regulatory financial compliance across 200 countries"
    ],
    correct: 3,
    explanation: {
        correct: "Institution-type OUs with regulatory guardrails ensure automated compliance across 200 countries, Clean Rooms enables systemic risk monitoring while protecting competitive trading data, FinSpace provides consortium-wide financial analytics with built-in compliance, Fraud Detector offers unified protection without data sharing, Cost Categories handle complex product and jurisdiction allocation, and Audit Manager automates compliance across multiple regulatory frameworks.",
        whyWrong: {
            0: "Basic structure doesn't meet systemic risk monitoring and lacks multi-country regulatory compliance",
            1: "Separate Organizations eliminate systemic risk benefits and create massive compliance overhead",
            2: "Single Organization doesn't provide financial isolation required for competing institutions"
        },
        examStrategy: "For financial services at scale: Clean Rooms enables systemic risk monitoring while protecting competitive data, FinSpace provides financial industry analytics. Multi-regulatory compliance automation is essential."
    }
},

// SAP-C02 Questions 251-300 (Batch 6) - 50 Questions - PROPERLY RANDOMIZED ANSWERS

{
    id: 'sap_251',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global streaming platform launches a new interactive feature allowing 100 million concurrent users to vote in real-time during live events. Votes must be processed within 50ms, aggregated instantly, and displayed globally with sub-second latency. The system must handle 10 million votes per second during peak events while maintaining cost efficiency during lower traffic periods.",
    question: "Which architecture provides OPTIMAL real-time performance at global scale?",
    options: [
        "Amazon ElastiCache Redis Global Datastore for vote storage, AWS Lambda with provisioned concurrency for processing, Amazon Kinesis Data Streams for aggregation, and CloudFront with Lambda@Edge for global distribution",
        "Amazon DynamoDB Global Tables with DAX acceleration, Amazon Kinesis Data Analytics for real-time aggregation, API Gateway with caching, and CloudFront for global distribution",
        "Amazon Aurora Global Database with read replicas, Amazon SQS for vote queuing, AWS Batch for processing, and CloudFront for content delivery",
        "Amazon ElastiCache Redis cluster mode, Amazon EventBridge for event routing, AWS Step Functions for orchestration, and Application Load Balancer for distribution"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB Global Tables with DAX provides single-digit millisecond performance globally with multi-region replication. Kinesis Data Analytics enables real-time vote aggregation. API Gateway caching reduces backend load. CloudFront ensures global sub-second distribution.",
        whyWrong: {
            0: "Lambda@Edge has payload limitations and higher latency than DynamoDB Global Tables for global consistency",
            2: "Aurora Global has write limitations and SQS adds latency that violates the 50ms requirement",
            3: "EventBridge and Step Functions add unnecessary latency for simple vote aggregation workflows"
        },
        examStrategy: "For global real-time applications: DynamoDB Global Tables + DAX + Kinesis Analytics is the proven pattern for sub-50ms performance."
    }
},
{
    id: 'sap_252',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A legacy e-commerce application running on EC2 instances experiences 300% traffic spikes during flash sales, causing frequent outages. The application uses a monolithic architecture with a MySQL database that becomes the bottleneck. The company wants to modernize incrementally without complete redesign.",
    question: "Which incremental modernization strategy provides the BEST immediate improvement?",
    options: [
        "Migrate to Amazon ECS Fargate with auto scaling, implement Amazon RDS Proxy for connection pooling, and use ElastiCache for session management",
        "Deploy the application on AWS Lambda with API Gateway, use DynamoDB for data storage, and implement CloudFront for caching",
        "Implement Amazon Aurora Auto Scaling for database scaling, Application Load Balancer for traffic distribution, and CloudWatch auto scaling for EC2 instances",
        "Containerize with Amazon EKS, implement Horizontal Pod Autoscaler, use Amazon DocumentDB for flexibility, and Redis for caching"
    ],
    correct: 2,
    explanation: {
        correct: "Aurora Auto Scaling provides immediate database scaling without application changes. ALB distributes traffic efficiently. CloudWatch auto scaling handles EC2 instance scaling automatically. This requires minimal application modification.",
        whyWrong: {
            0: "ECS migration requires significant containerization effort and application changes",
            1: "Lambda migration requires complete application restructuring, not incremental improvement",
            3: "EKS adds Kubernetes complexity and DocumentDB requires data model changes"
        },
        examStrategy: "Incremental modernization: Start with Aurora Auto Scaling + ALB + EC2 Auto Scaling before major architectural changes."
    }
},
{
    id: 'sap_253',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A healthcare organization with 15 AWS accounts must comply with HIPAA, implement zero-trust networking, and enable secure research data sharing between institutions. Each account handles different patient data types with varying sensitivity levels. All network traffic must be encrypted, logged, and monitored for anomalies.",
    question: "Which comprehensive security architecture ensures HIPAA compliance and zero-trust implementation?",
    options: [
        "AWS Organizations with SCPs, AWS Transit Gateway with route tables, VPC Flow Logs with Amazon Detective, AWS Config for compliance monitoring, and AWS Systems Manager for patch management",
        "AWS Security Hub for centralized monitoring, AWS WAF for application protection, AWS Direct Connect for private connectivity, Amazon GuardDuty for threat detection, and AWS Secrets Manager for credential management",
        "AWS Landing Zone for account setup, Amazon VPC with NACLs, AWS Certificate Manager for SSL, Amazon CloudWatch for monitoring, and AWS IAM for access control",
        "AWS Control Tower for account governance, AWS Network Firewall with intrusion detection, AWS PrivateLink for service communication, AWS CloudTrail with CloudWatch anomaly detection, and AWS KMS with customer-managed keys"
    ],
    correct: 3,
    explanation: {
        correct: "Control Tower provides HIPAA-compliant account governance. Network Firewall offers deep packet inspection and intrusion detection. PrivateLink ensures private service communication. CloudTrail + CloudWatch provides comprehensive anomaly detection. Customer-managed KMS keys ensure encryption control.",
        whyWrong: {
            0: "Transit Gateway alone doesn't provide intrusion detection or deep packet inspection capabilities",
            1: "Security Hub is monitoring-focused but doesn't provide the network-level zero-trust controls needed",
            2: "Landing Zone is deprecated in favor of Control Tower, and basic VPC/NACLs don't provide zero-trust capabilities"
        },
        examStrategy: "HIPAA + Zero-trust: Control Tower + Network Firewall + PrivateLink + customer-managed KMS is the comprehensive approach."
    }
},
{
    id: 'sap_254',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A manufacturing company needs to migrate 50 TB of historical sensor data from on-premises Oracle databases to AWS. The data must remain available during migration, and the new system should enable real-time analytics on incoming sensor data while maintaining 7-year retention for compliance.",
    question: "Which migration and modernization strategy provides OPTIMAL performance and compliance?",
    options: [
        "Use AWS DMS for Oracle to RDS Aurora migration, Amazon Kinesis for real-time sensor data, Amazon Redshift for analytics, and S3 Glacier for long-term retention",
        "Implement AWS DataSync for data transfer, migrate to Amazon DynamoDB, use Amazon Kinesis Data Analytics for real-time processing, and S3 Intelligent-Tiering for retention",
        "Deploy AWS Database Migration Service to Amazon Aurora, use AWS Glue for ETL processing, Amazon EMR for analytics, and S3 Standard for 7-year storage",
        "Use AWS Snow family for bulk transfer, migrate to Amazon RDS MySQL, implement Amazon QuickSight for analytics, and S3 Deep Archive for compliance retention"
    ],
    correct: 0,
    explanation: {
        correct: "DMS provides online migration from Oracle to Aurora with minimal downtime. Kinesis handles real-time sensor streams efficiently. Redshift is optimized for analytics workloads. S3 Glacier provides cost-effective 7-year retention with compliance features.",
        whyWrong: {
            1: "DynamoDB isn't optimal for analytical workloads and lacks SQL compatibility for Oracle migration",
            2: "EMR is overkill for standard analytics and S3 Standard is expensive for 7-year retention",
            3: "Snow family requires offline transfer causing downtime, and MySQL lacks Oracle compatibility features"
        },
        examStrategy: "Oracle migration: DMS to Aurora + Kinesis for real-time + Redshift for analytics + S3 Glacier for retention."
    }
},
{
    id: 'sap_255',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A fintech startup builds a high-frequency trading platform requiring sub-millisecond order execution, processing 1 million orders per second, with strict regulatory compliance for order audit trails. The system must handle market data feeds from multiple exchanges and execute complex algorithmic trading strategies.",
    question: "Which architecture achieves ULTRA-LOW latency while maintaining regulatory compliance?",
    options: [
        "Amazon EC2 C6gn instances with enhanced networking, Amazon ElastiCache Redis with cluster mode, AWS Lambda for order processing, and Amazon Kinesis for audit trails",
        "AWS Nitro System with SR-IOV networking, Amazon MemoryDB for Redis, Amazon EventBridge for order routing, and Amazon S3 for compliance storage",
        "Amazon EC2 R6i instances with placement groups, Amazon ElastiCache Redis with pipeline mode, Amazon SQS FIFO for order queuing, and Amazon DynamoDB for audit logs",
        "AWS Outposts with dedicated hardware, Amazon ElastiCache Redis with local zones, AWS Direct Connect for market data feeds, and Amazon Timestream for time-series audit data"
    ],
    correct: 3,
    explanation: {
        correct: "Outposts provides dedicated hardware for ultra-low latency. ElastiCache in Local Zones minimizes network hops. Direct Connect ensures reliable, low-latency market data feeds. Timestream is purpose-built for high-volume time-series audit data with automatic compression.",
        whyWrong: {
            0: "Lambda has inherent latency and isn't suitable for sub-millisecond requirements",
            1: "EventBridge adds latency and isn't designed for high-frequency trading workloads",
            2: "SQS FIFO has throughput limitations and adds latency vs direct processing"
        },
        examStrategy: "Ultra-low latency trading: Outposts + Local Zones + Direct Connect + Timestream for dedicated performance."
    }
},
{
    id: 'sap_256',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "An existing web application hosted on EC2 instances experiences inconsistent performance during traffic spikes. CloudWatch metrics show CPU utilization varies from 10% to 95%, memory usage spikes unpredictably, and response times become erratic. The application uses session-based authentication and has a MySQL database backend.",
    question: "Which optimization strategy provides the MOST consistent performance improvement?",
    options: [
        "Implement Amazon ElastiCache for session storage, use RDS Read Replicas for database scaling, enable Predictive Scaling for EC2 Auto Scaling, and implement Application Load Balancer sticky sessions",
        "Migrate sessions to Amazon DynamoDB, implement Aurora Auto Scaling, use Target Tracking Scaling based on ALB request count, and enable CloudFront caching",
        "Use Amazon EFS for shared session storage, implement RDS Proxy for connection pooling, enable scheduled scaling during peak hours, and implement AWS Global Accelerator",
        "Store sessions in S3, use DynamoDB for database migration, implement Lambda for auto scaling triggers, and use CloudFront with WAF protection"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB provides fast, consistent session storage without memory pressure on EC2. Aurora Auto Scaling handles database load automatically. Target Tracking with ALB metrics scales based on actual request patterns. CloudFront reduces backend load significantly.",
        whyWrong: {
            0: "Predictive Scaling can be unreliable for unpredictable traffic patterns, and sticky sessions reduce load balancing effectiveness",
            2: "EFS adds network latency for session access, and scheduled scaling doesn't adapt to actual traffic patterns",
            3: "S3 has higher latency for session access, and Lambda triggers add complexity without addressing root causes"
        },
        examStrategy: "Performance optimization: DynamoDB sessions + Aurora Auto Scaling + Target Tracking + CloudFront for consistent performance."
    }
},
{
    id: 'sap_257',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A large enterprise has 200+ AWS accounts across multiple business units and geographic regions. They need centralized billing, cost allocation by department, automated resource tagging enforcement, and the ability to apply security policies across all accounts while allowing local autonomy.",
    question: "Which organizational strategy provides COMPREHENSIVE governance with operational flexibility?",
    options: [
        "AWS Organizations with consolidated billing, AWS Config for compliance, AWS CloudFormation StackSets for standardization, and AWS Service Catalog for self-service provisioning",
        "AWS Control Tower for landing zones, Service Control Policies for guardrails, AWS Cost and Usage Reports for chargeback, and AWS Systems Manager for patch management",
        "AWS Resource Groups for organization, AWS Budgets for cost control, AWS CloudTrail for auditing, and AWS IAM for access management",
        "AWS Organizations with SCPs, AWS Trusted Advisor for optimization, AWS Well-Architected Tool for reviews, and AWS Personal Health Dashboard for monitoring"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower provides pre-configured landing zones with security guardrails. SCPs enforce organizational policies while allowing local flexibility. Cost and Usage Reports enable detailed chargeback. Systems Manager provides centralized operational management.",
        whyWrong: {
            0: "Config and StackSets alone don't provide the comprehensive governance needed for 200+ accounts",
            2: "Resource Groups don't provide organizational-level governance or security policy enforcement",
            3: "Trusted Advisor and Well-Architected Tool are advisory tools, not governance enforcement mechanisms"
        },
        examStrategy: "Large-scale governance: Control Tower + SCPs + Cost Reports + Systems Manager for comprehensive management."
    }
},
{
    id: 'sap_258',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A legacy mainframe application processes 10 million daily transactions using COBOL code and DB2 databases. The application has complex business logic, regulatory requirements, and tight coupling between components. The company wants to modernize to cloud while minimizing business risk and maintaining performance.",
    question: "Which modernization approach provides the LOWEST risk while enabling cloud benefits?",
    options: [
        "Rewrite application using microservices architecture on Amazon ECS, migrate data to Amazon DynamoDB, implement API Gateway for service communication, and use AWS Step Functions for workflow orchestration",
        "Use AWS Mainframe Modernization with automated COBOL conversion, migrate to Amazon RDS for DB2, implement AWS Lambda for business logic, and use Amazon EventBridge for integration",
        "Implement strangler fig pattern with AWS App2Container, gradually migrate components to AWS Fargate, use Amazon Aurora for database, and implement AWS AppSync for API management",
        "Rehost mainframe to Amazon EC2 using AWS Mainframe Modernization, implement AWS Database Migration Service for DB2 to Aurora migration, and use AWS Application Discovery Service for dependency mapping"
    ],
    correct: 3,
    explanation: {
        correct: "AWS Mainframe Modernization provides specialized tools for mainframe workloads with minimal code changes. DMS handles DB2 migration with data validation. Application Discovery Service maps dependencies to reduce migration risk. This lift-and-shift approach minimizes business disruption.",
        whyWrong: {
            0: "Complete rewrite introduces maximum risk and business disruption for critical mainframe workloads",
            1: "Automated COBOL conversion may not handle complex business logic correctly, introducing functional risks",
            2: "App2Container doesn't handle mainframe applications, and strangler fig requires significant development effort"
        },
        examStrategy: "Mainframe modernization: Start with Mainframe Modernization service + DMS for lowest risk, then gradually modernize components."
    }
},
{
    id: 'sap_259',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A social media platform needs to implement a content recommendation engine that analyzes user behavior in real-time and serves personalized content to 50 million active users. The system must process user interactions, update recommendation models hourly, and serve recommendations with sub-100ms latency.",
    question: "Which architecture provides OPTIMAL real-time personalization at scale?",
    options: [
        "Amazon Kinesis Data Streams for user events, Amazon SageMaker for model training, Amazon ElastiCache for serving recommendations, and Amazon API Gateway for content delivery",
        "Amazon Kinesis Data Firehose for data ingestion, AWS Batch for model training, Amazon DynamoDB for recommendations storage, and CloudFront for content caching",
        "Amazon EventBridge for event routing, Amazon EMR for model processing, Amazon RDS for recommendation data, and Application Load Balancer for traffic distribution",
        "Amazon MSK for event streaming, Amazon SageMaker Processing for batch training, Amazon Neptune for recommendation graphs, and AWS Lambda for API responses"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis Data Streams provides real-time event processing for user interactions. SageMaker enables automated model training and deployment. ElastiCache delivers sub-100ms recommendation serving. API Gateway provides scalable content delivery with caching.",
        whyWrong: {
            1: "Firehose is for batch processing, not real-time analysis, and Batch doesn't support hourly model updates efficiently",
            2: "EventBridge adds latency for high-volume events, and RDS can't scale to serve 50M users with sub-100ms latency",
            3: "MSK adds complexity, Neptune graph queries are slower than cache lookups, and Lambda has cold start latency"
        },
        examStrategy: "Real-time recommendations: Kinesis + SageMaker + ElastiCache + API Gateway for optimal performance."
    }
},
{
    id: 'sap_260',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An enterprise data warehouse built on Amazon Redshift experiences query performance degradation as data volume grows from 100TB to 1PB. ETL jobs take 8+ hours to complete, concurrent queries cause resource contention, and storage costs are increasing 40% annually. Business users need faster query response times and more concurrent access.",
    question: "Which optimization strategy provides the BEST performance and cost improvement?",
    options: [
        "Implement Redshift Spectrum for S3 data querying, use Redshift Concurrency Scaling for burst workloads, implement automated vacuum and analyze operations, and partition tables by date",
        "Migrate to Amazon EMR with Apache Spark, implement Delta Lake for data versioning, use Presto for interactive queries, and store data in S3 with columnar format",
        "Deploy Amazon Athena for ad-hoc queries, implement AWS Glue for ETL automation, use S3 Intelligent-Tiering for cost optimization, and create materialized views for common queries",
        "Upgrade to Redshift Serverless for automatic scaling, implement Amazon QuickSight for business intelligence, use AWS DataSync for data movement, and compress data with advanced encoding"
    ],
    correct: 0,
    explanation: {
        correct: "Redshift Spectrum allows querying S3 data without loading into Redshift, reducing storage costs. Concurrency Scaling handles burst queries automatically. Automated maintenance improves performance. Date partitioning optimizes query performance for time-based analytics.",
        whyWrong: {
            1: "EMR migration introduces complexity and requires application changes, with uncertain performance gains",
            2: "Athena alone can't replace data warehouse capabilities and may have slower performance for complex queries",
            3: "Redshift Serverless doesn't address the fundamental data organization and performance optimization needs"
        },
        examStrategy: "Redshift optimization: Spectrum + Concurrency Scaling + automated maintenance + partitioning for performance and cost."
    }
},
{
    id: 'sap_261',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global consulting firm with 500+ AWS accounts across 40 countries needs to implement data residency requirements, comply with GDPR/CCPA, enable cross-border data sharing for specific projects, and maintain separate billing for client work while consolidating security monitoring.",
    question: "Which governance architecture ensures COMPREHENSIVE compliance and operational efficiency?",
    options: [
        "Use separate AWS Organizations per region, AWS DataSync for controlled data transfer, AWS Cost Explorer for cost allocation, and Amazon CloudWatch for monitoring",
        "Deploy AWS Landing Zone with geographic account structure, AWS PrivateLink for secure communication, AWS Trusted Advisor for compliance, and AWS CloudFormation for standardization",
        "Create region-specific root accounts, AWS Transit Gateway for connectivity, AWS Resource Access Manager for sharing, and AWS Systems Manager for operational management",
        "Implement AWS Control Tower with region-restricted OUs, AWS Config for data residency compliance, AWS Organizations for consolidated billing, and AWS Security Hub for centralized monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "Control Tower provides automated region-restricted OUs ensuring data residency. Config monitors compliance continuously. Organizations enables consolidated billing with detailed cost allocation. Security Hub provides centralized security monitoring across all accounts.",
        whyWrong: {
            0: "Separate Organizations creates management overhead and prevents consolidated billing benefits",
            1: "Landing Zone is deprecated, and Trusted Advisor doesn't provide compliance enforcement",
            2: "Separate root accounts eliminate centralized governance and consolidated billing advantages"
        },
        examStrategy: "Global compliance: Control Tower + Config + Organizations + Security Hub for comprehensive governance."
    }
},
{
    id: 'sap_262',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A retail company needs to migrate their inventory management system from on-premises Oracle databases to AWS. The system processes real-time stock updates from 2,000 stores, supports complex queries for demand forecasting, and requires 99.9% availability during peak shopping seasons.",
    question: "Which migration strategy provides OPTIMAL performance and availability?",
    options: [
        "Migrate to Amazon RDS Oracle, implement Multi-AZ deployment, use Amazon Redshift for analytics, and AWS Database Migration Service for ongoing replication",
        "Deploy Oracle on Amazon EC2 with dedicated hosts, use EBS-optimized instances, implement AWS Storage Gateway for hybrid connectivity, and Amazon S3 for backup storage",
        "Migrate to Amazon DynamoDB with Global Tables, use DynamoDB Accelerator for performance, implement AWS Lambda for data processing, and Amazon Kinesis for real-time updates",
        "Use AWS DMS for Oracle to Amazon Aurora migration, implement Aurora Global Database for multi-region availability, Amazon ElastiCache for query acceleration, and AWS Backup for automated backup management"
    ],
    correct: 3,
    explanation: {
        correct: "DMS provides seamless Oracle to Aurora migration with minimal downtime. Aurora Global Database ensures 99.9% availability across regions. ElastiCache accelerates complex forecasting queries. AWS Backup provides automated, compliant backup management.",
        whyWrong: {
            0: "Staying on Oracle eliminates cloud-native benefits and doesn't provide cost optimization",
            1: "EC2-based Oracle requires more operational overhead and doesn't leverage managed services",
            2: "DynamoDB requires significant application rewrite and doesn't support complex SQL queries needed for forecasting"
        },
        examStrategy: "Oracle migration: DMS to Aurora + Global Database + ElastiCache for optimal cloud-native performance."
    }
},
{
    id: 'sap_263',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A gaming company develops a multiplayer battle royale game expecting 1 million concurrent players globally. The game requires real-time player synchronization, voice chat, leaderboards, and anti-cheat detection. Players must be matched based on skill level and geographic proximity with sub-50ms latency.",
    question: "Which architecture provides OPTIMAL gaming performance and scalability?",
    options: [
        "Amazon GameLift for game hosting, Amazon ElastiCache Redis for player state, Amazon Chime SDK for voice chat, Amazon DynamoDB for leaderboards, and AWS Lambda for matchmaking logic",
        "Amazon EC2 Auto Scaling groups for game servers, Amazon MemoryDB for player synchronization, Amazon Connect for voice services, Amazon Redshift for analytics, and Amazon API Gateway for client communication",
        "AWS Batch for server management, Amazon RDS Aurora for player data, Amazon Kinesis for real-time communication, Amazon S3 for static content, and AWS Step Functions for game orchestration",
        "Amazon ECS Fargate for microservices, Amazon DocumentDB for player profiles, Amazon EventBridge for game events, Amazon CloudFront for content delivery, and AWS AppSync for real-time updates"
    ],
    correct: 0,
    explanation: {
        correct: "GameLift provides managed game server hosting with auto scaling and fleet management. ElastiCache Redis delivers sub-millisecond player state synchronization. Chime SDK offers low-latency voice chat. DynamoDB handles high-volume leaderboard updates. Lambda enables flexible matchmaking algorithms.",
        whyWrong: {
            1: "Connect is for contact centers, not gaming voice chat, and MemoryDB is overkill for gaming state",
            2: "Batch is for background processing, not real-time game hosting, and Step Functions add latency",
            3: "ECS Fargate doesn't provide gaming-specific optimizations, and EventBridge adds latency vs direct communication"
        },
        examStrategy: "Gaming architecture: GameLift + ElastiCache + Chime SDK + DynamoDB + Lambda for optimal real-time performance."
    }
},
{
    id: 'sap_264',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "An e-learning platform experiences performance issues during exam periods when thousands of students submit assignments simultaneously. The current architecture uses EC2 instances with ELB, RDS MySQL, and S3 for file storage. Database writes become a bottleneck, and file uploads slow significantly.",
    question: "Which optimization provides the BEST scalability for concurrent submissions?",
    options: [
        "Implement Amazon Aurora Serverless v2 for automatic database scaling, Amazon S3 Transfer Acceleration for faster uploads, Amazon ElastiCache for session management, and AWS Lambda for processing submission workflows",
        "Deploy Amazon RDS Read Replicas for scaling, enable S3 Cross-Region Replication, implement Amazon CloudFront for file delivery, and AWS Auto Scaling for EC2 capacity",
        "Use Amazon DocumentDB for flexible data storage, implement AWS DataSync for file transfer, deploy Amazon ElastiCache Redis cluster, and AWS Batch for background processing",
        "Migrate to Amazon DynamoDB for submission metadata, use Amazon S3 multipart uploads, implement Amazon SQS for submission queuing, and AWS Step Functions for workflow orchestration"
    ],
    correct: 3,
    explanation: {
        correct: "DynamoDB handles burst write patterns better than RDS for submission metadata. S3 multipart uploads improve performance for large files. SQS decouples submissions from processing, preventing bottlenecks. Step Functions orchestrate complex submission workflows reliably.",
        whyWrong: {
            0: "Aurora Serverless v2 still has write limitations compared to DynamoDB for burst workloads",
            1: "Read Replicas don't help with write bottlenecks, and CRR doesn't improve upload performance",
            2: "DocumentDB doesn't solve write scaling issues, and DataSync is for bulk transfers, not real-time uploads"
        },
        examStrategy: "Burst write optimization: DynamoDB + S3 multipart + SQS + Step Functions for handling concurrent submissions."
    }
},
{
    id: 'sap_265',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A pharmaceutical company with 50 AWS accounts needs to implement strict access controls for drug research data, maintain separation between research teams, enable collaboration on approved projects, and ensure compliance with FDA 21 CFR Part 11 requirements.",
    question: "Which access control strategy ensures COMPREHENSIVE security and compliance?",
    options: [
        "AWS IAM Identity Center for centralized access, AWS Organizations SCPs for regulatory compliance, AWS Resource Access Manager for controlled sharing, and AWS CloudTrail with tamper detection for audit trails",
        "AWS Directory Service for user management, AWS Config for compliance monitoring, AWS PrivateLink for secure communication, and Amazon S3 with object-level logging",
        "AWS Cognito for user authentication, AWS WAF for application protection, AWS KMS for encryption, and Amazon CloudWatch for monitoring",
        "AWS Security Hub for centralized compliance, AWS GuardDuty for threat detection, AWS Secrets Manager for credential management, and AWS Certificate Manager for SSL/TLS"
    ],
    correct: 0,
    explanation: {
        correct: "IAM Identity Center provides centralized access with MFA and temporary credentials. SCPs enforce regulatory guardrails organization-wide. RAM enables controlled resource sharing between research teams. CloudTrail with tamper detection ensures FDA-compliant audit trails.",
        whyWrong: {
            1: "Directory Service alone doesn't provide the granular access controls needed for drug research data",
            2: "Cognito is for customer-facing applications, not enterprise access control",
            3: "Security Hub provides monitoring but doesn't enforce the access controls required for research data"
        },
        examStrategy: "Pharmaceutical compliance: Identity Center + SCPs + RAM + CloudTrail for comprehensive access control."
    }
},
{
    id: 'sap_266',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A manufacturing company needs to migrate a complex ERP system with 500+ interconnected modules, 10TB databases, real-time production data feeds, and integration with 50+ external systems. The migration must maintain business continuity and cannot afford extended downtime.",
    question: "Which migration strategy minimizes business risk while enabling modernization?",
    options: [
        "Use AWS Migration Hub for coordination, implement strangler fig pattern with AWS App2Container, deploy AWS Application Load Balancer for traffic routing, and AWS CloudFormation for infrastructure as code",
        "Deploy AWS Snow family for bulk data transfer, use AWS Direct Connect for hybrid connectivity, implement AWS Systems Manager for server management, and AWS Backup for data protection",
        "Utilize AWS Professional Services for assessment, implement AWS Well-Architected reviews, use AWS Trusted Advisor for optimization, and AWS Support for migration assistance",
        "Implement AWS Application Migration Service for server replication, use AWS Database Migration Service with change data capture, AWS DataSync for file system migration, and AWS Server Migration Service for gradual cutover"
    ],
    correct: 3,
    explanation: {
        correct: "Application Migration Service provides block-level server replication with minimal downtime. DMS with CDC ensures real-time database synchronization. DataSync migrates file systems continuously. SMS enables gradual, controlled cutover reducing business risk.",
        whyWrong: {
            0: "Strangler fig pattern requires significant development effort and doesn't address database migration complexity",
            1: "Snow family requires offline transfer causing downtime, and doesn't handle real-time data synchronization",
            2: "Professional Services and reviews are assessment tools, not migration execution strategies"
        },
        examStrategy: "Complex ERP migration: Application Migration Service + DMS + DataSync + SMS for minimal downtime migration."
    }
},
{
    id: 'sap_267',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A logistics company builds a real-time package tracking system that must process GPS updates from 100,000 delivery vehicles every 10 seconds, calculate estimated delivery times, send notifications to customers, and provide live tracking through a mobile app.",
    question: "Which architecture provides OPTIMAL real-time tracking and customer experience?",
    options: [
        "Amazon Kinesis Data Streams for GPS data, AWS Lambda for processing, Amazon DynamoDB for tracking data, Amazon SNS for notifications, and AWS AppSync for real-time mobile updates",
        "Amazon MSK for data streaming, Amazon EMR for processing, Amazon Aurora for data storage, Amazon SES for notifications, and AWS API Gateway for mobile API",
        "Amazon EventBridge for event routing, AWS Batch for computation, Amazon RDS for tracking database, Amazon Pinpoint for messaging, and Amazon CloudFront for mobile content",
        "Amazon SQS for message queuing, Amazon EC2 for processing, Amazon ElastiCache for caching, Amazon Connect for notifications, and AWS Mobile SDK for app integration"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis Data Streams handles high-volume GPS data efficiently. Lambda processes location updates with automatic scaling. DynamoDB provides fast lookups for tracking queries. SNS delivers notifications reliably. AppSync provides real-time subscriptions for mobile apps.",
        whyWrong: {
            1: "MSK and EMR add unnecessary complexity for simple location processing, and SES is for email, not push notifications",
            2: "EventBridge and Batch add latency for real-time tracking, and Pinpoint doesn't provide real-time subscriptions",
            3: "SQS adds latency vs streaming, Connect is for contact centers, and Mobile SDK alone doesn't provide real-time updates"
        },
        examStrategy: "Real-time tracking: Kinesis + Lambda + DynamoDB + SNS + AppSync for optimal mobile experience."
    }
},
{
    id: 'sap_268',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A SaaS application serving 10,000 tenants experiences database performance issues as customer data grows to 50TB. Each tenant has isolated schemas, queries are becoming slower, backup windows affect availability, and operational costs are increasing rapidly.",
    question: "Which optimization strategy provides the BEST scalability and cost efficiency?",
    options: [
        "Implement database sharding with Amazon Aurora, use Aurora Backtrack for instant recovery, Amazon RDS Proxy for connection pooling, and Amazon ElastiCache for query acceleration",
        "Deploy Amazon Redshift for analytics workloads, use Amazon Aurora Serverless for operational data, implement AWS Glue for ETL, and Amazon S3 for data archiving",
        "Implement tenant-specific Aurora clusters, use Aurora Cross-Region replication, Amazon RDS Performance Insights for monitoring, and AWS Database Migration Service for tenant migration",
        "Migrate to multi-tenant Amazon DynamoDB design, implement DynamoDB Auto Scaling, use DynamoDB Accelerator for performance, and DynamoDB Global Tables for availability"
    ],
    correct: 3,
    explanation: {
        correct: "DynamoDB's multi-tenant design patterns scale horizontally without performance degradation. Auto Scaling handles varying tenant loads automatically. DAX provides microsecond latency. Global Tables ensure high availability without backup windows affecting operations.",
        whyWrong: {
            0: "Aurora sharding adds operational complexity and doesn't solve the fundamental multi-tenant scaling challenge",
            1: "Redshift is for analytics, not operational SaaS workloads, and doesn't address the multi-tenant schema challenge",
            2: "Per-tenant clusters are cost-prohibitive at scale and create massive operational overhead"
        },
        examStrategy: "SaaS scaling: DynamoDB multi-tenant design + Auto Scaling + DAX + Global Tables for optimal performance."
    }
},
{
    id: 'sap_269',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational bank operates in 25 countries with strict data sovereignty requirements, needs to share aggregated analytics across regions while keeping customer data local, and must comply with Basel III capital requirements for risk calculations.",
    question: "Which architecture ensures COMPREHENSIVE compliance while enabling global analytics?",
    options: [
        "Deploy region-specific AWS accounts with AWS Control Tower, use AWS PrivateLink for cross-region communication, implement AWS Glue DataBrew for data preparation, and Amazon QuickSight for analytics dashboards",
        "Create separate AWS Organizations per country, use AWS DataSync for controlled data transfer, implement Amazon EMR for analytics processing, and AWS Config for compliance monitoring",
        "Deploy AWS Landing Zone with country-specific accounts, use AWS Direct Connect for connectivity, implement Amazon Kinesis for data streaming, and AWS CloudFormation for standardization",
        "Implement AWS Organizations with geographic OUs, use AWS Lake Formation for data governance, Amazon Redshift with data sharing for analytics, and AWS KMS with region-specific keys"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with geographic OUs ensure data sovereignty by organizational structure. Lake Formation provides granular data governance and access controls. Redshift data sharing enables analytics without data movement. Region-specific KMS keys ensure encryption compliance.",
        whyWrong: {
            0: "Control Tower alone doesn't provide the data governance needed for financial compliance",
            1: "Separate Organizations prevent consolidated governance and DataSync moves raw data violating sovereignty",
            2: "Landing Zone is deprecated, and Kinesis streaming across borders may violate data sovereignty requirements"
        },
        examStrategy: "Banking compliance: Organizations + Lake Formation + Redshift data sharing + region-specific KMS for sovereignty."
    }
},
{
    id: 'sap_270',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A media company needs to migrate their content management system handling 100TB of video files from on-premises storage to AWS. The system must support global content delivery, transcoding workflows, and maintain low-latency access for editors worldwide.",
    question: "Which migration and modernization approach provides OPTIMAL global performance?",
    options: [
        "Use AWS DataSync for file migration, Amazon S3 for storage, AWS Elemental MediaConvert for transcoding, and Amazon CloudFront for global distribution",
        "Implement AWS Storage Gateway for hybrid access, Amazon EFS for shared storage, AWS Batch for video processing, and AWS Global Accelerator for performance",
        "Deploy AWS Snow family for bulk transfer, Amazon FSx for high-performance storage, AWS Lambda for processing, and AWS Direct Connect for connectivity",
        "Use AWS Migration Hub for coordination, Amazon Glacier for archival, Amazon EC2 for transcoding, and AWS Transit Gateway for networking"
    ],
    correct: 0,
    explanation: {
        correct: "DataSync efficiently migrates large video files to AWS. S3 provides scalable, cost-effective storage with multiple tiers. MediaConvert is purpose-built for video transcoding at scale. CloudFront delivers content globally with low latency for editors.",
        whyWrong: {
            1: "Storage Gateway maintains hybrid complexity, EFS doesn't scale to 100TB efficiently, and Batch isn't optimized for video",
            2: "Snow family requires physical shipping delays, FSx is expensive for 100TB, and Lambda has time/memory limits for video",
            3: "Migration Hub is for coordination only, Glacier has retrieval delays, and EC2 transcoding lacks managed service benefits"
        },
        examStrategy: "Media migration: DataSync + S3 + MediaConvert + CloudFront for optimal video workflow modernization."
    }
},
{
    id: 'sap_271',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A biotechnology company develops a genome sequencing platform that processes 100GB files per sample, requires parallel analysis across multiple algorithms, generates terabytes of intermediate data, and must deliver results within 4 hours for clinical applications.",
    question: "Which architecture provides OPTIMAL computational performance for genomic analysis?",
    options: [
        "Amazon EC2 Spot Instances with HPC clusters, Amazon FSx for Lustre for shared storage, AWS Batch for job orchestration, and Amazon S3 Glacier for result archival",
        "AWS ParallelCluster for HPC workloads, Amazon EFS for shared file access, AWS Step Functions for workflow coordination, and Amazon S3 Intelligent-Tiering for data management",
        "Amazon EMR with Apache Spark, Amazon EBS Provisioned IOPS for storage, AWS Lambda for workflow triggers, and Amazon Redshift for analysis results",
        "AWS Fargate for containerized processing, Amazon S3 for data storage, Amazon EventBridge for orchestration, and Amazon Athena for results querying"
    ],
    correct: 0,
    explanation: {
        correct: "EC2 Spot Instances with HPC clusters provide massive computational power at reduced cost. FSx for Lustre offers high-performance parallel file system optimized for genomics. Batch orchestrates complex genomic workflows automatically. S3 Glacier provides cost-effective long-term storage for results.",
        whyWrong: {
            1: "ParallelCluster is good but EFS doesn't provide the IOPS needed for genomic workloads compared to FSx Lustre",
            2: "EMR is optimized for big data analytics, not intensive genomic computation, and Lambda has execution time limits",
            3: "Fargate has memory/CPU limitations for genomic workloads, and EventBridge isn't designed for HPC orchestration"
        },
        examStrategy: "Genomic HPC: EC2 Spot + FSx Lustre + Batch for optimal price-performance in computational biology."
    }
},
{
    id: 'sap_272',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A financial trading application experiences latency spikes during market volatility, causing missed trading opportunities. The application uses EC2 instances, RDS Aurora, and handles 50,000 transactions per second during peak trading hours. Network latency between components becomes critical during high-frequency trading.",
    question: "Which optimization provides the LOWEST latency for financial trading?",
    options: [
        "Implement EC2 placement groups for low latency, use Aurora Local Write Forwarding, deploy AWS Local Zones for ultra-low latency, and Amazon ElastiCache Redis with cluster mode",
        "Upgrade to EC2 instances with enhanced networking, use RDS Proxy for connection management, implement Amazon MemoryDB for Redis, and AWS Direct Connect for connectivity",
        "Deploy on AWS Outposts for on-premises latency, use Amazon DynamoDB with DAX, implement AWS PrivateLink for security, and Amazon VPC endpoints for AWS services",
        "Use EC2 instances with SR-IOV networking, implement Aurora Serverless v2, deploy Amazon DocumentDB for flexibility, and AWS Global Accelerator for performance"
    ],
    correct: 0,
    explanation: {
        correct: "Placement groups minimize network latency between instances. Aurora Local Write Forwarding reduces cross-AZ latency. Local Zones place resources closer to trading exchanges. ElastiCache cluster mode provides single-digit millisecond latency for trading data.",
        whyWrong: {
            1: "RDS Proxy adds latency for high-frequency trading, and MemoryDB doesn't provide lower latency than ElastiCache",
            2: "Outposts has higher latency than Local Zones, and DynamoDB consistency may not meet trading requirements",
            3: "Aurora Serverless v2 has scaling delays inappropriate for consistent trading performance"
        },
        examStrategy: "Trading latency: Placement groups + Local Write Forwarding + Local Zones + ElastiCache for ultra-low latency."
    }
},
{
    id: 'sap_273',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A healthcare network with 100+ hospitals needs centralized patient data analytics while maintaining HIPAA compliance, implementing role-based access for different medical specialties, and enabling emergency access during critical situations.",
    question: "Which governance framework ensures HIPAA compliance with operational flexibility?",
    options: [
        "AWS Organizations with healthcare-specific SCPs, AWS IAM Identity Center with attribute-based access control, AWS Config for compliance monitoring, and AWS CloudTrail for audit logging",
        "AWS Control Tower for landing zones, AWS Directory Service for user management, AWS Security Hub for monitoring, and AWS Backup for data protection",
        "AWS Organizations with OU structure, AWS Cognito for user authentication, AWS GuardDuty for threat detection, and AWS KMS for encryption",
        "AWS Landing Zone for account setup, AWS Resource Access Manager for sharing, AWS Systems Manager for compliance, and AWS Certificate Manager for security"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with healthcare SCPs enforce HIPAA guardrails. Identity Center with ABAC provides fine-grained access based on medical roles and patient relationships. Config continuously monitors compliance. CloudTrail provides HIPAA-required audit trails.",
        whyWrong: {
            1: "Control Tower is good but Directory Service alone doesn't provide the attribute-based access needed for medical specialties",
            2: "Cognito is for customer applications, not enterprise healthcare access control",
            3: "Landing Zone is deprecated, and RAM doesn't provide the access control granularity needed for patient data"
        },
        examStrategy: "Healthcare compliance: Organizations + Identity Center ABAC + Config + CloudTrail for HIPAA-compliant governance."
    }
},
{
    id: 'sap_274',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A telecommunications company needs to migrate a real-time network monitoring system that processes 500TB of network data daily, maintains sub-second alerting for network outages, integrates with legacy OSS/BSS systems, and supports 5G network rollout analytics.",
    question: "Which modernization strategy optimizes for real-time network operations?",
    options: [
        "Use Amazon Kinesis Data Firehose for data ingestion, Amazon EMR for batch processing, Amazon Redshift for analytics, and Amazon SNS for notifications",
        "Deploy Amazon EventBridge for event routing, AWS Batch for data processing, Amazon Aurora for operational data, and AWS Step Functions for workflow orchestration",
        "Implement Amazon SQS for message handling, Amazon EC2 for processing, Amazon RDS for data storage, and Amazon CloudWatch for monitoring",
        "Implement Amazon MSK for network event streaming, Amazon Kinesis Data Analytics for real-time processing, Amazon TimeStream for time-series data, and AWS Lambda for alerting workflows"
    ],
    correct: 3,
    explanation: {
        correct: "MSK handles massive network event streams with low latency. Kinesis Data Analytics provides real-time anomaly detection for outages. TimeStream is purpose-built for network telemetry time-series data. Lambda enables sub-second alerting for network incidents.",
        whyWrong: {
            0: "Firehose and EMR are for batch processing, not real-time sub-second alerting requirements",
            1: "EventBridge and Batch add latency inappropriate for real-time network monitoring",
            2: "SQS and RDS don't provide the real-time capabilities needed for network operations"
        },
        examStrategy: "Telecom modernization: MSK + Kinesis Analytics + TimeStream + Lambda for real-time network monitoring."
    }
},
{
    id: 'sap_275',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A food delivery platform needs to implement dynamic pricing based on real-time demand, driver availability, weather conditions, and local events. The system must update prices every 30 seconds for 200+ cities and handle 1 million price calculations per minute.",
    question: "Which architecture provides OPTIMAL real-time pricing at scale?",
    options: [
        "Amazon Kinesis Data Streams for demand signals, AWS Lambda for pricing algorithms, Amazon DynamoDB for price storage, and Amazon API Gateway for price delivery",
        "Amazon MSK for event streaming, Amazon ECS for pricing services, Amazon ElastiCache for price caching, and AWS App Runner for API hosting",
        "Amazon EventBridge for data routing, AWS Batch for price calculation, Amazon RDS Aurora for price history, and Amazon CloudFront for price distribution",
        "Amazon SQS for message queuing, Amazon EMR for computation, Amazon Redshift for analytics, and AWS Global Accelerator for performance"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis Data Streams ingests real-time demand and driver data efficiently. Lambda scales automatically for 1M calculations per minute. DynamoDB provides single-digit millisecond price lookups. API Gateway delivers prices with caching and global distribution.",
        whyWrong: {
            1: "MSK adds complexity for simple pricing workflows, and App Runner doesn't provide the caching needed for price delivery",
            2: "EventBridge and Batch add latency for 30-second pricing updates, and CloudFront doesn't cache dynamic pricing effectively",
            3: "SQS adds latency, EMR is for big data analytics not real-time pricing, and Redshift is for historical analysis"
        },
        examStrategy: "Dynamic pricing: Kinesis + Lambda + DynamoDB + API Gateway for scalable real-time price calculations."
    }
},
{
    id: 'sap_276',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An IoT platform ingesting sensor data from 1 million devices experiences data pipeline failures during traffic spikes, resulting in data loss and delayed analytics. Current architecture uses EC2 instances, MySQL RDS, and batch processing jobs that can't keep up with peak loads.",
    question: "Which optimization provides the MOST resilient IoT data processing?",
    options: [
        "Implement Amazon Kinesis Data Firehose for reliable ingestion, migrate to Amazon DynamoDB for device data, use AWS Glue for ETL processing, and Amazon S3 for data lake storage",
        "Deploy Amazon MSK for event streaming, migrate to Amazon TimeStream for sensor data, implement AWS Lambda for real-time processing, and Amazon Redshift for analytics",
        "Use Amazon EventBridge for device events, implement Amazon DocumentDB for flexible storage, deploy AWS Batch for processing, and Amazon Athena for ad-hoc queries",
        "Implement Amazon SQS for message buffering, use Amazon RDS Aurora for scaling, deploy Amazon EMR for big data processing, and Amazon QuickSight for visualization"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis Data Firehose provides automatic scaling, retry logic, and error handling for IoT ingestion. DynamoDB scales automatically with device growth. Glue handles ETL with automatic scaling. S3 provides unlimited storage for IoT data lakes with multiple access patterns.",
        whyWrong: {
            1: "MSK requires more operational overhead, and Lambda has concurrency limits for 1M devices",
            2: "EventBridge has throughput limitations for high-volume IoT data, and DocumentDB doesn't optimize for time-series IoT data",
            3: "SQS still requires processing capacity management, and Aurora doesn't solve the fundamental scaling challenge"
        },
        examStrategy: "IoT resilience: Kinesis Firehose + DynamoDB + Glue + S3 for reliable, scalable IoT data processing."
    }
},
{
    id: 'sap_277',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A government agency managing classified data across 30 AWS accounts needs to implement strict security controls, maintain air-gapped environments for different classification levels, enable controlled collaboration between agencies, and ensure compliance with FedRAMP High requirements.",
    question: "Which architecture ensures MAXIMUM security for classified government workloads?",
    options: [
        "Use AWS Organizations with strict SCPs, implement AWS Config for compliance, deploy AWS PrivateLink for secure communication, and AWS Certificate Manager for encryption",
        "Implement AWS Control Tower in commercial regions, use AWS Directory Service for authentication, deploy AWS GuardDuty for threat detection, and AWS Backup for data protection",
        "Deploy separate AWS Organizations per classification, use AWS Transit Gateway for connectivity, implement AWS Systems Manager for patch management, and AWS Trusted Advisor for optimization",
        "Deploy AWS GovCloud with isolated VPCs per classification level, implement AWS Security Hub for monitoring, use AWS KMS with FIPS endpoints, and AWS CloudTrail with log file validation"
    ],
    correct: 3,
    explanation: {
        correct: "GovCloud provides FedRAMP High-compliant infrastructure. Isolated VPCs ensure air-gapped environments. Security Hub provides continuous monitoring. KMS with FIPS ensures cryptographic compliance. CloudTrail with validation provides tamper-evident audit logs.",
        whyWrong: {
            0: "Commercial AWS regions don't meet FedRAMP High requirements for classified data",
            1: "Control Tower in commercial regions doesn't provide the isolation needed for classified workloads",
            2: "Transit Gateway connectivity between classifications could violate air-gap requirements"
        },
        examStrategy: "Classified data: AWS GovCloud + isolated VPCs + FIPS KMS + validated CloudTrail for FedRAMP compliance."
    }
},
{
    id: 'sap_278',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A university needs to migrate their student information system from legacy AS/400 servers to AWS. The system handles enrollment, grades, financial aid, and integrates with multiple campus systems. The migration must preserve data integrity and maintain availability during the academic year.",
    question: "Which migration strategy provides OPTIMAL modernization with minimal disruption?",
    options: [
        "Use AWS Mainframe Modernization for AS/400 migration, implement AWS Database Migration Service for data conversion, AWS App2Container for application modernization, and AWS Systems Manager for operational management",
        "Implement VM Import/Export for server migration, use AWS DataSync for file transfer, deploy Amazon RDS for data modernization, and AWS CloudFormation for infrastructure automation",
        "Deploy AWS Application Migration Service for lift-and-shift, implement AWS Schema Conversion Tool for database migration, use AWS Elastic Beanstalk for application hosting, and AWS CodePipeline for CI/CD",
        "Use AWS Snow family for bulk data transfer, implement AWS Direct Connect for hybrid connectivity, deploy Amazon EC2 for application hosting, and AWS Backup for data protection"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization specifically handles AS/400 legacy systems with minimal code changes. DMS converts legacy database formats to modern AWS databases. App2Container modernizes applications while preserving functionality. Systems Manager provides centralized operational management.",
        whyWrong: {
            1: "VM Import doesn't handle AS/400 systems, and basic RDS migration doesn't address legacy data formats",
            2: "Application Migration Service doesn't support AS/400 architecture, and SCT requires manual intervention",
            3: "Snow family requires downtime, and standard EC2 hosting doesn't address AS/400 compatibility"
        },
        examStrategy: "AS/400 migration: Mainframe Modernization + DMS + App2Container for specialized legacy system modernization."
    }
},
{
    id: 'sap_279',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart city initiative monitors traffic, air quality, noise levels, and energy consumption across 10,000 sensors. The system must detect anomalies in real-time, predict traffic congestion, optimize energy distribution, and provide public dashboards with sub-second updates.",
    question: "Which architecture provides COMPREHENSIVE smart city analytics at scale?",
    options: [
        "AWS IoT Core for device connectivity, Amazon MSK for data streaming, Amazon EMR for analytics processing, and Amazon QuickSight for visualization",
        "Amazon EventBridge for sensor events, AWS Lambda for data processing, Amazon DynamoDB for storage, and Amazon CloudFront for dashboard delivery",
        "AWS IoT Device Management for sensors, Amazon Kinesis Data Firehose for ingestion, AWS Batch for analytics, and Amazon S3 for data storage",
        "Amazon Kinesis Data Streams for sensor ingestion, Amazon Kinesis Data Analytics for anomaly detection, Amazon SageMaker for predictive models, and AWS IoT Core for device management"
    ],
    correct: 3,
    explanation: {
        correct: "Kinesis Data Streams handles 10,000 sensors efficiently with real-time processing. Kinesis Analytics provides built-in anomaly detection and real-time analytics. SageMaker enables sophisticated predictive models for traffic and energy optimization. IoT Core provides secure, scalable device management.",
        whyWrong: {
            0: "MSK adds complexity, EMR is for batch processing not real-time anomalies, and QuickSight doesn't provide sub-second updates",
            1: "EventBridge and Lambda have throughput limitations for 10,000 sensors, and CloudFront doesn't enable real-time dashboard updates",
            2: "Firehose is for batch delivery, Batch processing doesn't meet real-time requirements, and S3 doesn't enable real-time queries"
        },
        examStrategy: "Smart city IoT: Kinesis ecosystem + SageMaker + IoT Core for comprehensive real-time city analytics."
    }
},
{
    id: 'sap_280',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A content management system experiences poor performance when generating reports from 5TB of document metadata. The current MySQL database struggles with complex analytical queries, backup windows cause downtime, and storage costs increase 30% annually.",
    question: "Which optimization strategy provides the BEST analytical performance and cost efficiency?",
    options: [
        "Migrate analytical workloads to Amazon Redshift, implement Amazon S3 for document storage, use AWS Glue for ETL processing, and Amazon QuickSight for reporting",
        "Upgrade to Amazon Aurora with read replicas, implement Amazon ElastiCache for query acceleration, use AWS Lambda for report generation, and Amazon S3 for archival",
        "Migrate to Amazon DynamoDB with global secondary indexes, implement Amazon Kinesis for real-time updates, use AWS Step Functions for workflow orchestration, and Amazon Athena for ad-hoc queries",
        "Deploy Amazon RDS PostgreSQL with enhanced monitoring, implement Amazon EFS for shared storage, use Amazon EC2 for report processing, and Amazon CloudWatch for optimization insights"
    ],
    correct: 0,
    explanation: {
        correct: "Redshift is purpose-built for analytical workloads and complex reporting queries. S3 provides cost-effective document storage with multiple tiers. Glue automates ETL for data preparation. QuickSight integrates natively with Redshift for fast, interactive reporting.",
        whyWrong: {
            1: "Aurora with read replicas still struggles with complex analytical queries compared to Redshift",
            2: "DynamoDB isn't optimized for complex analytical queries and reporting workloads",
            3: "PostgreSQL on RDS doesn't provide the analytical performance advantages of a purpose-built data warehouse"
        },
        examStrategy: "Analytical optimization: Redshift + S3 + Glue + QuickSight for purpose-built analytical architecture."
    }
},
{
    id: 'sap_281',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A retail chain with 2,000 stores needs centralized inventory management, real-time sales analytics, PCI compliance for payment processing, and the ability to operate stores independently during network outages.",
    question: "Which hybrid architecture provides OPTIMAL resilience and compliance?",
    options: [
        "Deploy AWS Outposts in stores for local processing, use AWS Storage Gateway for data synchronization, implement AWS Config for PCI compliance, and Amazon Kinesis for real-time analytics",
        "Implement AWS Snow Edge devices for edge computing, use AWS DataSync for store-to-cloud replication, deploy AWS Security Hub for compliance monitoring, and Amazon Redshift for analytics",
        "Use AWS IoT Greengrass for local processing, implement AWS Database Migration Service for data sync, deploy AWS WAF for PCI protection, and Amazon EMR for analytics",
        "Deploy AWS Local Zones near store clusters, use AWS Direct Connect for connectivity, implement AWS CloudTrail for compliance, and Amazon QuickSight for business intelligence"
    ],
    correct: 0,
    explanation: {
        correct: "Outposts provides local AWS infrastructure for independent store operations during outages. Storage Gateway enables seamless data synchronization when connectivity resumes. Config continuously monitors PCI compliance across all locations. Kinesis enables real-time analytics from store data streams.",
        whyWrong: {
            1: "Snow Edge devices are for data transfer, not ongoing operations, and DataSync doesn't provide real-time capabilities",
            2: "IoT Greengrass is for IoT devices not retail systems, and DMS doesn't handle bi-directional sync needed for store independence",
            3: "Local Zones require connectivity and don't provide independence during network outages"
        },
        examStrategy: "Retail resilience: Outposts + Storage Gateway + Config + Kinesis for autonomous store operations with compliance."
    }
},
{
    id: 'sap_282',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A transportation company needs to modernize their route optimization system that currently runs on supercomputers, processes 100 million route combinations, requires specialized mathematical libraries, and must complete calculations within 2-hour windows for next-day logistics.",
    question: "Which modernization approach provides OPTIMAL computational performance for route optimization?",
    options: [
        "Deploy Amazon ECS with GPU instances, implement Amazon EFS for data sharing, use AWS Step Functions for orchestration, and Amazon S3 for result storage",
        "Implement AWS Lambda with increased memory allocation, use Amazon DynamoDB for data storage, deploy Amazon SQS for job queuing, and Amazon EventBridge for coordination",
        "Migrate to Amazon EMR with Spark clusters, implement Amazon S3 for data lake storage, use AWS Glue for data processing, and Amazon Redshift for analytics",
        "Migrate to AWS ParallelCluster with HPC-optimized instances, use Amazon FSx for Lustre for shared storage, implement AWS Batch for job scheduling, and Amazon CloudWatch for monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "ParallelCluster provides HPC infrastructure optimized for mathematical computations. HPC-optimized instances deliver supercomputer-class performance. FSx Lustre provides high-performance parallel file system for computation workloads. Batch orchestrates complex optimization jobs efficiently.",
        whyWrong: {
            0: "ECS doesn't provide HPC optimizations, and Step Functions aren't designed for mathematical computation orchestration",
            1: "Lambda has execution time limits inappropriate for 2-hour optimization windows",
            2: "EMR is optimized for big data analytics, not intensive mathematical route optimization algorithms"
        },
        examStrategy: "HPC modernization: ParallelCluster + HPC instances + FSx Lustre + Batch for computational workloads."
    }
},
{
    id: 'sap_283',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A telemedicine platform connects patients with doctors globally, requiring secure video calls, real-time vital sign monitoring from IoT devices, prescription management, and integration with hospital systems across different countries with varying privacy regulations.",
    question: "Which architecture ensures GLOBAL compliance while maintaining optimal patient experience?",
    options: [
        "Amazon Chime SDK for video calls, AWS IoT Core for device management, Amazon QLDB for prescription integrity, and AWS PrivateLink for hospital integrations",
        "Amazon Connect for patient communication, Amazon Kinesis for device data, Amazon DocumentDB for medical records, and AWS VPN for secure connectivity",
        "AWS WebRTC for video streaming, Amazon EventBridge for device events, Amazon RDS for data storage, and AWS Direct Connect for hospital connections",
        "Amazon API Gateway for communication, Amazon MSK for data streaming, Amazon Aurora for medical data, and AWS Transit Gateway for network connectivity"
    ],
    correct: 0,
    explanation: {
        correct: "Chime SDK provides HIPAA-compliant video with global reach. IoT Core handles device connectivity with security. QLDB ensures prescription data integrity and audit trails for regulatory compliance. PrivateLink provides secure hospital integrations without internet exposure.",
        whyWrong: {
            1: "Connect is for contact centers not patient-doctor video calls, and DocumentDB doesn't provide healthcare-specific compliance features",
            2: "WebRTC requires custom implementation and doesn't provide compliance features needed for healthcare",
            3: "API Gateway alone doesn't provide video capabilities, and MSK adds complexity for simple device data"
        },
        examStrategy: "Telemedicine compliance: Chime SDK + IoT Core + QLDB + PrivateLink for secure, compliant healthcare platform."
    }
},
{
    id: 'sap_284',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial analytics platform processes market data from 100+ exchanges worldwide, experiences 10x traffic spikes during market volatility, requires sub-50ms query response times, and maintains 10 years of historical data for regulatory compliance.",
    question: "Which optimization strategy provides OPTIMAL performance during market volatility?",
    options: [
        "Deploy Amazon MemoryDB for Redis for persistence, use Amazon Redshift with Concurrency Scaling for analytics, implement Amazon MSK for data ingestion, and Amazon EMR for historical analysis",
        "Use Amazon DynamoDB with DAX for low latency, implement Amazon Athena for historical queries, deploy Amazon Kinesis Data Analytics for real-time processing, and AWS Step Functions for workflow coordination",
        "Implement Amazon Aurora with read replicas, use Amazon CloudFront for global distribution, deploy Amazon EventBridge for event routing, and AWS Batch for batch processing",
        "Implement Amazon ElastiCache Redis with cluster mode for hot data, Amazon S3 with Intelligent-Tiering for historical data, Amazon Kinesis Data Streams for market feeds, and AWS Lambda for alert processing"
    ],
    correct: 3,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond access to hot market data with automatic scaling. S3 Intelligent-Tiering optimizes costs for 10 years of historical data. Kinesis Data Streams handles high-volume market feeds efficiently. Lambda scales automatically for volatility-driven alert spikes.",
        whyWrong: {
            0: "MemoryDB has higher latency than ElastiCache, and MSK adds complexity for market data feeds",
            1: "DynamoDB DAX can't achieve sub-50ms for complex analytical queries, and Athena has higher latency for real-time requirements",
            2: "Aurora read replicas can't match ElastiCache latency, and EventBridge adds latency for high-frequency market data"
        },
        examStrategy: "Market data optimization: ElastiCache cluster + S3 Intelligent-Tiering + Kinesis + Lambda for volatility handling."
    }
},
{
    id: 'sap_285',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global law firm with 100+ offices needs to maintain attorney-client privilege across jurisdictions, implement document retention policies varying by country, enable secure collaboration on cases, and ensure compliance with legal discovery requirements.",
    question: "Which governance framework ensures COMPREHENSIVE legal compliance and security?",
    options: [
        "AWS Control Tower for office management, AWS Config for compliance rules, AWS PrivateLink for secure communication, and Amazon S3 with object lock for retention",
        "AWS Directory Service for user management, AWS Security Hub for monitoring, AWS Secrets Manager for credential protection, and AWS Backup for data preservation",
        "AWS Organizations with legal SCPs, AWS Identity Center for access control, AWS GuardDuty for threat detection, and Amazon Glacier for long-term retention",
        "AWS Organizations with jurisdiction-specific OUs, AWS Lake Formation for document governance, AWS KMS with customer-managed keys per jurisdiction, and AWS CloudTrail with legal hold capabilities"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure data residency for attorney-client privilege. Lake Formation provides fine-grained document access controls and governance. Customer-managed KMS keys per jurisdiction ensure encryption sovereignty. CloudTrail with legal hold provides tamper-evident audit trails for discovery.",
        whyWrong: {
            0: "Control Tower doesn't provide jurisdiction-specific governance needed for legal compliance",
            1: "Directory Service alone doesn't provide the document-level governance needed for legal work",
            2: "Organizations with generic SCPs don't address jurisdiction-specific legal requirements"
        },
        examStrategy: "Legal compliance: Organizations jurisdictional OUs + Lake Formation + jurisdiction-specific KMS + CloudTrail legal hold."
    }
},
{
    id: 'sap_286',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A research institution needs to migrate their computational fluid dynamics applications from on-premises HPC clusters to AWS. The applications require specialized hardware, tight coupling between nodes, and access to 500TB of simulation data.",
    question: "Which migration strategy provides OPTIMAL HPC performance in the cloud?",
    options: [
        "Implement Amazon ECS with GPU instances, use Amazon EFS for shared storage, deploy AWS Batch for job management, and AWS Storage Gateway for data access",
        "Use Amazon EMR with custom AMIs, implement Amazon S3 for data storage, deploy AWS Glue for data processing, and AWS Lambda for job triggers",
        "Deploy Amazon EC2 Spot Fleet for cost optimization, use Amazon EBS for storage, implement AWS Step Functions for orchestration, and AWS Direct Connect for data transfer",
        "Deploy AWS ParallelCluster with cluster placement groups, use Amazon FSx for Lustre for high-performance storage, implement Elastic Fabric Adapter for low-latency networking, and AWS DataSync for data migration"
    ],
    correct: 3,
    explanation: {
        correct: "ParallelCluster provides HPC-optimized infrastructure with automatic scaling. Cluster placement groups ensure low-latency, high-bandwidth networking for tight coupling. FSx Lustre delivers parallel file system performance for computational workloads. EFA provides HPC-grade networking. DataSync efficiently migrates 500TB.",
        whyWrong: {
            0: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for computational workloads",
            1: "EMR is for big data analytics, not HPC computational fluid dynamics applications",
            2: "Spot Fleet doesn't guarantee placement for tightly coupled applications, EBS doesn't provide parallel storage performance"
        },
        examStrategy: "HPC migration: ParallelCluster + placement groups + FSx Lustre + EFA + DataSync for optimal performance."
    }
},
{
    id: 'sap_287',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous vehicle company develops a platform processing sensor data from 10,000 test vehicles, performing real-time object detection, updating ML models continuously, sharing insights with regulatory agencies, and ensuring vehicle safety systems remain operational.",
    question: "Which architecture provides OPTIMAL autonomous vehicle data processing and safety?",
    options: [
        "AWS IoT Greengrass for edge processing, Amazon Kinesis Data Streams for vehicle telemetry, Amazon SageMaker for ML model training, and AWS IoT Device Defender for vehicle security",
        "Amazon EC2 instances in vehicles, Amazon MSK for data streaming, Amazon EMR for ML processing, and AWS GuardDuty for security monitoring",
        "AWS Snow Edge devices for mobile processing, Amazon EventBridge for data routing, AWS Batch for model training, and AWS Config for compliance monitoring",
        "AWS Lambda@Edge for distributed processing, Amazon Kinesis Data Firehose for data collection, Amazon Redshift for analytics, and AWS Security Hub for monitoring"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Greengrass enables real-time edge processing for safety-critical decisions. Kinesis Data Streams handles high-volume vehicle telemetry efficiently. SageMaker provides continuous ML model training and deployment. IoT Device Defender ensures vehicle security and fleet management.",
        whyWrong: {
            1: "EC2 instances in vehicles lack edge computing optimizations and cellular connectivity management",
            2: "Snow Edge devices are for data transfer, not ongoing real-time processing in moving vehicles",
            3: "Lambda@Edge doesn't provide the edge computing capabilities needed for vehicle processing"
        },
        examStrategy: "Autonomous vehicles: IoT Greengrass + Kinesis + SageMaker + IoT Device Defender for edge-to-cloud processing."
    }
},
{
    id: 'sap_288',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A video streaming service experiences buffering during peak viewing hours, has 50TB of content requiring global distribution, needs to optimize costs for infrequently accessed content, and must deliver 4K streams with minimal latency worldwide.",
    question: "Which optimization strategy provides the BEST global streaming performance and cost efficiency?",
    options: [
        "Deploy AWS Global Accelerator for performance, use Amazon EFS for content storage, implement AWS Batch for video processing, and Application Load Balancer for distribution",
        "Use Amazon S3 Transfer Acceleration, implement Amazon ElastiCache for content caching, deploy AWS Lambda for stream processing, and AWS Direct Connect for content delivery",
        "Implement AWS PrivateLink for content delivery, use Amazon Glacier for content archival, deploy Amazon EMR for analytics, and Amazon API Gateway for stream management",
        "Implement Amazon CloudFront with origin shield, use S3 Intelligent-Tiering for content storage, AWS Elemental MediaPackage for stream optimization, and Amazon Route 53 for global load balancing"
    ],
    correct: 3,
    explanation: {
        correct: "CloudFront with origin shield provides global edge caching with optimized origin protection. S3 Intelligent-Tiering automatically optimizes storage costs for varying access patterns. MediaPackage optimizes video delivery with adaptive bitrate streaming. Route 53 enables intelligent global traffic routing.",
        whyWrong: {
            0: "Global Accelerator doesn't provide content caching, EFS doesn't optimize for video content delivery",
            1: "Transfer Acceleration doesn't provide edge caching, Lambda has limitations for video stream processing",
            2: "PrivateLink doesn't provide global content delivery, Glacier has retrieval delays inappropriate for streaming"
        },
        examStrategy: "Video streaming: CloudFront + origin shield + S3 Intelligent-Tiering + MediaPackage for optimal global delivery."
    }
},
{
    id: 'sap_289',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A energy utility company with 200+ power plants needs centralized monitoring, predictive maintenance analytics, regulatory reporting for environmental compliance, and emergency response coordination during outages affecting millions of customers.",
    question: "Which governance architecture provides COMPREHENSIVE operational resilience and compliance?",
    options: [
        "AWS Organizations with utility-specific SCPs, AWS IoT Core for plant monitoring, Amazon Kinesis Data Analytics for predictive maintenance, and AWS Config for regulatory compliance",
        "AWS Control Tower for plant management, Amazon MSK for data streaming, Amazon EMR for analytics, and AWS CloudTrail for audit logging",
        "AWS Directory Service for user management, Amazon EventBridge for event coordination, AWS Batch for maintenance processing, and AWS Security Hub for monitoring",
        "AWS Organizations with geographic OUs, Amazon Kinesis Data Firehose for data collection, AWS Glue for data processing, and Amazon QuickSight for reporting"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with utility SCPs enforce safety and compliance guardrails. IoT Core manages plant sensors and equipment connectivity. Kinesis Data Analytics provides real-time predictive maintenance. Config continuously monitors regulatory compliance across all facilities.",
        whyWrong: {
            1: "Control Tower doesn't provide utility-specific governance, MSK adds complexity for simple monitoring workflows",
            2: "Directory Service doesn't provide operational governance, EventBridge alone doesn't handle emergency coordination complexity",
            3: "Geographic OUs don't address utility-specific operational requirements, Firehose delays real-time predictive maintenance"
        },
        examStrategy: "Utility governance: Organizations + utility SCPs + IoT Core + Kinesis Analytics + Config for operational compliance."
    }
},
{
    id: 'sap_290',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A pharmaceutical company needs to migrate their drug discovery platform that performs molecular simulations on supercomputers, requires specialized scientific software, processes petabytes of research data, and must maintain FDA validation for drug approval processes.",
    question: "Which modernization approach ensures OPTIMAL computational performance while maintaining FDA compliance?",
    options: [
        "Migrate to Amazon EMR with Spark clusters, use Amazon S3 for data lake storage, implement AWS Glue for data processing, and AWS CloudTrail for audit compliance",
        "Deploy Amazon ECS with GPU instances, implement Amazon EFS for shared storage, use AWS Step Functions for orchestration, and AWS Security Hub for compliance monitoring",
        "Implement AWS Outposts for on-premises compliance, use Amazon EBS for data storage, deploy AWS Lambda for processing triggers, and AWS Systems Manager for validation",
        "Deploy AWS ParallelCluster with HPC instances, implement Amazon FSx for Lustre for research data, use AWS Batch for computational workflows, and AWS Config for FDA compliance validation"
    ],
    correct: 3,
    explanation: {
        correct: "ParallelCluster provides validated HPC infrastructure for pharmaceutical computing. HPC instances deliver supercomputer performance for molecular simulations. FSx Lustre handles petabyte-scale research data with parallel access. Batch orchestrates complex scientific workflows. Config ensures continuous FDA compliance validation.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive molecular simulation computational requirements",
            1: "ECS doesn't provide HPC optimizations needed for scientific computing workloads",
            2: "Outposts doesn't provide supercomputer performance, Lambda has execution limits inappropriate for molecular simulations"
        },
        examStrategy: "Pharmaceutical HPC: ParallelCluster + HPC instances + FSx Lustre + Batch + Config for FDA-compliant drug discovery."
    }
},
{
    id: 'sap_291',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A digital banking platform serves 10 million customers globally, requires real-time fraud detection, supports multiple currencies and payment methods, ensures PCI DSS compliance, and must maintain 99.99% availability for financial transactions.",
    question: "Which architecture provides OPTIMAL security and availability for digital banking?",
    options: [
        "Amazon API Gateway for banking APIs, AWS Lambda for transaction processing, Amazon DynamoDB Global Tables for customer data, and Amazon Kinesis Data Analytics for fraud detection",
        "AWS App Runner for banking services, Amazon RDS Aurora for transaction data, Amazon ElastiCache for session management, and Amazon GuardDuty for security monitoring",
        "Amazon ECS Fargate for microservices, Amazon DocumentDB for customer profiles, Amazon MSK for event streaming, and AWS Security Hub for compliance",
        "AWS Elastic Beanstalk for application hosting, Amazon Redshift for transaction analytics, Amazon SQS for message queuing, and AWS Config for PCI compliance"
    ],
    correct: 0,
    explanation: {
        correct: "API Gateway provides PCI-compliant API management with throttling and security. Lambda scales automatically for transaction spikes with built-in availability. DynamoDB Global Tables ensures 99.99% availability with multi-region replication. Kinesis Analytics provides real-time fraud detection at scale.",
        whyWrong: {
            1: "App Runner doesn't provide the API management and security features needed for banking",
            2: "ECS adds operational complexity, DocumentDB doesn't provide the consistency needed for financial data",
            3: "Elastic Beanstalk doesn't provide the security and compliance features required for banking"
        },
        examStrategy: "Digital banking: API Gateway + Lambda + DynamoDB Global Tables + Kinesis Analytics for secure, available banking."
    }
},
{
    id: 'sap_292',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A supply chain management system tracking 100,000+ shipments globally experiences performance degradation as data volume reaches 20TB. Queries for shipment tracking take 30+ seconds, real-time inventory updates lag, and warehouse operations are impacted by system slowdowns.",
    question: "Which optimization strategy provides the BEST supply chain performance improvement?",
    options: [
        "Migrate to Amazon Aurora with read replicas, implement Amazon S3 for data archival, use AWS Lambda for inventory processing, and Amazon CloudFront for global access",
        "Deploy Amazon DocumentDB for flexible schemas, implement Amazon EventBridge for event coordination, use AWS Step Functions for workflow orchestration, and Amazon Athena for ad-hoc queries",
        "Upgrade to Amazon RDS with enhanced monitoring, implement Amazon EFS for shared storage, use Amazon SQS for message handling, and AWS Batch for batch processing",
        "Implement Amazon DynamoDB with Global Secondary Indexes for shipment tracking, Amazon Kinesis Data Streams for real-time updates, Amazon ElastiCache for frequently accessed data, and Amazon Redshift for supply chain analytics"
    ],
    correct: 3,
    explanation: {
        correct: "DynamoDB with GSI provides single-digit millisecond shipment lookups at global scale. Kinesis Data Streams enables real-time inventory updates without lag. ElastiCache accelerates frequently accessed supply chain data. Redshift handles complex supply chain analytics efficiently.",
        whyWrong: {
            0: "Aurora read replicas still can't match DynamoDB performance for high-volume tracking queries",
            1: "DocumentDB doesn't provide the query performance needed for real-time supply chain operations",
            2: "RDS enhancement doesn't solve the fundamental performance limitations for 100,000+ shipments"
        },
        examStrategy: "Supply chain optimization: DynamoDB + GSI + Kinesis + ElastiCache + Redshift for optimal tracking performance."
    }
},
{
    id: 'sap_293',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A aerospace contractor working on classified projects needs to maintain ITAR compliance, implement export control restrictions, enable secure collaboration with international partners, and ensure data sovereignty for different government contracts.",
    question: "Which governance framework ensures COMPREHENSIVE ITAR compliance and export control?",
    options: [
        "Use AWS Organizations with geography-based OUs, implement AWS Config for ITAR monitoring, deploy AWS Direct Connect for partner access, and AWS CloudTrail for export audit trails",
        "Deploy AWS Control Tower in isolated regions, use AWS WAF for access control, implement AWS Certificate Manager for partner security, and AWS GuardDuty for threat detection",
        "Implement AWS Security Hub for compliance monitoring, use AWS Secrets Manager for key management, deploy AWS VPN for partner connectivity, and AWS Systems Manager for operational control",
        "Deploy AWS GovCloud with ITAR-compliant regions, implement AWS Organizations with export control SCPs, use AWS KMS with government-controlled keys, and AWS PrivateLink for partner collaboration"
    ],
    correct: 3,
    explanation: {
        correct: "GovCloud with ITAR regions provides validated infrastructure for controlled technology. Organizations with export control SCPs enforce ITAR restrictions automatically. Government-controlled KMS keys ensure encryption sovereignty. PrivateLink enables secure partner collaboration without internet exposure.",
        whyWrong: {
            0: "Commercial AWS regions don't provide ITAR compliance validation needed for aerospace contracts",
            1: "Control Tower in standard regions doesn't meet ITAR infrastructure requirements",
            2: "Security Hub alone doesn't provide the export control enforcement needed for ITAR compliance"
        },
        examStrategy: "ITAR compliance: GovCloud + export control SCPs + government KMS + PrivateLink for classified aerospace projects."
    }
},
{
    id: 'sap_294',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A weather forecasting agency needs to migrate their numerical weather prediction models from supercomputers to AWS. The models require massive parallel processing, process 100TB of atmospheric data daily, and must deliver forecasts within 3-hour windows for public safety.",
    question: "Which migration strategy provides OPTIMAL meteorological computing performance?",
    options: [
        "Deploy AWS ParallelCluster with HPC-optimized instances, use Amazon FSx for Lustre for atmospheric data, implement AWS Batch for forecast scheduling, and Amazon S3 for forecast distribution",
        "Implement Amazon EMR with Spark clusters, use Amazon Redshift for data warehousing, deploy AWS Glue for data processing, and Amazon QuickSight for visualization",
        "Deploy Amazon ECS with GPU instances, implement Amazon EFS for data sharing, use AWS Step Functions for model orchestration, and Amazon CloudFront for forecast delivery",
        "Use AWS Fargate for containerized models, implement Amazon DynamoDB for forecast data, deploy AWS Lambda for processing triggers, and Amazon API Gateway for forecast APIs"
    ],
    correct: 0,
    explanation: {
        correct: "ParallelCluster provides HPC infrastructure optimized for weather modeling. HPC-optimized instances deliver the massive parallel processing needed for atmospheric simulations. FSx Lustre handles 100TB daily data with parallel file system performance. Batch schedules forecast runs within 3-hour windows. S3 distributes forecasts globally.",
        whyWrong: {
            1: "EMR is for big data analytics, not intensive numerical weather prediction computations",
            2: "ECS doesn't provide HPC optimizations needed for meteorological modeling",
            3: "Fargate has resource limitations inappropriate for massive parallel weather computations"
        },
        examStrategy: "Weather forecasting: ParallelCluster + HPC instances + FSx Lustre + Batch for meteorological computing."
    }
},
{
    id: 'sap_295',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous drone delivery service operates 50,000 drones across 100 cities, requires real-time flight path optimization, weather-based route adjustments, collision avoidance, regulatory compliance reporting, and integration with air traffic control systems.",
    question: "Which architecture provides COMPREHENSIVE drone fleet management and safety?",
    options: [
        "AWS IoT Core for drone connectivity, Amazon Kinesis Data Streams for telemetry, Amazon SageMaker for route optimization, AWS Lambda for real-time decisions, and Amazon Timestream for flight data recording",
        "Amazon EC2 instances for fleet management, Amazon MSK for data streaming, Amazon EMR for analytics, AWS Step Functions for workflow coordination, and Amazon S3 for data storage",
        "AWS IoT Greengrass for edge processing, Amazon EventBridge for event coordination, AWS Batch for route calculations, Amazon DynamoDB for drone data, and Amazon CloudWatch for monitoring",
        "Amazon API Gateway for drone communication, Amazon Kinesis Data Firehose for data collection, Amazon Redshift for analytics, AWS Glue for data processing, and Amazon QuickSight for reporting"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core manages 50,000 drone connections with device security and fleet management. Kinesis Data Streams processes real-time telemetry for flight optimization. SageMaker provides ML-based route optimization and weather integration. Lambda enables real-time collision avoidance decisions. Timestream stores flight data for regulatory compliance.",
        whyWrong: {
            1: "EC2 instances don't provide IoT device management capabilities, MSK adds complexity for drone telemetry",
            2: "IoT Greengrass alone can't coordinate 50,000 drones, Batch adds latency for real-time flight decisions",
            3: "API Gateway doesn't provide IoT device management, Firehose delays real-time flight optimization"
        },
        examStrategy: "Drone fleet: IoT Core + Kinesis + SageMaker + Lambda + Timestream for comprehensive autonomous operations."
    }
},
{
    id: 'sap_296',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A social media platform experiences performance issues during viral content spikes, with image and video processing taking 10+ minutes, user feeds becoming stale, and recommendation algorithms slowing down due to increased user activity.",
    question: "Which optimization strategy provides the BEST viral content handling performance?",
    options: [
        "Implement Amazon Kinesis Data Streams for user activity, AWS Lambda for image processing, Amazon ElastiCache for feed caching, and Amazon SageMaker for real-time recommendations",
        "Deploy Amazon SQS for content queuing, Amazon ECS for media processing, Amazon DynamoDB for user data, and Amazon EMR for recommendation algorithms",
        "Use Amazon EventBridge for activity events, AWS Batch for media processing, Amazon RDS Aurora for content data, and Amazon Redshift for user analytics",
        "Implement Amazon MSK for event streaming, Amazon EC2 Auto Scaling for processing, Amazon DocumentDB for content metadata, and AWS Step Functions for workflow coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis Data Streams handles viral activity spikes efficiently with automatic scaling. Lambda processes images/videos with automatic concurrency scaling. ElastiCache provides sub-millisecond feed access during traffic spikes. SageMaker enables real-time recommendation inference at scale.",
        whyWrong: {
            1: "SQS can become a bottleneck during viral spikes, ECS requires capacity planning for viral loads",
            2: "EventBridge and Batch add latency inappropriate for viral content processing",
            3: "MSK adds complexity, EC2 Auto Scaling has delays compared to Lambda's instant scaling"
        },
        examStrategy: "Viral content: Kinesis + Lambda + ElastiCache + SageMaker for automatic scaling during content spikes."
    }
},
{
    id: 'sap_297',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A insurance company with 75 offices across multiple states needs to comply with varying state insurance regulations, maintain separate data for different insurance products, enable cross-office collaboration on claims, and implement disaster recovery for business continuity.",
    question: "Which governance architecture provides OPTIMAL regulatory compliance and operational resilience?",
    options: [
        "AWS Control Tower for office management, AWS Security Hub for monitoring, AWS Site Recovery for disaster recovery, and AWS Directory Service for user management",
        "AWS Organizations with product-based OUs, AWS CloudTrail for audit logging, AWS Storage Gateway for hybrid backup, and AWS PrivateLink for office connectivity",
        "AWS Landing Zone for standardization, AWS GuardDuty for threat detection, AWS DataSync for backup replication, and AWS Systems Manager for operational management",
        "AWS Organizations with state-specific OUs, AWS Config for regulatory compliance, AWS Backup with cross-region replication, and AWS Resource Access Manager for controlled collaboration"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with state OUs ensure compliance with varying state regulations. Config monitors regulatory compliance continuously across all offices. AWS Backup with cross-region replication provides automated disaster recovery. RAM enables controlled resource sharing for claims collaboration.",
        whyWrong: {
            0: "Control Tower doesn't address state-specific regulatory requirements, Site Recovery isn't an AWS service",
            1: "Product-based OUs don't address state regulatory compliance requirements",
            2: "Landing Zone is deprecated, and doesn't provide the regulatory governance needed"
        },
        examStrategy: "Insurance compliance: Organizations + state OUs + Config + AWS Backup + RAM for regulatory governance."
    }
},
{
    id: 'sap_298',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A scientific research consortium needs to migrate their particle physics data analysis from on-premises clusters to AWS. The workloads process exabytes of experimental data, require GPUs for machine learning analysis, support international collaboration, and must maintain data provenance for research integrity.",
    question: "Which modernization approach provides OPTIMAL scientific computing collaboration and data integrity?",
    options: [
        "Implement Amazon EMR with GPU support, use Amazon Redshift for data warehousing, deploy AWS Glue for data processing, and Amazon QuickSight for research visualization",
        "Deploy Amazon ECS with GPU instances, implement Amazon EFS for shared storage, use AWS Step Functions for workflow orchestration, and AWS Config for data compliance",
        "Use AWS Batch with GPU instances, implement Amazon FSx for Lustre for high-performance storage, deploy AWS DataSync for international collaboration, and AWS Systems Manager for operational management",
        "Deploy AWS ParallelCluster with GPU instances, implement Amazon S3 with Cross-Region Replication for global data access, use AWS Lake Formation for data governance, and AWS CloudTrail for provenance tracking"
    ],
    correct: 3,
    explanation: {
        correct: "ParallelCluster with GPU instances provides optimal scientific computing performance. S3 with CRR enables global collaboration on exabyte-scale data. Lake Formation provides fine-grained data governance for research datasets. CloudTrail ensures complete data provenance for scientific integrity.",
        whyWrong: {
            0: "EMR is for big data analytics, not intensive particle physics computations",
            1: "ECS doesn't provide HPC optimizations needed for scientific computing",
            2: "Batch alone doesn't provide the HPC cluster capabilities needed for particle physics"
        },
        examStrategy: "Scientific computing: ParallelCluster + GPU + S3 CRR + Lake Formation + CloudTrail for research collaboration."
    }
},
{
    id: 'sap_299',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart agriculture platform monitors 10,000 farms globally using IoT sensors for soil moisture, weather conditions, and crop health. The system must provide real-time irrigation recommendations, predict crop yields, alert farmers to diseases, and optimize resource usage.",
    question: "Which architecture provides OPTIMAL agricultural intelligence and resource optimization?",
    options: [
        "AWS IoT Core for sensor management, Amazon Kinesis Data Analytics for real-time analysis, Amazon SageMaker for crop prediction models, and Amazon SNS for farmer alerts",
        "Amazon EventBridge for sensor events, AWS Lambda for data processing, Amazon DynamoDB for farm data, and Amazon SES for email notifications",
        "AWS IoT Greengrass for edge processing, Amazon MSK for data streaming, Amazon EMR for analytics, and AWS Step Functions for workflow coordination",
        "Amazon API Gateway for sensor communication, Amazon Kinesis Data Firehose for data collection, Amazon Redshift for analytics, and Amazon Connect for farmer communication"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure, scalable management for 10,000 farms' sensors. Kinesis Data Analytics processes real-time sensor data for immediate irrigation decisions. SageMaker builds and deploys crop prediction models. SNS delivers real-time alerts to farmers for disease detection.",
        whyWrong: {
            1: "EventBridge has throughput limitations for high-volume sensor data, SES is for email not real-time alerts",
            2: "IoT Greengrass on every farm adds operational complexity, MSK is overkill for sensor data processing",
            3: "API Gateway doesn't provide IoT device management, Connect is for contact centers not farm alerts"
        },
        examStrategy: "Smart agriculture: IoT Core + Kinesis Analytics + SageMaker + SNS for intelligent farm management."
    }
},
{
    id: 'sap_300',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A enterprise resource planning (ERP) system serving 5,000 concurrent users experiences database deadlocks during month-end closing, has complex reporting requirements taking hours to complete, and needs to maintain real-time inventory accuracy across 200 warehouses globally.",
    question: "Which optimization strategy provides the BEST ERP performance and scalability?",
    options: [
        "Deploy Amazon RDS with Multi-AZ for high availability, implement Amazon Redshift for reporting, use Amazon SQS for inventory updates, and Amazon EC2 Auto Scaling for application scaling",
        "Migrate to Amazon DynamoDB with Global Tables for inventory, implement Amazon EMR for reporting, use Amazon EventBridge for event coordination, and AWS Step Functions for business processes",
        "Upgrade to Amazon RDS PostgreSQL with enhanced monitoring, implement Amazon Athena for reporting, use Amazon Kinesis Data Firehose for data collection, and AWS Batch for batch processing",
        "Migrate to Amazon Aurora with read replicas for reporting, implement Amazon ElastiCache for inventory caching, use Amazon Kinesis Data Streams for real-time inventory updates, and AWS Lambda for lightweight processing"
    ],
    correct: 3,
    explanation: {
        correct: "Aurora read replicas isolate reporting workloads preventing deadlocks during month-end closing. ElastiCache provides real-time inventory lookups across 200 warehouses. Kinesis Data Streams ensures real-time inventory accuracy without database bottlenecks. Lambda handles lightweight ERP processes efficiently.",
        whyWrong: {
            0: "Multi-AZ doesn't solve deadlock issues, SQS adds latency for real-time inventory requirements",
            1: "DynamoDB requires significant ERP redesign, EMR is for big data analytics not operational reporting",
            2: "PostgreSQL enhancement doesn't solve fundamental deadlock issues, Firehose delays real-time inventory updates"
        },
        examStrategy: "ERP optimization: Aurora read replicas + ElastiCache + Kinesis + Lambda for deadlock-free performance."
    }
},	

// SAP-C02 Questions 301-350 (Batch 7) - 50 Questions - PROPERLY RANDOMIZED ANSWERS
// Distribution: 12 A's, 13 B's, 12 C's, 13 D's

// Questions 301-312: correct: 0 (A)
{
    id: 'sap_301',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational corporation operates 150+ manufacturing facilities across 40 countries, each with different environmental regulations, labor laws, and tax requirements. The company needs centralized monitoring, compliance reporting, supply chain optimization, and real-time coordination during global disruptions.",
    question: "Which comprehensive governance architecture ensures OPTIMAL regulatory compliance and operational resilience?",
    options: [
        "AWS Organizations with country-specific OUs, AWS Config for regulatory monitoring, AWS IoT Core for facility sensors, Amazon Kinesis for supply chain data, and AWS Step Functions for disruption response workflows",
        "AWS Control Tower with geographic landing zones, AWS Security Hub for compliance dashboards, Amazon MSK for facility communication, AWS Lambda for regulatory processing, and Amazon EventBridge for crisis coordination",
        "AWS Organizations with regulatory SCPs, AWS Directory Service for user management, Amazon EMR for supply chain analytics, AWS Batch for compliance processing, and Amazon SQS for facility messaging",
        "AWS Landing Zone with facility accounts, AWS GuardDuty for monitoring, Amazon Redshift for reporting, AWS Glue for data processing, and Amazon SNS for alert distribution"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with country OUs ensures compliance with varying national regulations. Config provides continuous regulatory monitoring across all facilities. IoT Core manages facility sensors and equipment. Kinesis processes real-time supply chain data. Step Functions orchestrate complex disruption response workflows automatically.",
        whyWrong: {
            1: "Control Tower doesn't address country-specific regulatory requirements, MSK adds complexity for facility communication",
            2: "Directory Service doesn't provide regulatory governance, EMR is overkill for simple compliance processing",
            3: "Landing Zone is deprecated, Redshift alone doesn't provide real-time compliance monitoring capabilities"
        },
        examStrategy: "Multi-national compliance: Organizations + country OUs + Config + IoT Core + Kinesis for comprehensive governance."
    }
},
{
    id: 'sap_302',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A cryptocurrency exchange handles 500,000 transactions per second with sub-10ms latency requirements, real-time fraud detection, regulatory compliance across multiple jurisdictions, and cold storage for customer funds with multi-signature security.",
    question: "Which architecture provides ULTRA-HIGH performance with maximum security for cryptocurrency trading?",
    options: [
        "Amazon ElastiCache Redis cluster mode for hot wallets, AWS Lambda for transaction processing, Amazon DynamoDB for trade history, and AWS KMS with CloudHSM for cold storage keys",
        "Amazon MemoryDB for transaction state, Amazon Kinesis for trade streams, Amazon Aurora for compliance data, and AWS Secrets Manager for key management",
        "Amazon RDS with read replicas for trading data, Amazon SQS for transaction queuing, AWS Step Functions for compliance workflows, and Amazon S3 with encryption for cold storage",
        "Amazon DocumentDB for flexible schemas, Amazon MSK for high-throughput messaging, AWS Batch for regulatory processing, and AWS Certificate Manager for security"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond performance for 500K TPS with automatic sharding. Lambda scales instantly for transaction spikes. DynamoDB handles high-volume trade history efficiently. KMS with CloudHSM provides FIPS 140-2 Level 3 security for cold storage multi-signature keys.",
        whyWrong: {
            1: "MemoryDB has higher latency than ElastiCache cluster mode for ultra-high frequency trading",
            2: "RDS can't handle 500K TPS, SQS adds latency for real-time trading requirements",
            3: "DocumentDB doesn't provide the performance needed, MSK adds complexity for simple transaction processing"
        },
        examStrategy: "Crypto exchange: ElastiCache cluster + Lambda + DynamoDB + KMS CloudHSM for ultra-high performance trading."
    }
},
{
    id: 'sap_303',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global e-commerce platform experiences 1000% traffic spikes during flash sales, with order processing delays, inventory inconsistencies across regions, payment failures, and customer service overload. The current monolithic application struggles with elastic scaling.",
    question: "Which modernization strategy provides the BEST resilience for extreme traffic events?",
    options: [
        "Implement microservices with Amazon ECS Fargate, Amazon DynamoDB Global Tables for inventory, Amazon SQS FIFO for order processing, AWS Step Functions for payment workflows, and Amazon Connect for customer service automation",
        "Deploy AWS Lambda for order processing, Amazon ElastiCache for session management, Amazon RDS Aurora with read replicas, Amazon API Gateway for rate limiting, and Amazon Pinpoint for customer notifications",
        "Use Amazon EKS with Horizontal Pod Autoscaler, Amazon DocumentDB for order data, Amazon EventBridge for event coordination, AWS Batch for inventory processing, and Amazon Chime for customer support",
        "Implement AWS App Runner for automatic scaling, Amazon Redshift for analytics, Amazon Kinesis for real-time processing, AWS Glue for data transformation, and Amazon QuickSight for dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "ECS Fargate provides automatic scaling for microservices without server management. DynamoDB Global Tables ensure inventory consistency across regions with automatic scaling. SQS FIFO maintains order processing sequence during spikes. Step Functions orchestrate complex payment workflows reliably. Connect provides scalable customer service automation.",
        whyWrong: {
            1: "Lambda has concurrency limits that may not handle 1000% spikes, Aurora read replicas don't solve write bottlenecks",
            2: "EKS adds operational complexity, DocumentDB doesn't optimize for e-commerce inventory patterns",
            3: "App Runner doesn't provide microservices architecture benefits, Redshift is for analytics not real-time order processing"
        },
        examStrategy: "E-commerce scalability: Microservices + Fargate + DynamoDB Global Tables + SQS FIFO + Step Functions for extreme traffic."
    }
},
{
    id: 'sap_304',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A financial institution needs to migrate their core banking system from IBM z/OS mainframes to AWS while maintaining 24/7 availability, ensuring data consistency, meeting regulatory requirements, and supporting real-time transaction processing for 50 million customers.",
    question: "Which migration approach ensures ZERO downtime while maintaining regulatory compliance?",
    options: [
        "AWS Mainframe Modernization with automated refactoring, AWS Database Migration Service with change data capture, AWS Application Migration Service for gradual cutover, and AWS Config for regulatory compliance monitoring",
        "AWS Snow family for bulk data migration, AWS Direct Connect for hybrid connectivity, Amazon EC2 for application hosting, and AWS CloudTrail for audit compliance",
        "AWS Professional Services for assessment, AWS Well-Architected Framework implementation, AWS Trusted Advisor for optimization, and AWS Support for ongoing assistance",
        "VM Import/Export for server migration, AWS DataSync for file transfer, Amazon RDS for database modernization, and AWS Security Hub for compliance management"
    ],
    correct: 0,
    explanation: {
        correct: "Mainframe Modernization provides specialized z/OS migration tools with minimal business disruption. DMS with CDC enables real-time data synchronization during migration. Application Migration Service allows gradual, controlled cutover maintaining 24/7 availability. Config ensures continuous regulatory compliance monitoring.",
        whyWrong: {
            1: "Snow family requires offline transfer causing downtime, EC2 alone doesn't address mainframe compatibility",
            2: "Professional Services provides assessment but not actual migration execution strategies",
            3: "VM Import doesn't handle z/OS mainframe architecture, basic RDS doesn't address mainframe database complexity"
        },
        examStrategy: "Mainframe migration: Mainframe Modernization + DMS CDC + Application Migration Service for zero-downtime banking migration."
    }
},
{
    id: 'sap_305',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A defense contractor manages 80+ classified projects across different security clearance levels, requiring strict access controls, data isolation, audit trails, and integration with government systems while maintaining operational efficiency for unclassified work.",
    question: "Which governance framework provides MAXIMUM security segmentation with operational flexibility?",
    options: [
        "AWS GovCloud with clearance-level OUs, AWS Organizations SCPs for classification controls, AWS IAM Identity Center with attribute-based access, and AWS CloudTrail with tamper-evident logging",
        "AWS Control Tower for project management, AWS Config for security monitoring, AWS PrivateLink for government integration, and AWS Backup for data protection",
        "AWS Organizations with project-based accounts, AWS Security Hub for centralized monitoring, AWS Direct Connect for government connectivity, and AWS KMS for encryption",
        "AWS Landing Zone with security templates, AWS Directory Service for user authentication, AWS GuardDuty for threat detection, and AWS Certificate Manager for secure communications"
    ],
    correct: 0,
    explanation: {
        correct: "GovCloud provides validated infrastructure for classified workloads. Organizations with clearance-level OUs ensure proper data isolation. SCPs enforce classification controls automatically. Identity Center with ABAC provides fine-grained access based on clearance levels. CloudTrail with tamper-evident logging meets audit requirements.",
        whyWrong: {
            1: "Control Tower doesn't provide the security segmentation needed for classified projects",
            2: "Project-based accounts don't address classification-level security requirements",
            3: "Landing Zone is deprecated, Directory Service alone doesn't provide clearance-based access controls"
        },
        examStrategy: "Defense classification: GovCloud + clearance OUs + SCPs + Identity Center ABAC for maximum security segmentation."
    }
},
{
    id: 'sap_306',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A space technology company develops satellite constellation management systems requiring real-time tracking of 10,000+ satellites, ground station coordination across 50+ locations, mission planning optimization, and space debris collision avoidance with sub-second response times.",
    question: "Which architecture provides OPTIMAL space mission management and safety?",
    options: [
        "AWS Ground Station for satellite communication, Amazon Kinesis Data Streams for telemetry, Amazon SageMaker for collision prediction, AWS Lambda for automated responses, and Amazon Timestream for orbital data",
        "Amazon EC2 instances for ground control, Amazon MSK for data streaming, Amazon EMR for mission analytics, AWS Step Functions for mission coordination, and Amazon S3 for data archival",
        "AWS IoT Core for satellite connectivity, Amazon EventBridge for mission events, AWS Batch for trajectory calculations, Amazon DynamoDB for satellite data, and Amazon CloudWatch for monitoring",
        "Amazon API Gateway for ground station APIs, Amazon Kinesis Data Firehose for data collection, Amazon Redshift for mission analytics, AWS Glue for data processing, and Amazon QuickSight for mission dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Ground Station provides direct satellite communication infrastructure. Kinesis Data Streams handles high-volume real-time telemetry from 10,000+ satellites. SageMaker enables sophisticated collision prediction models. Lambda provides sub-second automated collision avoidance responses. Timestream optimizes orbital time-series data storage and analysis.",
        whyWrong: {
            1: "EC2 instances don't provide satellite communication capabilities, MSK adds complexity for telemetry processing",
            2: "IoT Core doesn't handle satellite communication protocols, Batch adds latency for real-time collision avoidance",
            3: "API Gateway doesn't provide satellite communication, Firehose delays real-time collision detection"
        },
        examStrategy: "Space technology: Ground Station + Kinesis + SageMaker + Lambda + Timestream for comprehensive satellite management."
    }
},
{
    id: 'sap_307',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational hotel chain's reservation system experiences booking conflicts during peak travel seasons, slow response times for loyalty program queries, integration issues with third-party travel sites, and difficulty managing dynamic pricing across 5,000+ properties globally.",
    question: "Which optimization provides the BEST global reservation system performance?",
    options: [
        "Implement Amazon DynamoDB Global Tables for reservations, Amazon ElastiCache for loyalty data, Amazon API Gateway for partner integrations, and AWS Lambda for dynamic pricing algorithms",
        "Deploy Amazon Aurora Global Database with read replicas, Amazon RDS Proxy for connection management, Amazon EventBridge for booking events, and Amazon EMR for pricing analytics",
        "Use Amazon DocumentDB for flexible booking data, Amazon Kinesis for real-time updates, AWS Step Functions for booking workflows, and Amazon Redshift for loyalty analytics",
        "Implement Amazon RDS Multi-AZ for availability, Amazon SQS for booking queues, AWS Batch for pricing calculations, and Amazon CloudFront for global distribution"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB Global Tables provide conflict-free booking management across regions with automatic scaling. ElastiCache delivers fast loyalty program queries. API Gateway enables scalable partner integrations with rate limiting. Lambda scales automatically for dynamic pricing calculations across 5,000+ properties.",
        whyWrong: {
            1: "Aurora Global Database still has write conflicts during peak booking periods",
            2: "DocumentDB doesn't optimize for reservation conflict resolution, Step Functions add latency for booking workflows",
            3: "RDS Multi-AZ doesn't solve global scaling issues, SQS adds latency for real-time bookings"
        },
        examStrategy: "Hotel reservations: DynamoDB Global Tables + ElastiCache + API Gateway + Lambda for conflict-free global bookings."
    }
},
{
    id: 'sap_308',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A pharmaceutical research company needs to migrate their drug discovery supercomputing clusters processing molecular dynamics simulations, genetic sequencing analysis, and clinical trial data while maintaining FDA validation, ensuring data integrity, and enabling global research collaboration.",
    question: "Which modernization strategy ensures OPTIMAL scientific computing performance with regulatory compliance?",
    options: [
        "Deploy AWS ParallelCluster with GPU instances, Amazon FSx for Lustre for research data, AWS Batch for computational workflows, AWS Lake Formation for data governance, and AWS CloudTrail for FDA audit trails",
        "Implement Amazon EMR with Spark clusters, Amazon S3 for data lakes, AWS Glue for data processing, Amazon Redshift for analytics, and AWS Config for compliance monitoring",
        "Use Amazon ECS with custom containers, Amazon EFS for shared storage, AWS Step Functions for workflow orchestration, Amazon DynamoDB for research metadata, and AWS Security Hub for compliance",
        "Deploy AWS Fargate for containerized workloads, Amazon DocumentDB for flexible data models, Amazon EventBridge for research coordination, AWS Lambda for data processing, and Amazon QuickSight for visualization"
    ],
    correct: 0,
    explanation: {
        correct: "ParallelCluster with GPU instances provides HPC performance for molecular simulations. FSx Lustre handles high-performance research data access. Batch orchestrates complex scientific workflows. Lake Formation ensures data governance for global collaboration. CloudTrail provides FDA-compliant audit trails for drug discovery.",
        whyWrong: {
            1: "EMR is optimized for big data analytics, not intensive molecular dynamics computations",
            2: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for scientific computing",
            3: "Fargate has resource limitations for scientific computing, DocumentDB doesn't optimize for research data patterns"
        },
        examStrategy: "Scientific HPC: ParallelCluster + GPU + FSx Lustre + Batch + Lake Formation for FDA-compliant drug discovery."
    }
},
{
    id: 'sap_309',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global consulting firm with 300+ offices needs to manage client data across different privacy jurisdictions, implement Chinese walls between competing clients, enable secure collaboration on joint projects, and maintain audit trails for regulatory compliance while supporting remote workforce.",
    question: "Which governance architecture ensures COMPREHENSIVE privacy protection and operational flexibility?",
    options: [
        "AWS Organizations with jurisdiction-specific OUs, AWS Lake Formation for fine-grained access control, AWS PrivateLink for secure client communication, AWS CloudTrail with legal hold for audit compliance, and AWS WorkSpaces for secure remote access",
        "AWS Control Tower for office standardization, AWS Config for privacy monitoring, AWS Direct Connect for office connectivity, AWS Backup for data protection, and AWS Client VPN for remote access",
        "AWS Organizations with client-based accounts, AWS Security Hub for centralized monitoring, AWS Transit Gateway for network segmentation, AWS KMS for encryption, and Amazon AppStream for remote applications",
        "AWS Landing Zone with geographic templates, AWS Directory Service for user management, AWS WAF for privacy protection, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure privacy law compliance. Lake Formation provides Chinese wall enforcement through fine-grained access controls. PrivateLink enables secure client communication without internet exposure. CloudTrail with legal hold provides compliant audit trails. WorkSpaces provides secure remote access with data loss prevention.",
        whyWrong: {
            1: "Control Tower doesn't address jurisdiction-specific privacy requirements or Chinese wall implementations",
            2: "Client-based accounts create management overhead without addressing privacy jurisdiction requirements",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the fine-grained access controls needed for Chinese walls"
        },
        examStrategy: "Consulting governance: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + WorkSpaces for privacy compliance."
    }
},
{
    id: 'sap_310',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A renewable energy management platform monitors 50,000+ solar panels and wind turbines globally, optimizes energy distribution based on weather forecasts, manages grid integration, and provides real-time energy trading capabilities with utility companies.",
    question: "Which architecture provides OPTIMAL renewable energy management and grid integration?",
    options: [
        "AWS IoT Core for device management, Amazon Kinesis Data Analytics for energy optimization, Amazon SageMaker for weather-based predictions, AWS Lambda for trading algorithms, and Amazon Timestream for energy metrics",
        "Amazon EventBridge for device events, AWS Batch for energy calculations, Amazon DocumentDB for device data, Amazon API Gateway for utility integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for edge processing, Amazon MSK for high-throughput messaging, Amazon EMR for analytics, AWS Step Functions for workflow coordination, and Amazon S3 for data storage",
        "Amazon EC2 Auto Scaling for processing, Amazon ElastiCache for real-time data, Amazon RDS for energy records, AWS Direct Connect for utility connectivity, and Amazon QuickSight for energy dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure, scalable management for 50,000+ renewable energy devices. Kinesis Data Analytics processes real-time energy data for optimization. SageMaker builds weather-based energy prediction models. Lambda enables real-time energy trading algorithms. Timestream is purpose-built for time-series energy metrics and optimization.",
        whyWrong: {
            1: "EventBridge has throughput limitations for 50,000+ devices, Batch adds latency for real-time energy optimization",
            2: "IoT Greengrass on every device adds operational complexity, MSK is overkill for energy device communication",
            3: "EC2 Auto Scaling doesn't provide IoT device management, RDS doesn't optimize for time-series energy data"
        },
        examStrategy: "Renewable energy: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for optimal energy management."
    }
},
{
    id: 'sap_311',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global logistics company's package tracking system experiences performance degradation with 500 million packages annually, real-time location updates lagging, customer service overload during peak shipping, and integration challenges with 1,000+ delivery partners worldwide.",
    question: "Which optimization strategy provides the BEST global logistics performance and partner integration?",
    options: [
        "Implement Amazon DynamoDB with Global Secondary Indexes for package tracking, Amazon Kinesis Data Streams for location updates, Amazon API Gateway for partner integration, AWS Lambda for notification processing, and Amazon Connect for automated customer service",
        "Deploy Amazon Aurora Global Database for package data, Amazon ElastiCache for location caching, Amazon EventBridge for partner events, AWS Step Functions for delivery workflows, and Amazon Pinpoint for customer notifications",
        "Use Amazon DocumentDB for flexible package schemas, Amazon MSK for real-time messaging, AWS Batch for route optimization, Amazon SQS for partner communication, and Amazon Chime for customer support",
        "Implement Amazon RDS with read replicas, Amazon S3 for tracking data, AWS Glue for partner data integration, Amazon EMR for analytics, and AWS Support for customer service"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB with GSI provides single-digit millisecond package lookups at global scale for 500M packages. Kinesis Data Streams handles real-time location updates efficiently. API Gateway enables scalable partner integration with rate limiting. Lambda scales automatically for notification processing. Connect provides automated customer service reducing overload.",
        whyWrong: {
            1: "Aurora Global Database still has write limitations for high-volume package updates",
            2: "DocumentDB doesn't optimize for package tracking queries, MSK adds complexity for simple location updates",
            3: "RDS with read replicas can't match DynamoDB performance for package tracking scale"
        },
        examStrategy: "Global logistics: DynamoDB + GSI + Kinesis + API Gateway + Lambda + Connect for optimal package tracking."
    }
},
{
    id: 'sap_312',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A telecommunications company needs to migrate their network monitoring system processing 2TB of network data hourly, maintaining real-time fault detection, ensuring 99.99% uptime for critical infrastructure, and supporting 5G network expansion while integrating with legacy OSS/BSS systems.",
    question: "Which migration strategy provides OPTIMAL network monitoring performance and reliability?",
    options: [
        "Implement Amazon MSK for network event streaming, Amazon Kinesis Data Analytics for real-time fault detection, Amazon Timestream for network metrics, AWS Lambda for alert processing, and AWS Direct Connect for legacy system integration",
        "Deploy Amazon EventBridge for network events, AWS Batch for data processing, Amazon Redshift for network analytics, Amazon SNS for notifications, and AWS VPN for legacy connectivity",
        "Use Amazon SQS for message handling, Amazon EC2 for processing, Amazon RDS for network data, Amazon CloudWatch for monitoring, and AWS Site-to-Site VPN for legacy integration",
        "Implement Amazon Kinesis Data Firehose for data ingestion, Amazon EMR for analytics, Amazon DynamoDB for network records, AWS Step Functions for workflows, and AWS Storage Gateway for legacy access"
    ],
    correct: 0,
    explanation: {
        correct: "MSK handles massive network event streams with high throughput and reliability. Kinesis Data Analytics provides real-time fault detection for 99.99% uptime requirements. Timestream is optimized for network time-series data. Lambda scales automatically for alert processing. Direct Connect ensures reliable integration with legacy OSS/BSS systems.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time fault detection",
            2: "SQS and RDS don't provide the real-time capabilities needed for network monitoring",
            3: "Firehose delays real-time fault detection, EMR is for batch analytics not real-time monitoring"
        },
        examStrategy: "Telecom monitoring: MSK + Kinesis Analytics + Timestream + Lambda + Direct Connect for real-time network operations."
    }
},

// Questions 313-325: correct: 1 (B)
{
    id: 'sap_313',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment bank operates trading floors in 15 major financial centers, requiring microsecond latency for high-frequency trading, strict regulatory compliance per jurisdiction, real-time risk management, and secure communication between trading desks while maintaining operational independence during network partitions.",
    question: "Which architecture ensures ULTRA-LOW latency trading with comprehensive risk management?",
    options: [
        "AWS Outposts in each trading floor, Amazon ElastiCache Redis clusters, AWS Lambda for trade processing, Amazon DynamoDB for trade records, and AWS Direct Connect for inter-floor communication",
        "AWS Local Zones for latency optimization, Amazon MemoryDB for trading state, AWS Batch for risk calculations, Amazon Timestream for trade data, and AWS PrivateLink for secure communication",
        "Amazon EC2 with placement groups, Amazon RDS Aurora for trade database, AWS Step Functions for trade workflows, Amazon EventBridge for inter-floor messaging, and AWS Site-to-Site VPN for connectivity",
        "AWS Wavelength for edge computing, Amazon DocumentDB for flexible trade schemas, AWS Glue for data processing, Amazon MSK for high-throughput messaging, and AWS Transit Gateway for network connectivity"
    ],
    correct: 1,
    explanation: {
        correct: "Local Zones provide ultra-low latency compute closer to financial exchanges. MemoryDB delivers microsecond performance with data persistence for trading state. Batch handles intensive risk calculations automatically. Timestream optimizes financial time-series data. PrivateLink ensures secure, low-latency communication between trading floors.",
        whyWrong: {
            0: "Outposts requires on-premises infrastructure management, Lambda has inherent latency for microsecond trading requirements",
            2: "Standard EC2 placement groups don't achieve Local Zone latency, Aurora adds latency for microsecond requirements",
            3: "Wavelength is for mobile edge computing, DocumentDB doesn't optimize for trading performance patterns"
        },
        examStrategy: "High-frequency trading: Local Zones + MemoryDB + Batch + Timestream + PrivateLink for ultra-low latency."
    }
},
{
    id: 'sap_314',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart city traffic management system monitors 100,000+ traffic lights, cameras, and sensors across a major metropolitan area, optimizes traffic flow in real-time, coordinates emergency vehicle routing, and provides public transportation integration with sub-second response requirements.",
    question: "Which architecture provides COMPREHENSIVE smart traffic management at metropolitan scale?",
    options: [
        "AWS IoT Core for device connectivity, Amazon MSK for high-volume messaging, Amazon EMR for traffic analytics, AWS Lambda for optimization algorithms, and Amazon S3 for historical data storage",
        "Amazon Kinesis Data Streams for sensor ingestion, Amazon Kinesis Data Analytics for real-time optimization, Amazon SageMaker for traffic prediction, AWS Lambda for emergency routing, and Amazon ElastiCache for traffic state caching",
        "Amazon EventBridge for traffic events, AWS Batch for route calculations, Amazon DocumentDB for traffic data, Amazon API Gateway for public transportation integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for edge processing, Amazon DynamoDB for traffic records, AWS Step Functions for traffic coordination, Amazon Redshift for analytics, and Amazon QuickSight for traffic dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently ingests data from 100,000+ devices. Kinesis Data Analytics provides real-time traffic optimization with sub-second response. SageMaker enables sophisticated traffic prediction models. Lambda handles emergency vehicle routing automatically. ElastiCache provides fast traffic state access for real-time decisions.",
        whyWrong: {
            0: "MSK adds complexity for traffic data, EMR is for batch analytics not real-time traffic optimization",
            2: "EventBridge and Batch add latency inappropriate for sub-second traffic management",
            3: "IoT Greengrass on every device adds operational complexity, Step Functions add latency for real-time traffic control"
        },
        examStrategy: "Smart traffic: Kinesis ecosystem + SageMaker + Lambda + ElastiCache for real-time metropolitan traffic management."
    }
},
{
    id: 'sap_315',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global media streaming platform serves 200 million subscribers with 4K/8K content, experiences buffering during major live events, has content delivery costs increasing 50% annually, and needs to optimize for emerging markets with limited bandwidth while maintaining premium quality in developed markets.",
    question: "Which optimization strategy provides the BEST global streaming performance with cost efficiency?",
    options: [
        "Implement Amazon CloudFront with custom origins, AWS Elemental MediaStore for content storage, AWS Elemental MediaConvert for adaptive streaming, and Amazon Route 53 for intelligent routing",
        "Deploy Amazon CloudFront with origin shield, S3 Intelligent-Tiering for content storage, AWS Elemental MediaPackage for stream optimization, AWS Elemental MediaTailor for personalization, and AWS Global Accelerator for performance",
        "Use AWS Global Accelerator for content delivery, Amazon EFS for content storage, AWS Batch for video processing, Amazon API Gateway for client requests, and Amazon ElastiCache for content caching",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Lambda for stream processing, Amazon EventBridge for event coordination, and Amazon DynamoDB for content metadata"
    ],
    correct: 1,
    explanation: {
        correct: "CloudFront with origin shield reduces origin requests and costs significantly. S3 Intelligent-Tiering automatically optimizes storage costs for varying content access patterns. MediaPackage provides adaptive bitrate streaming for bandwidth optimization. MediaTailor enables content personalization and ad insertion. Global Accelerator optimizes performance for emerging markets.",
        whyWrong: {
            0: "MediaStore is being deprecated, custom origins don't provide the cost optimization needed",
            2: "Global Accelerator alone doesn't provide content caching, EFS doesn't optimize for video content delivery",
            3: "Direct Connect for content delivery is expensive and doesn't provide global edge caching benefits"
        },
        examStrategy: "Media streaming: CloudFront + origin shield + S3 Intelligent-Tiering + MediaPackage + MediaTailor for cost-optimized delivery."
    }
},
{
    id: 'sap_316',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A university research center needs to migrate their computational biology workflows from on-premises HPC clusters to support COVID-19 vaccine research, protein folding simulations, genomic sequencing analysis, and collaborative research with international institutions while maintaining data security.",
    question: "Which modernization approach provides OPTIMAL research computing with global collaboration?",
    options: [
        "Deploy Amazon EMR with Spark clusters, implement Amazon S3 for research data, use AWS Glue for data processing, Amazon Redshift for analytics, and AWS Direct Connect for institutional connectivity",
        "Implement AWS ParallelCluster with Spot instances, Amazon FSx for Lustre for research data, AWS Batch for computational workflows, Amazon S3 with Cross-Region Replication for collaboration, and AWS Lake Formation for data governance",
        "Use Amazon ECS with GPU instances, implement Amazon EFS for shared storage, AWS Step Functions for workflow orchestration, Amazon DocumentDB for research metadata, and AWS VPN for institutional access",
        "Deploy AWS Fargate for containerized workloads, Amazon DynamoDB for research records, Amazon EventBridge for research coordination, AWS Lambda for data processing, and Amazon API Gateway for collaboration APIs"
    ],
    correct: 1,
    explanation: {
        correct: "ParallelCluster with Spot instances provides cost-effective HPC performance for computational biology. FSx Lustre delivers high-performance parallel file system for genomic data. Batch orchestrates complex computational workflows automatically. S3 with CRR enables secure global collaboration. Lake Formation provides fine-grained data governance for research sharing.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive computational biology workloads",
            2: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for genomic computing",
            3: "Fargate has resource limitations for computational biology, Lambda has execution time limits for research workloads"
        },
        examStrategy: "Research computing: ParallelCluster + Spot + FSx Lustre + Batch + Lake Formation for collaborative computational biology."
    }
},
{
    id: 'sap_317',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational retail corporation with 5,000+ stores across 60 countries needs centralized inventory management, real-time sales analytics, PCI compliance for payments, supply chain optimization, and the ability to operate stores independently during connectivity outages.",
    question: "Which governance architecture provides OPTIMAL retail operations with resilience?",
    options: [
        "AWS Organizations with geographic OUs, AWS Config for PCI compliance, AWS IoT Core for store devices, Amazon Kinesis for real-time analytics, and AWS Storage Gateway for offline operation",
        "AWS Control Tower with retail-specific landing zones, AWS Outposts in each store for local processing, Amazon DynamoDB Global Tables for inventory, AWS Lambda for supply chain optimization, and AWS Direct Connect for store connectivity",
        "AWS Organizations with store-based accounts, AWS Security Hub for compliance monitoring, Amazon ElastiCache for inventory caching, AWS Step Functions for supply chain workflows, and AWS Site-to-Site VPN for store connectivity",
        "AWS Landing Zone with country templates, AWS Directory Service for user management, Amazon RDS Aurora for inventory data, AWS Batch for analytics processing, and AWS Client VPN for store access"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower provides retail-specific governance and compliance templates. Outposts enables stores to operate independently during outages with local AWS infrastructure. DynamoDB Global Tables provides conflict-free inventory management across 5,000+ stores. Lambda scales automatically for supply chain optimization. Direct Connect ensures reliable store connectivity.",
        whyWrong: {
            0: "Storage Gateway alone doesn't provide full store operation during outages, IoT Core doesn't handle retail POS systems",
            2: "Store-based accounts create massive management overhead for 5,000+ stores",
            3: "Landing Zone is deprecated, RDS Aurora doesn't provide the global scale needed for 5,000+ stores"
        },
        examStrategy: "Retail governance: Control Tower + Outposts + DynamoDB Global Tables + Lambda for resilient store operations."
    }
},
{
    id: 'sap_318',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An autonomous shipping company operates 1,000+ cargo vessels globally, requiring real-time navigation optimization, weather-based route adjustments, port coordination, maritime regulatory compliance, and collision avoidance with sub-minute response times for course corrections.",
    question: "Which architecture provides COMPREHENSIVE maritime autonomous operations and safety?",
    options: [
        "AWS IoT Core for vessel connectivity, Amazon MSK for maritime data, Amazon EMR for route analytics, AWS Lambda for navigation processing, and Amazon S3 for voyage data storage",
        "AWS IoT Greengrass for vessel edge computing, Amazon Kinesis Data Streams for vessel telemetry, Amazon SageMaker for route optimization, AWS Lambda for collision avoidance, and Amazon Timestream for maritime data",
        "Amazon EventBridge for maritime events, AWS Batch for route calculations, Amazon DocumentDB for vessel data, Amazon API Gateway for port integration, and Amazon CloudWatch for vessel monitoring",
        "AWS Satellite Ground Station for vessel communication, Amazon MSK for data streaming, Amazon Redshift for voyage analytics, AWS Step Functions for port coordination, and Amazon QuickSight for fleet dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "IoT Greengrass enables autonomous decision-making at sea when connectivity is limited. Kinesis Data Streams handles real-time vessel telemetry from 1,000+ ships. SageMaker provides sophisticated route optimization with weather integration. Lambda enables sub-minute collision avoidance responses. Timestream optimizes maritime time-series data storage and analysis.",
        whyWrong: {
            0: "MSK adds complexity for vessel communication, EMR is for batch analytics not real-time navigation",
            2: "EventBridge and Batch add latency inappropriate for collision avoidance",
            3: "Ground Station is for satellite operations not vessel management, Step Functions add latency for real-time port coordination"
        },
        examStrategy: "Autonomous shipping: IoT Greengrass + Kinesis + SageMaker + Lambda + Timestream for maritime operations."
    }
},
{
    id: 'sap_319',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A healthcare system's electronic medical records platform serves 100+ hospitals with 50 million patient records, experiences slow query performance for medical history searches, has compliance issues with HIPAA auditing, and needs real-time integration with diagnostic equipment and laboratory systems.",
    question: "Which optimization provides the BEST healthcare system performance and compliance?",
    options: [
        "Implement Amazon Aurora with read replicas for medical records, Amazon ElastiCache for patient data caching, AWS Lambda for diagnostic integration, and AWS CloudTrail for HIPAA audit trails",
        "Deploy Amazon DynamoDB with Global Secondary Indexes for patient records, Amazon ElastiCache for medical history caching, AWS API Gateway for equipment integration, Amazon EventBridge for lab system coordination, and AWS Config for HIPAA compliance monitoring",
        "Use Amazon DocumentDB for flexible medical schemas, Amazon Kinesis for real-time data, AWS Step Functions for clinical workflows, Amazon S3 for medical imaging, and AWS Security Hub for compliance monitoring",
        "Implement Amazon RDS PostgreSQL with enhanced monitoring, Amazon Redshift for medical analytics, AWS Glue for data integration, Amazon QuickSight for clinical dashboards, and AWS Backup for data protection"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB with GSI provides fast medical history searches for 50M patient records with HIPAA compliance. ElastiCache accelerates frequent medical data queries. API Gateway enables secure diagnostic equipment integration. EventBridge coordinates lab system workflows efficiently. Config provides continuous HIPAA compliance monitoring.",
        whyWrong: {
            0: "Aurora with read replicas still struggles with complex medical history queries at 50M patient scale",
            2: "DocumentDB doesn't optimize for medical record query patterns, Step Functions add latency for clinical workflows",
            3: "RDS PostgreSQL doesn't provide the scale needed for 50M patient records, Redshift is for analytics not operational queries"
        },
        examStrategy: "Healthcare EMR: DynamoDB + GSI + ElastiCache + API Gateway + EventBridge + Config for HIPAA-compliant performance."
    }
},
{
    id: 'sap_320',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A government agency needs to migrate their citizen services platform from legacy systems to support 50 million citizens, ensure 99.99% availability, meet federal security requirements, integrate with state and local systems, and provide multilingual support while maintaining audit trails for compliance.",
    question: "Which modernization strategy ensures OPTIMAL citizen services with federal compliance?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for citizen data, use AWS Lambda for service processing, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for citizen records, AWS App Runner for service applications, Amazon EventBridge for system coordination, and AWS Config for federal compliance monitoring",
        "Use AWS Elastic Beanstalk for application hosting, Amazon Aurora for citizen database, AWS Step Functions for service workflows, Amazon Translate for multilingual support, and AWS Security Hub for compliance management",
        "Deploy Amazon EKS in GovCloud, implement Amazon DocumentDB for citizen data, AWS Lambda for microservices, Amazon API Gateway for integration, and AWS Systems Manager for compliance automation"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower in GovCloud provides federal compliance-ready landing zones. DynamoDB scales to 50M citizens with high availability. App Runner provides fully managed service hosting with automatic scaling. EventBridge coordinates state/local system integration efficiently. Config ensures continuous federal compliance monitoring.",
        whyWrong: {
            0: "Standard EC2 deployment doesn't provide the governance needed for 50M citizen scale",
            2: "Elastic Beanstalk doesn't provide the security controls needed for federal requirements",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for citizen service query patterns"
        },
        examStrategy: "Government services: Control Tower GovCloud + DynamoDB + App Runner + EventBridge + Config for federal citizen services."
    }
},
{
    id: 'sap_321',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global pharmaceutical company conducting clinical trials across 80+ countries needs to manage patient data with varying privacy regulations, coordinate multi-site research, ensure FDA/EMA compliance, enable secure researcher collaboration, and maintain data integrity for drug approval processes.",
    question: "Which governance framework ensures COMPREHENSIVE clinical trial compliance and collaboration?",
    options: [
        "AWS Organizations with regulatory OUs, AWS Config for compliance monitoring, AWS PrivateLink for researcher access, AWS CloudTrail for audit trails, and AWS Lake Formation for data governance",
        "AWS Organizations with country-specific OUs, AWS Lake Formation for fine-grained access control, AWS PrivateLink for secure site communication, AWS CloudTrail with tamper detection for FDA compliance, and AWS WorkSpaces for secure researcher access",
        "AWS Control Tower for site management, AWS Security Hub for compliance dashboards, AWS Direct Connect for site connectivity, AWS Backup for data protection, and AWS Client VPN for researcher access",
        "AWS Landing Zone with trial templates, AWS Directory Service for user management, AWS GuardDuty for threat detection, AWS Certificate Manager for secure communications, and Amazon WorkDocs for research collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with country OUs ensure privacy law compliance across 80+ countries. Lake Formation provides fine-grained access control for patient data protection. PrivateLink enables secure communication between trial sites. CloudTrail with tamper detection provides FDA/EMA-compliant audit trails. WorkSpaces ensures secure researcher access with data loss prevention.",
        whyWrong: {
            0: "Regulatory OUs alone don't address country-specific privacy laws, basic PrivateLink doesn't provide comprehensive access control",
            2: "Control Tower doesn't address country-specific clinical trial regulations",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the fine-grained access needed for patient data"
        },
        examStrategy: "Clinical trials: Organizations + country OUs + Lake Formation + PrivateLink + CloudTrail tamper detection for global compliance."
    }
},
{
    id: 'sap_322',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A precision agriculture platform monitors 1 million acres of farmland using IoT sensors, drones, and satellite imagery, providing real-time crop health analysis, automated irrigation control, pest detection, and yield prediction while optimizing resource usage and environmental impact.",
    question: "Which architecture provides OPTIMAL precision agriculture at scale?",
    options: [
        "AWS IoT Core for sensor management, Amazon MSK for high-volume data, Amazon EMR for crop analytics, AWS Lambda for irrigation control, and Amazon S3 for satellite imagery storage",
        "Amazon Kinesis Data Streams for sensor ingestion, Amazon Kinesis Data Analytics for real-time crop analysis, Amazon SageMaker for yield prediction, AWS IoT Device Management for equipment control, and Amazon Rekognition for pest detection",
        "Amazon EventBridge for agricultural events, AWS Batch for crop calculations, Amazon DocumentDB for farm data, Amazon API Gateway for equipment integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for edge processing, Amazon DynamoDB for crop records, AWS Step Functions for agricultural workflows, Amazon Redshift for analytics, and Amazon QuickSight for farm dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles sensor data from 1M acres. Kinesis Data Analytics provides real-time crop health analysis. SageMaker enables sophisticated yield prediction models. IoT Device Management controls irrigation and equipment remotely. Rekognition provides automated pest detection from drone imagery.",
        whyWrong: {
            0: "MSK adds complexity for agricultural sensor data, EMR is for batch analytics not real-time crop monitoring",
            2: "EventBridge and Batch add latency inappropriate for real-time irrigation control",
            3: "IoT Greengrass on every field adds operational complexity, Step Functions add latency for irrigation control"
        },
        examStrategy: "Precision agriculture: Kinesis ecosystem + SageMaker + IoT Device Management + Rekognition for intelligent farming."
    }
},
{
    id: 'sap_323',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global airline reservation system handles 10 million bookings daily, experiences overbooking issues during peak travel, has integration challenges with 500+ travel partners, faces performance issues during flash sales, and needs dynamic pricing optimization across 2,000+ routes globally.",
    question: "Which optimization strategy provides the BEST airline reservation system performance?",
    options: [
        "Implement Amazon Aurora Global Database for booking data, Amazon ElastiCache for seat availability, AWS Lambda for pricing algorithms, Amazon API Gateway for partner integration, and Amazon EventBridge for booking coordination",
        "Deploy Amazon DynamoDB Global Tables for reservations, Amazon ElastiCache for inventory management, AWS API Gateway for partner APIs, AWS Lambda for dynamic pricing, and Amazon Kinesis for real-time booking events",
        "Use Amazon DocumentDB for flexible booking schemas, Amazon MSK for high-throughput messaging, AWS Batch for pricing calculations, Amazon SQS for partner communication, and AWS Step Functions for booking workflows",
        "Implement Amazon RDS with read replicas, Amazon Redshift for booking analytics, AWS Glue for partner data integration, Amazon EMR for pricing optimization, and Amazon CloudFront for global distribution"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB Global Tables prevent overbooking conflicts with automatic conflict resolution across regions. ElastiCache provides real-time seat inventory management. API Gateway enables scalable integration with 500+ partners. Lambda scales automatically for dynamic pricing across 2,000+ routes. Kinesis handles 10M daily booking events efficiently.",
        whyWrong: {
            0: "Aurora Global Database still has write conflicts during peak booking periods causing overbooking",
            2: "DocumentDB doesn't optimize for reservation conflict resolution, MSK adds complexity for booking workflows",
            3: "RDS with read replicas doesn't solve write conflicts, EMR adds latency for real-time pricing"
        },
        examStrategy: "Airline reservations: DynamoDB Global Tables + ElastiCache + API Gateway + Lambda + Kinesis for conflict-free bookings."
    }
},
{
    id: 'sap_324',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A manufacturing company needs to migrate their industrial IoT system monitoring 100,000+ sensors across 200 factories, maintaining real-time production optimization, predictive maintenance, quality control, and integration with legacy MES and ERP systems while ensuring minimal disruption.",
    question: "Which migration approach provides OPTIMAL industrial IoT modernization with legacy integration?",
    options: [
        "Deploy AWS IoT Core for sensor connectivity, implement Amazon Kinesis for real-time data, AWS Lambda for processing, Amazon DynamoDB for sensor data, and AWS Direct Connect for legacy system integration",
        "Implement AWS IoT Greengrass for factory edge computing, Amazon Kinesis Data Analytics for real-time optimization, Amazon SageMaker for predictive maintenance, Amazon Timestream for sensor metrics, and AWS Storage Gateway for legacy integration",
        "Use Amazon EventBridge for sensor events, AWS Batch for manufacturing analytics, Amazon DocumentDB for sensor data, Amazon API Gateway for legacy integration, and Amazon CloudWatch for monitoring",
        "Deploy Amazon EC2 for sensor processing, Amazon MSK for data streaming, Amazon EMR for analytics, Amazon RDS for manufacturing data, and AWS VPN for legacy connectivity"
    ],
    correct: 1,
    explanation: {
        correct: "IoT Greengrass enables real-time processing at factory edge reducing latency. Kinesis Data Analytics provides real-time production optimization. SageMaker enables sophisticated predictive maintenance models. Timestream is purpose-built for industrial sensor time-series data. Storage Gateway provides seamless legacy MES/ERP integration.",
        whyWrong: {
            0: "IoT Core alone doesn't provide edge processing needed for real-time production optimization",
            2: "EventBridge and Batch add latency inappropriate for real-time manufacturing",
            3: "EC2 doesn't provide IoT-optimized connectivity, MSK adds complexity for sensor communication"
        },
        examStrategy: "Industrial IoT: IoT Greengrass + Kinesis Analytics + SageMaker + Timestream + Storage Gateway for factory modernization."
    }
},
{
    id: 'sap_325',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global energy trading company operates across 25 countries with different energy regulations, needs real-time commodity trading, risk management across multiple time zones, regulatory reporting per jurisdiction, and secure communication between trading desks while maintaining audit trails for compliance.",
    question: "Which governance architecture ensures COMPREHENSIVE energy trading compliance and performance?",
    options: [
        "AWS Organizations with trading desk OUs, AWS Config for regulatory compliance, AWS Lambda for trade processing, Amazon DynamoDB for trade records, and AWS PrivateLink for secure communication",
        "AWS Organizations with jurisdiction-specific OUs, AWS Config for regulatory monitoring, Amazon ElastiCache for real-time trading data, AWS Lambda for risk calculations, and AWS CloudTrail with legal hold for audit compliance",
        "AWS Control Tower for trading standardization, AWS Security Hub for compliance dashboards, Amazon MSK for trading messages, AWS Step Functions for trade workflows, and AWS Direct Connect for desk connectivity",
        "AWS Landing Zone with energy templates, AWS Directory Service for trader authentication, Amazon Kinesis for trading data, AWS Batch for risk processing, and AWS Certificate Manager for secure communications"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 25 different energy regulations. Config monitors regulatory compliance continuously. ElastiCache provides microsecond latency for real-time commodity trading. Lambda scales automatically for risk calculations. CloudTrail with legal hold provides compliant audit trails for energy trading.",
        whyWrong: {
            0: "Trading desk OUs don't address jurisdiction-specific regulatory requirements",
            2: "Control Tower doesn't address jurisdiction-specific energy trading regulations",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the regulatory governance needed"
        },
        examStrategy: "Energy trading: Organizations + jurisdiction OUs + Config + ElastiCache + Lambda + CloudTrail legal hold for compliance."
    }
},

// Questions 326-337: correct: 2 (C)
{
    id: 'sap_326',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A quantum computing research platform enables 500+ researchers globally to access quantum simulators, run quantum algorithms, manage quantum circuit designs, and collaborate on quantum machine learning projects while ensuring fair resource allocation and protecting intellectual property.",
    question: "Which architecture provides OPTIMAL quantum computing research collaboration and IP protection?",
    options: [
        "Amazon EC2 with GPU instances for quantum simulation, Amazon S3 for circuit storage, AWS Lambda for algorithm processing, Amazon DynamoDB for research data, and AWS IAM for access control",
        "AWS Batch for quantum job scheduling, Amazon EFS for shared research data, AWS Step Functions for quantum workflows, Amazon DocumentDB for circuit metadata, and AWS VPN for researcher access",
        "Amazon Braket for quantum computing access, AWS Lake Formation for IP protection and fine-grained access control, Amazon SageMaker for quantum ML, AWS Batch for fair resource scheduling, and AWS PrivateLink for secure researcher collaboration",
        "Amazon EMR for quantum analytics, Amazon Redshift for research data, AWS Glue for data processing, Amazon QuickSight for research dashboards, and AWS Direct Connect for institutional access"
    ],
    correct: 2,
    explanation: {
        correct: "Amazon Braket provides managed quantum computing access with fair resource allocation. Lake Formation ensures IP protection through fine-grained access controls and data governance. SageMaker enables quantum machine learning research. Batch provides fair scheduling for quantum jobs among 500+ researchers. PrivateLink ensures secure collaboration without internet exposure.",
        whyWrong: {
            0: "Standard EC2 doesn't provide quantum computing capabilities, basic IAM doesn't provide IP protection needed",
            1: "Batch alone doesn't provide quantum computing access, EFS doesn't provide IP protection",
            3: "EMR doesn't provide quantum computing capabilities, Redshift is for traditional analytics not quantum research"
        },
        examStrategy: "Quantum research: Braket + Lake Formation + SageMaker + Batch + PrivateLink for collaborative quantum computing."
    }
},
{
    id: 'sap_327',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global ride-sharing platform serves 50 million active users, experiences driver matching delays during peak hours, has surge pricing algorithm issues, faces real-time location tracking problems, and needs to optimize routes for 1 million concurrent rides while reducing operational costs.",
    question: "Which optimization strategy provides the BEST ride-sharing platform performance and cost efficiency?",
    options: [
        "Implement Amazon ElastiCache for driver location caching, AWS Lambda for matching algorithms, Amazon DynamoDB for ride data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for ride records, Amazon MSK for real-time messaging, AWS Step Functions for ride workflows, Amazon SageMaker for surge pricing, and AWS Direct Connect for mobile connectivity",
        "Use Amazon ElastiCache Redis cluster mode for real-time location data, AWS Lambda for driver matching and surge pricing, Amazon DynamoDB Global Tables for ride state, Amazon Kinesis for location streams, and Amazon SageMaker for route optimization",
        "Implement Amazon RDS with read replicas, Amazon SQS for ride requests, AWS Batch for route calculations, Amazon EventBridge for ride coordination, and Amazon S3 for location data storage"
    ],
    correct: 2,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond location data access for 1M concurrent rides. Lambda scales automatically for matching and surge pricing algorithms. DynamoDB Global Tables maintains ride state globally. Kinesis handles real-time location streams efficiently. SageMaker provides sophisticated route optimization reducing operational costs.",
        whyWrong: {
            0: "Basic ElastiCache setup doesn't provide the clustering needed for 1M concurrent rides",
            1: "Aurora Global Database has write limitations for high-frequency location updates, MSK adds complexity",
            3: "RDS with read replicas can't handle 1M concurrent ride updates, SQS adds latency for real-time matching"
        },
        examStrategy: "Ride-sharing optimization: ElastiCache cluster + Lambda + DynamoDB Global Tables + Kinesis + SageMaker for scale."
    }
},
{
    id: 'sap_328',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A space agency needs to migrate their mission control systems from on-premises infrastructure to support multiple simultaneous space missions, real-time telemetry from satellites, ground station coordination across continents, and mission-critical decision making with zero tolerance for downtime.",
    question: "Which modernization approach ensures MAXIMUM reliability for mission-critical space operations?",
    options: [
        "Deploy Amazon EC2 in multiple regions, implement Amazon RDS Aurora for mission data, use AWS Lambda for telemetry processing, Amazon API Gateway for ground station integration, and AWS CloudFormation for infrastructure automation",
        "Implement AWS Outposts for mission control rooms, Amazon Kinesis for telemetry streams, AWS Step Functions for mission workflows, Amazon DynamoDB for satellite data, and AWS Direct Connect for ground station connectivity",
        "Use AWS Ground Station for satellite communication, Amazon Kinesis Data Streams for real-time telemetry, AWS Lambda for mission-critical processing, Amazon DynamoDB Global Tables for mission state, Amazon CloudWatch for monitoring with AWS Auto Scaling for automatic failover",
        "Deploy AWS Wavelength for low-latency processing, Amazon MSK for data streaming, Amazon EMR for mission analytics, Amazon Redshift for mission data, and AWS Transit Gateway for ground station networking"
    ],
    correct: 2,
    explanation: {
        correct: "AWS Ground Station provides managed satellite communication infrastructure. Kinesis Data Streams handles real-time telemetry with automatic scaling. Lambda provides serverless processing for mission-critical decisions. DynamoDB Global Tables ensures mission state availability across regions. CloudWatch with Auto Scaling provides automatic failover for zero downtime tolerance.",
        whyWrong: {
            0: "Standard EC2 deployment doesn't provide satellite communication capabilities",
            1: "Outposts requires on-premises infrastructure management, Step Functions add latency for mission-critical decisions",
            3: "Wavelength is for mobile edge computing not space missions, EMR adds latency for real-time telemetry"
        },
        examStrategy: "Space missions: Ground Station + Kinesis + Lambda + DynamoDB Global Tables + Auto Scaling for mission-critical reliability."
    }
},
{
    id: 'sap_329',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global insurance company operates in 40+ countries with different insurance regulations, needs real-time claims processing, fraud detection across multiple lines of business, regulatory reporting per jurisdiction, and secure customer data handling while enabling cross-border risk assessment.",
    question: "Which governance framework ensures COMPREHENSIVE insurance compliance with operational efficiency?",
    options: [
        "AWS Organizations with business line OUs, AWS Config for regulatory compliance, AWS Lambda for claims processing, Amazon DynamoDB for customer data, and AWS PrivateLink for secure communication",
        "AWS Control Tower for insurance standardization, AWS Security Hub for compliance monitoring, Amazon Kinesis for claims data, AWS Step Functions for claims workflows, and AWS Direct Connect for regulatory connectivity",
        "AWS Organizations with jurisdiction-specific OUs, AWS Config for regulatory monitoring, AWS Lake Formation for customer data governance, Amazon Kinesis Data Analytics for fraud detection, and AWS CloudTrail with legal hold for audit compliance",
        "AWS Landing Zone with insurance templates, AWS Directory Service for agent authentication, Amazon ElastiCache for claims caching, AWS Batch for fraud analysis, and AWS Certificate Manager for secure communications"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 40+ different insurance regulations. Config monitors regulatory compliance continuously. Lake Formation provides fine-grained customer data governance for cross-border operations. Kinesis Data Analytics enables real-time fraud detection. CloudTrail with legal hold provides compliant audit trails.",
        whyWrong: {
            0: "Business line OUs don't address jurisdiction-specific regulatory requirements",
            1: "Control Tower doesn't address jurisdiction-specific insurance regulations",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the data governance needed for insurance"
        },
        examStrategy: "Insurance compliance: Organizations + jurisdiction OUs + Config + Lake Formation + Kinesis Analytics + CloudTrail legal hold."
    }
},
{
    id: 'sap_330',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A digital twin platform for manufacturing creates virtual replicas of 10,000+ industrial machines, simulates production scenarios, predicts equipment failures, optimizes maintenance schedules, and enables remote monitoring with real-time synchronization between physical and digital twins.",
    question: "Which architecture provides OPTIMAL digital twin manufacturing platform capabilities?",
    options: [
        "AWS IoT Core for machine connectivity, Amazon MSK for high-volume data, Amazon EMR for simulation processing, AWS Lambda for twin synchronization, and Amazon S3 for simulation data storage",
        "Amazon EventBridge for machine events, AWS Batch for simulation calculations, Amazon DocumentDB for twin data, Amazon API Gateway for remote access, and Amazon CloudWatch for monitoring",
        "AWS IoT TwinMaker for digital twin creation, Amazon Kinesis Data Streams for real-time machine data, Amazon SageMaker for predictive maintenance, AWS Lambda for twin synchronization, and Amazon Timestream for industrial time-series data",
        "AWS IoT Greengrass for edge processing, Amazon DynamoDB for twin records, AWS Step Functions for simulation workflows, Amazon Redshift for analytics, and Amazon QuickSight for monitoring dashboards"
    ],
    correct: 2,
    explanation: {
        correct: "AWS IoT TwinMaker provides purpose-built digital twin creation and management. Kinesis Data Streams handles real-time synchronization between physical and digital twins. SageMaker enables sophisticated predictive maintenance models. Lambda provides real-time twin synchronization. Timestream optimizes industrial time-series data for digital twin analytics.",
        whyWrong: {
            0: "MSK adds complexity for digital twin synchronization, EMR doesn't provide real-time simulation capabilities",
            1: "EventBridge and Batch add latency inappropriate for real-time twin synchronization",
            3: "IoT Greengrass doesn't provide digital twin capabilities, Step Functions add latency for real-time synchronization"
        },
        examStrategy: "Digital twins: IoT TwinMaker + Kinesis + SageMaker + Lambda + Timestream for manufacturing twin platform."
    }
},
{
    id: 'sap_331',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global video conferencing platform serves 500 million users, experiences quality degradation during peak usage, has audio/video sync issues, faces scaling challenges during viral meetings, and needs to optimize bandwidth usage while maintaining HD quality across diverse network conditions.",
    question: "Which optimization strategy provides the BEST video conferencing performance and scalability?",
    options: [
        "Implement Amazon CloudFront for content delivery, AWS Elemental MediaLive for video processing, Amazon ElastiCache for session management, AWS Lambda for meeting coordination, and Amazon Route 53 for traffic routing",
        "Deploy AWS Global Accelerator for performance, Amazon Kinesis Video Streams for real-time processing, AWS Batch for video transcoding, Amazon DynamoDB for meeting data, and Amazon API Gateway for client connections",
        "Use Amazon Chime SDK for video infrastructure, Amazon CloudFront with Lambda@Edge for adaptive streaming, Amazon Kinesis Video Streams for media processing, AWS Auto Scaling for dynamic capacity, and Amazon ElastiCache for session state management",
        "Implement AWS Direct Connect for dedicated connectivity, Amazon ECS for video processing, Amazon EventBridge for meeting coordination, Amazon RDS for meeting records, and AWS Certificate Manager for security"
    ],
    correct: 2,
    explanation: {
        correct: "Chime SDK provides purpose-built video conferencing infrastructure with automatic scaling. CloudFront with Lambda@Edge enables adaptive streaming based on network conditions. Kinesis Video Streams handles real-time media processing and sync. Auto Scaling manages viral meeting spikes automatically. ElastiCache provides fast session state management.",
        whyWrong: {
            0: "MediaLive is for broadcast streaming not interactive video conferencing",
            1: "Global Accelerator alone doesn't provide video conferencing capabilities, Batch adds latency for real-time video",
            3: "Direct Connect doesn't provide global video conferencing infrastructure, ECS doesn't optimize for video processing"
        },
        examStrategy: "Video conferencing: Chime SDK + CloudFront Lambda@Edge + Kinesis Video Streams + Auto Scaling for optimal performance."
    }
},
{
    id: 'sap_332',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A scientific research institute needs to migrate their climate modeling supercomputing workloads to AWS, supporting atmospheric simulations, ocean current modeling, carbon cycle analysis, and international climate research collaboration while maintaining data accuracy and computational precision.",
    question: "Which modernization approach provides OPTIMAL climate research computing capabilities?",
    options: [
        "Deploy Amazon EMR with Spark clusters, implement Amazon S3 for climate data, use AWS Glue for data processing, Amazon Redshift for analytics, and AWS Direct Connect for institutional collaboration",
        "Implement Amazon ECS with GPU instances, use Amazon EFS for shared storage, AWS Step Functions for workflow orchestration, Amazon DocumentDB for research metadata, and AWS VPN for collaborative access",
        "Use AWS ParallelCluster with HPC-optimized instances, Amazon FSx for Lustre for climate data, AWS Batch for simulation scheduling, Amazon S3 with Cross-Region Replication for global collaboration, and AWS Lake Formation for research data governance",
        "Deploy AWS Fargate for containerized simulations, Amazon DynamoDB for climate records, Amazon EventBridge for research coordination, AWS Lambda for data processing, and Amazon API Gateway for collaboration APIs"
    ],
    correct: 2,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for climate modeling. FSx Lustre delivers high-performance parallel file system for massive climate datasets. Batch schedules complex simulation workflows efficiently. S3 with CRR enables global research collaboration. Lake Formation provides data governance for international research sharing.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive climate modeling computations",
            1: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for climate computing",
            3: "Fargate has resource limitations for climate modeling, Lambda has execution time limits for complex simulations"
        },
        examStrategy: "Climate research: ParallelCluster + HPC + FSx Lustre + Batch + Lake Formation for scientific computing collaboration."
    }
},
{
    id: 'sap_333',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational media conglomerate with 200+ content production studios across 50 countries needs centralized content management, regional content distribution, intellectual property protection, talent management, and compliance with varying media regulations while enabling creative collaboration.",
    question: "Which governance architecture ensures COMPREHENSIVE media content management and IP protection?",
    options: [
        "AWS Organizations with studio-based OUs, AWS Config for media compliance, AWS PrivateLink for content distribution, AWS CloudTrail for IP audit trails, and AWS WorkSpaces for creative collaboration",
        "AWS Control Tower for studio standardization, AWS Security Hub for compliance monitoring, Amazon S3 for content storage, AWS Lambda for content processing, and AWS Direct Connect for studio connectivity",
        "AWS Organizations with regional OUs, AWS Lake Formation for IP protection and content governance, AWS Elemental MediaServices for content workflow, AWS PrivateLink for secure collaboration, and AWS CloudTrail with legal hold for IP compliance",
        "AWS Landing Zone with media templates, AWS Directory Service for talent authentication, Amazon CloudFront for content delivery, AWS Certificate Manager for security, and Amazon WorkDocs for collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with regional OUs ensure compliance with 50 different media regulations. Lake Formation provides IP protection through fine-grained content access controls. Elemental MediaServices handles content production workflows efficiently. PrivateLink enables secure creative collaboration. CloudTrail with legal hold provides IP-compliant audit trails.",
        whyWrong: {
            0: "Studio-based OUs don't address regional media regulation requirements",
            1: "Control Tower doesn't address region-specific media regulations, basic S3 doesn't provide IP protection",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the IP governance needed for media content"
        },
        examStrategy: "Media governance: Organizations + regional OUs + Lake Formation + Elemental MediaServices + CloudTrail legal hold for IP protection."
    }
},
{
    id: 'sap_334',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An advanced materials research platform enables 1,000+ scientists to design new materials using AI/ML, simulate molecular structures, predict material properties, manage experimental data, and collaborate on breakthrough discoveries while protecting research IP and enabling technology transfer.",
    question: "Which architecture provides OPTIMAL materials research collaboration with IP protection?",
    options: [
        "Amazon EC2 with GPU instances for molecular simulation, Amazon S3 for research data, AWS Lambda for material analysis, Amazon DynamoDB for experimental records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for research coordination, Amazon DocumentDB for material metadata, and AWS VPN for scientist access",
        "Amazon SageMaker for AI/ML materials discovery, AWS ParallelCluster for molecular simulations, AWS Lake Formation for IP protection and research governance, Amazon FSx for Lustre for computational data, and AWS PrivateLink for secure research collaboration",
        "Amazon EMR for materials analytics, Amazon Redshift for research data, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional access"
    ],
    correct: 2,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for materials discovery research. ParallelCluster delivers HPC performance for molecular simulations. Lake Formation ensures IP protection through fine-grained access controls. FSx Lustre provides high-performance storage for computational workloads. PrivateLink enables secure research collaboration and technology transfer.",
        whyWrong: {
            0: "Standard EC2 doesn't provide the AI/ML and HPC capabilities needed, basic IAM doesn't provide IP protection",
            1: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide IP protection needed for research",
            3: "EMR doesn't provide AI/ML or HPC capabilities for materials research, Redshift is for traditional analytics"
        },
        examStrategy: "Materials research: SageMaker + ParallelCluster + Lake Formation + FSx Lustre + PrivateLink for AI-driven discovery."
    }
},
{
    id: 'sap_335',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online marketplace with 100 million products experiences search performance issues, product recommendation delays, inventory synchronization problems across regions, seller onboarding bottlenecks, and payment processing failures during flash sales.",
    question: "Which optimization strategy provides the BEST marketplace performance and seller experience?",
    options: [
        "Implement Amazon ElastiCache for product caching, AWS Lambda for search algorithms, Amazon DynamoDB for inventory data, Amazon API Gateway for seller APIs, and Amazon SQS for payment processing",
        "Deploy Amazon Aurora Global Database for product catalog, Amazon MSK for real-time updates, AWS Step Functions for seller workflows, Amazon SageMaker for recommendations, and AWS Batch for payment processing",
        "Use Amazon OpenSearch for product search, Amazon SageMaker for real-time recommendations, Amazon DynamoDB Global Tables for inventory synchronization, AWS Lambda for seller onboarding automation, and Amazon Kinesis for payment event processing",
        "Implement Amazon RDS with read replicas, Amazon Redshift for product analytics, AWS Glue for seller data integration, Amazon EMR for recommendation processing, and Amazon EventBridge for payment coordination"
    ],
    correct: 2,
    explanation: {
        correct: "OpenSearch provides sophisticated product search capabilities with real-time indexing. SageMaker enables real-time recommendation inference. DynamoDB Global Tables ensures inventory synchronization across regions. Lambda automates seller onboarding workflows. Kinesis handles payment event processing during flash sales efficiently.",
        whyWrong: {
            0: "Basic Lambda search algorithms can't match OpenSearch capabilities for 100M products",
            1: "Aurora Global Database has write limitations for high-frequency inventory updates, Batch adds latency for payments",
            3: "RDS with read replicas can't handle marketplace scale, EMR adds latency for real-time recommendations"
        },
        examStrategy: "Marketplace optimization: OpenSearch + SageMaker + DynamoDB Global Tables + Lambda + Kinesis for optimal performance."
    }
},
{
    id: 'sap_336',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A defense contractor needs to migrate their weapons systems simulation platform from classified on-premises infrastructure to AWS, maintaining security clearance requirements, ensuring real-time simulation performance, supporting multi-domain operations, and enabling secure collaboration with allied nations.",
    question: "Which modernization approach ensures MAXIMUM security for classified defense simulations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for simulation data, use AWS Lambda for simulation processing, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for simulation records, AWS Batch for computational workflows, Amazon S3 for simulation assets, and AWS Config for security compliance",
        "Use AWS GovCloud with isolated VPCs per security classification, AWS ParallelCluster for simulation computing, AWS Lake Formation for data governance, AWS PrivateLink for allied collaboration, and AWS CloudTrail with tamper detection for audit compliance",
        "Deploy AWS Outposts in secure facilities, Amazon FSx for simulation data, AWS Step Functions for simulation workflows, Amazon ElastiCache for performance, and AWS Direct Connect for allied connectivity"
    ],
    correct: 2,
    explanation: {
        correct: "GovCloud with isolated VPCs ensures proper security classification separation. ParallelCluster provides HPC performance for weapons simulations. Lake Formation provides fine-grained data governance for classified information. PrivateLink enables secure allied collaboration without internet exposure. CloudTrail with tamper detection provides audit compliance for defense operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide security classification separation needed for weapons systems",
            1: "Control Tower doesn't provide the security isolation needed for different classification levels",
            3: "Outposts requires on-premises management, Step Functions add latency for real-time simulations"
        },
        examStrategy: "Defense simulations: GovCloud + isolated VPCs + ParallelCluster + Lake Formation + PrivateLink for classified security."
    }
},
{
    id: 'sap_337',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational automotive manufacturer with 150+ assembly plants needs real-time production monitoring, supply chain coordination, quality control integration, regulatory compliance across markets, and the ability to rapidly scale production for new vehicle models while maintaining operational independence during disruptions.",
    question: "Which governance architecture provides OPTIMAL automotive manufacturing resilience and compliance?",
    options: [
        "AWS Organizations with plant-based OUs, AWS Config for quality compliance, AWS IoT Core for production monitoring, Amazon Kinesis for supply chain data, and AWS Storage Gateway for operational independence",
        "AWS Control Tower for manufacturing standardization, AWS Security Hub for compliance monitoring, Amazon MSK for plant communication, AWS Lambda for production processing, and AWS Direct Connect for plant connectivity",
        "AWS Organizations with regional OUs for market compliance, AWS IoT Core for production monitoring, Amazon Kinesis Data Analytics for real-time optimization, AWS Outposts for plant independence, and AWS Config for automotive regulatory compliance",
        "AWS Landing Zone with automotive templates, AWS Directory Service for worker authentication, Amazon ElastiCache for production caching, AWS Batch for quality analysis, and AWS Certificate Manager for secure communications"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with regional OUs ensure compliance with different automotive market regulations. IoT Core manages production equipment and sensors across 150+ plants. Kinesis Data Analytics provides real-time production optimization. Outposts enables plants to operate independently during disruptions. Config monitors automotive regulatory compliance continuously.",
        whyWrong: {
            0: "Plant-based OUs don't address market-specific automotive regulations, Storage Gateway doesn't provide full independence",
            1: "Control Tower doesn't address market-specific automotive regulations, MSK adds complexity for plant communication",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the regulatory governance needed for automotive manufacturing"
        },
        examStrategy: "Automotive governance: Organizations + regional OUs + IoT Core + Kinesis Analytics + Outposts + Config for manufacturing resilience."
    }
},

// Questions 338-350: correct: 3 (D)
{
    id: 'sap_338',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A brain-computer interface platform enables 10,000+ researchers to analyze neural signals, decode brain patterns, develop therapeutic applications, collaborate on neuroscience research, and maintain patient privacy while advancing medical treatments for neurological disorders.",
    question: "Which architecture provides OPTIMAL neuroscience research platform with privacy protection?",
    options: [
        "Amazon EC2 with GPU instances for neural processing, Amazon S3 for brain data, AWS Lambda for signal analysis, Amazon DynamoDB for research records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for research coordination, Amazon DocumentDB for neural metadata, and AWS VPN for researcher access",
        "Amazon SageMaker for neural pattern analysis, AWS ParallelCluster for brain signal processing, Amazon Kinesis for real-time neural streams, Amazon FSx for research data, and AWS Lake Formation for privacy protection",
        "Amazon SageMaker for AI/ML neural analysis, AWS ParallelCluster for intensive signal processing, AWS Lake Formation for patient privacy governance, Amazon Kinesis for real-time neural data streams, and AWS PrivateLink for secure research collaboration",
    ],
    correct: 3,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for neural pattern analysis and brain decoding. ParallelCluster delivers HPC performance for intensive neural signal processing. Lake Formation ensures patient privacy through fine-grained access controls and HIPAA compliance. Kinesis handles real-time neural data streams. PrivateLink enables secure research collaboration while protecting patient data.",
        whyWrong: {
            0: "Standard EC2 doesn't provide AI/ML capabilities needed for neural analysis, basic IAM doesn't provide patient privacy protection",
            1: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide privacy protection for patient data",
            2: "Missing comprehensive privacy governance needed for patient neural data"
        },
        examStrategy: "Neuroscience research: SageMaker + ParallelCluster + Lake Formation + Kinesis + PrivateLink for privacy-protected neural research."
    }
},
{
    id: 'sap_339',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global news and media platform delivers content to 500 million users, experiences performance issues during breaking news events, has content moderation delays, faces regional content restriction challenges, and needs real-time audience analytics while maintaining editorial workflow efficiency.",
    question: "Which optimization strategy provides the BEST news platform performance and content management?",
    options: [
        "Implement Amazon CloudFront for content delivery, AWS Lambda for content processing, Amazon DynamoDB for article data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for analytics",
        "Deploy Amazon Aurora Global Database for content storage, Amazon MSK for real-time messaging, AWS Step Functions for editorial workflows, Amazon SageMaker for content recommendations, and AWS Batch for content moderation",
        "Use Amazon ElastiCache for content caching, Amazon Kinesis for real-time analytics, AWS Lambda for content moderation, Amazon S3 for media storage, and Amazon EventBridge for editorial coordination",
        "Implement Amazon CloudFront with Lambda@Edge for regional content restrictions, Amazon Rekognition for automated content moderation, Amazon Kinesis Data Analytics for real-time audience insights, Amazon DynamoDB Global Tables for content distribution, and AWS Step Functions for editorial workflows",
    ],
    correct: 3,
    explanation: {
        correct: "CloudFront with Lambda@Edge enables regional content restrictions and breaking news scaling. Rekognition provides automated content moderation reducing delays. Kinesis Data Analytics delivers real-time audience analytics. DynamoDB Global Tables ensures global content distribution. Step Functions streamline editorial workflows efficiently.",
        whyWrong: {
            0: "Basic Lambda processing doesn't provide content moderation capabilities, CloudWatch doesn't provide real-time audience analytics",
            1: "Aurora Global Database has write limitations during breaking news spikes, Batch adds latency for content moderation",
            2: "Basic Lambda doesn't provide content moderation capabilities, EventBridge adds complexity for editorial workflows"
        },
        examStrategy: "News platform: CloudFront Lambda@Edge + Rekognition + Kinesis Analytics + DynamoDB Global Tables + Step Functions for optimal news delivery."
    }
},
{
    id: 'sap_340',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A nuclear research facility needs to migrate their reactor simulation and safety monitoring systems from air-gapped networks to AWS, maintaining the highest security standards, ensuring real-time safety monitoring, supporting international nuclear research collaboration, and meeting strict regulatory compliance.",
    question: "Which modernization approach ensures MAXIMUM security for nuclear research systems?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for research data, use AWS Lambda for monitoring processing, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for monitoring records, AWS Batch for simulation workflows, Amazon S3 for research data, and AWS Config for regulatory compliance",
        "Use AWS Outposts in secure facilities, Amazon FSx for simulation data, AWS Step Functions for safety workflows, Amazon ElastiCache for performance, and AWS Direct Connect for research collaboration",
        "Deploy AWS GovCloud with air-gapped VPCs, AWS ParallelCluster for nuclear simulations, AWS Lake Formation for research data governance, AWS IoT Core for reactor monitoring with AWS PrivateLink for international collaboration",
    ],
    correct: 3,
    explanation: {
        correct: "GovCloud with air-gapped VPCs provides maximum security isolation for nuclear systems. ParallelCluster delivers HPC performance for nuclear reactor simulations. Lake Formation provides strict data governance for nuclear research. IoT Core enables secure reactor monitoring. PrivateLink allows international collaboration without internet exposure maintaining security.",
        whyWrong: {
            0: "Standard deployment doesn't provide the air-gapped security needed for nuclear facilities",
            1: "Control Tower doesn't provide air-gapped security isolation needed for nuclear systems",
            2: "Outposts requires on-premises management and doesn't provide the security isolation needed"
        },
        examStrategy: "Nuclear research: GovCloud air-gapped VPCs + ParallelCluster + Lake Formation + IoT Core + PrivateLink for maximum security."
    }
},
{
    id: 'sap_341',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment management firm with $2 trillion assets under management operates across 60+ countries, needs real-time portfolio risk assessment, regulatory compliance per jurisdiction, client data protection, algorithmic trading capabilities, and secure communication with fund managers while maintaining operational resilience.",
    question: "Which governance architecture ensures COMPREHENSIVE investment management compliance and performance?",
    options: [
        "AWS Organizations with fund-based OUs, AWS Config for investment compliance, AWS Lambda for trading algorithms, Amazon DynamoDB for portfolio data, and AWS PrivateLink for manager communication",
        "AWS Control Tower for investment standardization, AWS Security Hub for compliance monitoring, Amazon Kinesis for trading data, AWS Step Functions for investment workflows, and AWS Direct Connect for regulatory connectivity",
        "AWS Organizations with asset class OUs, AWS Lake Formation for client data governance, Amazon ElastiCache for portfolio caching, AWS Batch for risk calculations, and AWS Certificate Manager for secure communications",
        "AWS Organizations with jurisdiction-specific OUs, AWS Lake Formation for client data protection and regulatory compliance, Amazon ElastiCache for real-time risk assessment, AWS Lambda for algorithmic trading, and AWS CloudTrail with legal hold for regulatory audit trails",
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 60+ different financial regulations. Lake Formation provides client data protection and regulatory compliance governance. ElastiCache enables real-time portfolio risk assessment for $2T AUM. Lambda scales automatically for algorithmic trading. CloudTrail with legal hold provides regulatory audit trails for investment management.",
        whyWrong: {
            0: "Fund-based OUs don't address jurisdiction-specific regulatory requirements",
            1: "Control Tower doesn't address jurisdiction-specific investment regulations",
            2: "Asset class OUs don't address jurisdiction-specific regulatory compliance requirements"
        },
        examStrategy: "Investment management: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for compliance."
    }
},
{
    id: 'sap_342',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart waste management platform monitors 500,000+ waste bins across major cities using IoT sensors, optimizes collection routes, predicts bin fill levels, coordinates fleet management, and provides environmental impact analytics while reducing operational costs and carbon footprint.",
    question: "Which architecture provides OPTIMAL smart waste management at city scale?",
    options: [
        "AWS IoT Core for bin connectivity, Amazon MSK for high-volume data, Amazon EMR for route analytics, AWS Lambda for collection optimization, and Amazon S3 for environmental data storage",
        "Amazon EventBridge for waste events, AWS Batch for route calculations, Amazon DocumentDB for bin data, Amazon API Gateway for fleet integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for edge processing, Amazon DynamoDB for waste records, AWS Step Functions for collection workflows, Amazon Redshift for analytics, and Amazon QuickSight for environmental dashboards",
        "AWS IoT Core for sensor management, Amazon Kinesis Data Analytics for real-time bin monitoring, Amazon SageMaker for route optimization and fill prediction, AWS Lambda for fleet coordination, and Amazon Timestream for environmental metrics tracking",
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure, scalable management for 500,000+ waste bin sensors. Kinesis Data Analytics processes real-time bin fill level data. SageMaker enables sophisticated route optimization and fill level prediction models. Lambda coordinates fleet management automatically. Timestream tracks environmental impact metrics and carbon footprint optimization.",
        whyWrong: {
            0: "MSK adds complexity for waste sensor data, EMR is for batch analytics not real-time route optimization",
            1: "EventBridge and Batch add latency inappropriate for real-time waste collection optimization",
            2: "IoT Greengrass on every bin adds operational complexity, Step Functions add latency for collection coordination"
        },
        examStrategy: "Smart waste management: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for intelligent city waste operations."
    }
},
{
    id: 'sap_343',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global freight and logistics network coordinates 1 million shipments monthly, experiences tracking delays, has complex customs clearance issues, faces capacity optimization challenges across 10,000+ vehicles, and needs real-time visibility for customers while reducing operational costs.",
    question: "Which optimization strategy provides the BEST freight logistics performance and cost efficiency?",
    options: [
        "Implement Amazon ElastiCache for shipment caching, AWS Lambda for tracking updates, Amazon DynamoDB for logistics data, Amazon API Gateway for customer visibility, and Amazon SQS for customs processing",
        "Deploy Amazon Aurora Global Database for shipment records, Amazon MSK for real-time messaging, AWS Step Functions for logistics workflows, Amazon SageMaker for capacity optimization, and AWS Batch for customs processing",
        "Use Amazon DocumentDB for flexible logistics schemas, Amazon Kinesis for real-time updates, AWS Batch for route calculations, Amazon EventBridge for customs coordination, and Amazon S3 for shipment documentation",
        "Implement Amazon DynamoDB Global Tables for shipment tracking, Amazon Kinesis Data Streams for real-time logistics events, Amazon SageMaker for capacity optimization and route planning, AWS Lambda for customs automation, and Amazon API Gateway for customer visibility APIs",
    ],
    correct: 3,
    explanation: {
        correct: "DynamoDB Global Tables provides conflict-free shipment tracking for 1M monthly shipments across regions. Kinesis Data Streams handles real-time logistics events efficiently. SageMaker optimizes capacity across 10,000+ vehicles and improves route planning. Lambda automates customs clearance processes. API Gateway provides scalable customer visibility APIs.",
        whyWrong: {
            0: "Basic caching doesn't solve complex logistics optimization, SQS adds latency for customs processing",
            1: "Aurora Global Database has write limitations for high-frequency shipment updates, Batch adds latency for customs",
            2: "DocumentDB doesn't optimize for logistics tracking queries, Batch adds latency for real-time route optimization"
        },
        examStrategy: "Freight logistics: DynamoDB Global Tables + Kinesis + SageMaker + Lambda + API Gateway for optimal logistics performance."
    }
},
{
    id: 'sap_344',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A weather prediction agency needs to migrate their global weather forecasting models from supercomputing clusters to AWS, supporting real-time atmospheric simulations, ensemble forecasting, climate change research, and international meteorological collaboration while maintaining forecast accuracy.",
    question: "Which modernization approach provides OPTIMAL meteorological computing with global collaboration?",
    options: [
        "Deploy Amazon EMR with Spark clusters, implement Amazon S3 for weather data, use AWS Glue for data processing, Amazon Redshift for climate analytics, and AWS Direct Connect for international connectivity",
        "Implement Amazon ECS with GPU instances, use Amazon EFS for shared storage, AWS Step Functions for forecast workflows, Amazon DocumentDB for weather metadata, and AWS VPN for collaborative access",
        "Use AWS Fargate for containerized forecasting, Amazon DynamoDB for weather records, Amazon EventBridge for forecast coordination, AWS Lambda for data processing, and Amazon API Gateway for collaboration APIs",
        "Deploy AWS ParallelCluster with HPC-optimized instances for atmospheric modeling, Amazon FSx for Lustre for weather data processing, AWS Batch for ensemble forecasting, Amazon S3 with Cross-Region Replication for global collaboration, and AWS Lake Formation for meteorological data governance",
    ],
    correct: 3,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for atmospheric modeling. FSx Lustre handles massive weather datasets with parallel processing. Batch schedules ensemble forecasting efficiently. S3 with CRR enables global meteorological collaboration. Lake Formation provides data governance for international weather research sharing.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive atmospheric modeling computations",
            1: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for weather computing",
            2: "Fargate has resource limitations for weather modeling, Lambda has execution time limits for atmospheric simulations"
        },
        examStrategy: "Weather forecasting: ParallelCluster + HPC + FSx Lustre + Batch + Lake Formation for meteorological collaboration."
    }
},
{
    id: 'sap_345',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational telecommunications company operates networks in 80+ countries with different regulatory frameworks, needs centralized network monitoring, emergency response coordination, customer service integration, and compliance reporting while maintaining network independence during international incidents.",
    question: "Which governance architecture provides OPTIMAL telecommunications network resilience and compliance?",
    options: [
        "AWS Organizations with technology-based OUs, AWS Config for telecom compliance, AWS IoT Core for network monitoring, Amazon Kinesis for network data, and AWS Storage Gateway for network independence",
        "AWS Control Tower for network standardization, AWS Security Hub for compliance monitoring, Amazon MSK for network communication, AWS Lambda for network processing, and AWS Direct Connect for international connectivity",
        "AWS Organizations with service-based OUs, AWS Directory Service for technician authentication, Amazon ElastiCache for network caching, AWS Batch for network analysis, and AWS Certificate Manager for secure communications",
        "AWS Organizations with country-specific OUs for regulatory compliance, AWS IoT Core for network infrastructure monitoring, Amazon Kinesis Data Analytics for real-time network optimization, AWS Outposts for network independence, and AWS Config for telecommunications regulatory monitoring",
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 80+ different telecom regulations. IoT Core manages network equipment and infrastructure monitoring. Kinesis Data Analytics provides real-time network optimization and fault detection. Outposts enables network operations independence during international incidents. Config monitors telecommunications regulatory compliance continuously.",
        whyWrong: {
            0: "Technology-based OUs don't address country-specific regulatory requirements, Storage Gateway doesn't provide full independence",
            1: "Control Tower doesn't address country-specific telecommunications regulations",
            2: "Service-based OUs don't address country-specific regulatory compliance requirements"
        },
        examStrategy: "Telecom governance: Organizations + country OUs + IoT Core + Kinesis Analytics + Outposts + Config for network resilience."
    }
},
{
    id: 'sap_346',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A precision medicine platform analyzes genomic data from 1 million patients, performs drug discovery simulations, matches patients to clinical trials, predicts treatment outcomes, and enables secure research collaboration while maintaining strict patient privacy and regulatory compliance.",
    question: "Which architecture provides OPTIMAL precision medicine capabilities with comprehensive privacy protection?",
    options: [
        "Amazon EC2 with GPU instances for genomic analysis, Amazon S3 for patient data, AWS Lambda for drug discovery, Amazon DynamoDB for medical records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for clinical coordination, Amazon DocumentDB for patient metadata, and AWS VPN for researcher access",
        "Amazon SageMaker for drug discovery ML, AWS ParallelCluster for genomic processing, Amazon Kinesis for real-time patient data, Amazon FSx for research data, and AWS Lake Formation for privacy protection",
        "Amazon SageMaker for ML-driven drug discovery and treatment prediction, AWS ParallelCluster for intensive genomic analysis, AWS Lake Formation for patient privacy governance and HIPAA compliance, Amazon HealthLake for medical data management, and AWS PrivateLink for secure research collaboration",
    ],
    correct: 3,
    explanation: {
        correct: "SageMaker provides AI/ML for drug discovery and treatment prediction. ParallelCluster delivers HPC performance for genomic analysis of 1M patients. Lake Formation ensures patient privacy and HIPAA compliance. HealthLake provides purpose-built medical data management and clinical trial matching. PrivateLink enables secure research collaboration while protecting patient data.",
        whyWrong: {
            0: "Standard EC2 doesn't provide ML capabilities for drug discovery, basic IAM doesn't provide patient privacy protection",
            1: "Batch alone doesn't provide ML capabilities, EFS doesn't provide privacy protection for patient data",
            2: "Missing comprehensive medical data management and HIPAA compliance needed for precision medicine"
        },
        examStrategy: "Precision medicine: SageMaker + ParallelCluster + Lake Formation + HealthLake + PrivateLink for privacy-protected medical research."
    }
},
{
    id: 'sap_347',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global food delivery platform serves 100 million customers across 1,000+ cities, experiences delivery time prediction errors, has driver-restaurant coordination issues, faces surge pricing calculation delays, and needs real-time order tracking optimization while reducing operational costs.",
    question: "Which optimization strategy provides the BEST food delivery platform performance and cost efficiency?",
    options: [
        "Implement Amazon ElastiCache for restaurant caching, AWS Lambda for delivery processing, Amazon DynamoDB for order data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for order records, Amazon MSK for real-time messaging, AWS Step Functions for delivery workflows, Amazon SageMaker for delivery predictions, and AWS Batch for surge calculations",
        "Use Amazon DocumentDB for flexible order schemas, Amazon Kinesis for real-time updates, AWS Batch for route calculations, Amazon EventBridge for coordination, and Amazon S3 for tracking data",
        "Implement Amazon ElastiCache Redis cluster mode for real-time location data, Amazon SageMaker for delivery time prediction and surge pricing, Amazon DynamoDB Global Tables for order state management, Amazon Kinesis for location streams, and AWS Lambda for driver-restaurant coordination",
    ],
    correct: 3,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond location access for real-time tracking. SageMaker provides accurate delivery time prediction and dynamic surge pricing calculations. DynamoDB Global Tables manages order state across 1,000+ cities. Kinesis handles real-time location streams efficiently. Lambda coordinates driver-restaurant communication automatically.",
        whyWrong: {
            0: "Basic caching doesn't solve delivery prediction accuracy, CloudWatch doesn't provide real-time optimization",
            1: "Aurora Global Database has write limitations for high-frequency location updates, Batch adds latency for surge pricing",
            2: "DocumentDB doesn't optimize for food delivery patterns, Batch adds latency for real-time route optimization"
        },
        examStrategy: "Food delivery: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for optimal delivery performance."
    }
},
{
    id: 'sap_348',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national intelligence agency needs to migrate their threat analysis and cybersecurity monitoring systems from air-gapped networks to AWS, maintaining the highest classification levels, enabling real-time threat detection, supporting inter-agency collaboration, and ensuring absolute security isolation.",
    question: "Which modernization approach ensures MAXIMUM security for classified intelligence systems?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for intelligence data, use AWS Lambda for threat analysis, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for threat records, AWS Batch for analysis workflows, Amazon S3 for intelligence data, and AWS Config for classification compliance",
        "Use AWS Outposts in secure facilities, Amazon FSx for intelligence data, AWS Step Functions for analysis workflows, Amazon ElastiCache for performance, and AWS Direct Connect for inter-agency connectivity",
        "Deploy AWS GovCloud with classification-isolated VPCs and AWS PrivateLink, AWS ParallelCluster for threat analysis computing, AWS Lake Formation for intelligence data governance, Amazon Kinesis Data Analytics for real-time threat detection, and AWS CloudTrail with tamper detection for audit compliance",
    ],
    correct: 3,
    explanation: {
        correct: "GovCloud with classification-isolated VPCs provides maximum security separation. ParallelCluster delivers HPC performance for threat analysis. Lake Formation provides strict intelligence data governance. Kinesis Data Analytics enables real-time threat detection. CloudTrail with tamper detection provides audit compliance for intelligence operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide classification-level security isolation needed",
            1: "Control Tower doesn't provide classification-isolated security needed for intelligence",
            2: "Outposts requires on-premises management and doesn't provide the security isolation needed"
        },
        examStrategy: "Intelligence systems: GovCloud isolated VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection."
    }
},
{
    id: 'sap_349',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global humanitarian organization coordinates disaster relief across 100+ countries, needs rapid deployment capabilities, secure communication in crisis zones, resource coordination, donor transparency, and compliance with international aid regulations while maintaining operational continuity during emergencies.",
    question: "Which governance architecture ensures COMPREHENSIVE humanitarian operations and crisis response?",
    options: [
        "AWS Organizations with program-based OUs, AWS Config for aid compliance, AWS IoT Core for field monitoring, Amazon Kinesis for crisis data, and AWS Storage Gateway for operational continuity",
        "AWS Control Tower for humanitarian standardization, AWS Security Hub for compliance monitoring, Amazon MSK for crisis communication, AWS Lambda for resource coordination, and AWS Direct Connect for field connectivity",
        "AWS Organizations with regional OUs, AWS Lake Formation for donor transparency, Amazon ElastiCache for resource caching, AWS Batch for aid distribution analysis, and AWS Certificate Manager for secure communications",
        "AWS Organizations with country-specific OUs for regulatory compliance, AWS Outposts for rapid crisis deployment, AWS Lake Formation for donor transparency and resource governance, Amazon Kinesis Data Analytics for real-time crisis coordination, and AWS Satellite for communication in remote areas",
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 100+ different aid regulations. Outposts enables rapid deployment in crisis zones with local AWS infrastructure. Lake Formation provides donor transparency and resource governance. Kinesis Data Analytics coordinates real-time crisis response. AWS Satellite ensures communication in remote disaster areas.",
        whyWrong: {
            0: "Program-based OUs don't address country-specific aid regulations, Storage Gateway doesn't provide crisis deployment capabilities",
            1: "Control Tower doesn't address country-specific humanitarian regulations, MSK doesn't provide crisis zone communication",
            2: "Regional OUs don't provide the granular country compliance needed for aid regulations"
        },
        examStrategy: "Humanitarian operations: Organizations + country OUs + Outposts + Lake Formation + Kinesis Analytics + Satellite for crisis response."
    }
},
{
    id: 'sap_350',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A carbon capture and storage monitoring platform tracks 1,000+ industrial facilities globally, measures emissions in real-time, verifies carbon offset credits, coordinates with regulatory bodies, and provides transparent reporting while optimizing capture efficiency and environmental impact.",
    question: "Which architecture provides OPTIMAL carbon monitoring and environmental compliance?",
    options: [
        "AWS IoT Core for facility connectivity, Amazon MSK for high-volume emissions data, Amazon EMR for carbon analytics, AWS Lambda for offset processing, and Amazon S3 for environmental reporting",
        "Amazon EventBridge for facility events, AWS Batch for emissions calculations, Amazon DocumentDB for carbon data, Amazon API Gateway for regulatory integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for facility edge processing, Amazon DynamoDB for carbon records, AWS Step Functions for offset workflows, Amazon Redshift for analytics, and Amazon QuickSight for environmental dashboards",
        "AWS IoT Core for emissions sensor management, Amazon Kinesis Data Analytics for real-time carbon monitoring, Amazon SageMaker for capture optimization, AWS Lambda for offset credit verification, and Amazon Timestream for environmental metrics with AWS Lake Formation for regulatory transparency",
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure management for emissions sensors across 1,000+ facilities. Kinesis Data Analytics monitors carbon capture in real-time. SageMaker optimizes capture efficiency and predicts environmental impact. Lambda automates carbon offset credit verification. Timestream tracks environmental metrics, and Lake Formation ensures regulatory transparency and compliance.",
        whyWrong: {
            0: "MSK adds complexity for emissions sensor data, EMR is for batch analytics not real-time carbon monitoring",
            1: "EventBridge and Batch add latency inappropriate for real-time emissions monitoring",
            2: "IoT Greengrass on every facility adds operational complexity, Step Functions add latency for real-time optimization"
        },
        examStrategy: "Carbon monitoring: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream + Lake Formation for environmental compliance."
    }
},

// SAP-C02 Questions 351-400 (Batch 8) - 50 Questions - PROPERLY RANDOMIZED ANSWERS
// Distribution: 12 A's, 13 B's, 12 C's, 13 D's

// Questions 351-362: correct: 0 (A)
{
    id: 'sap_351',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global cryptocurrency trading platform processes 2 million transactions per second, experiences slippage during high volatility, has liquidity fragmentation across exchanges, faces regulatory compliance challenges in 50+ jurisdictions, and needs sub-millisecond order execution while maintaining audit trails.",
    question: "Which optimization strategy provides the BEST cryptocurrency trading performance with regulatory compliance?",
    options: [
        "Implement Amazon ElastiCache Redis cluster mode for order books, AWS Lambda for trade execution, Amazon DynamoDB Global Tables for transaction records, Amazon Kinesis for real-time market data, and AWS Config for multi-jurisdiction compliance monitoring",
        "Deploy Amazon MemoryDB for trading state, Amazon MSK for high-throughput messaging, AWS Step Functions for trading workflows, Amazon Aurora Global for transaction history, and AWS Security Hub for compliance dashboards",
        "Use Amazon DocumentDB for flexible trading schemas, Amazon EventBridge for market events, AWS Batch for liquidity calculations, Amazon RDS for transaction storage, and AWS CloudTrail for regulatory auditing",
        "Implement Amazon Redshift for trading analytics, Amazon SQS for order queuing, AWS Glue for data processing, Amazon EMR for market analysis, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 2M TPS with automatic sharding. Lambda scales instantly for trade execution. DynamoDB Global Tables handles global transaction records with automatic scaling. Kinesis processes real-time market data efficiently. Config monitors compliance across 50+ jurisdictions continuously.",
        whyWrong: {
            1: "MemoryDB has higher latency than ElastiCache cluster mode for ultra-high frequency trading requirements",
            2: "DocumentDB doesn't optimize for trading performance, EventBridge adds latency for real-time order execution",
            3: "Redshift is for analytics not real-time trading, SQS adds latency violating sub-millisecond requirements"
        },
        examStrategy: "Crypto trading optimization: ElastiCache cluster + Lambda + DynamoDB Global Tables + Kinesis + Config for regulatory compliance."
    }
},
{
    id: 'sap_352',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational pharmaceutical company conducts clinical trials across 90+ countries with varying drug approval regulations, needs patient data protection, research collaboration security, FDA/EMA submission requirements, and intellectual property protection while enabling global research coordination.",
    question: "Which governance architecture ensures COMPREHENSIVE pharmaceutical research compliance and IP protection?",
    options: [
        "AWS Organizations with regulatory jurisdiction OUs, AWS Lake Formation for patient data governance and IP protection, AWS PrivateLink for secure research collaboration, AWS CloudTrail with tamper detection for regulatory audit trails, and AWS Config for continuous FDA/EMA compliance monitoring",
        "AWS Control Tower for research standardization, AWS Security Hub for compliance monitoring, Amazon S3 for research data storage, AWS Lambda for data processing, and AWS Direct Connect for institutional connectivity",
        "AWS Organizations with therapeutic area OUs, AWS Directory Service for researcher authentication, Amazon ElastiCache for research caching, AWS Batch for trial analysis, and AWS Certificate Manager for secure communications",
        "AWS Landing Zone with pharma templates, AWS GuardDuty for threat detection, Amazon Redshift for trial analytics, AWS Glue for data integration, and Amazon QuickSight for research dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with regulatory jurisdiction OUs ensure compliance with 90+ different drug approval regulations. Lake Formation provides patient data governance and IP protection through fine-grained access controls. PrivateLink enables secure research collaboration without internet exposure. CloudTrail with tamper detection provides FDA/EMA-compliant audit trails. Config ensures continuous regulatory compliance monitoring.",
        whyWrong: {
            1: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide IP protection",
            2: "Therapeutic area OUs don't address regulatory jurisdiction requirements, Directory Service doesn't provide IP governance",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the regulatory compliance framework needed"
        },
        examStrategy: "Pharmaceutical governance: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail tamper detection for IP protection."
    }
},
{
    id: 'sap_353',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An advanced robotics platform coordinates 100,000+ autonomous robots across manufacturing, logistics, and service industries, requiring real-time coordination, collision avoidance, task optimization, machine learning adaptation, and fleet management while ensuring safety protocols and operational efficiency.",
    question: "Which architecture provides OPTIMAL autonomous robotics coordination at industrial scale?",
    options: [
        "AWS IoT Core for robot connectivity, Amazon Kinesis Data Streams for real-time telemetry, Amazon SageMaker for ML-driven task optimization, AWS Lambda for collision avoidance algorithms, and Amazon Timestream for robotics performance metrics",
        "Amazon EventBridge for robot coordination, AWS Batch for task calculations, Amazon DocumentDB for robot data, Amazon API Gateway for fleet integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for robot edge computing, Amazon MSK for high-throughput messaging, Amazon EMR for fleet analytics, AWS Step Functions for task workflows, and Amazon S3 for operational data",
        "Amazon EC2 Auto Scaling for robot management, Amazon ElastiCache for coordination caching, Amazon RDS for robot records, AWS Direct Connect for fleet connectivity, and Amazon QuickSight for fleet dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure, scalable connectivity for 100,000+ robots with device management. Kinesis Data Streams handles real-time robot telemetry efficiently. SageMaker enables ML-driven task optimization and adaptation. Lambda provides real-time collision avoidance with automatic scaling. Timestream optimizes robotics performance metrics and safety monitoring.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time robot coordination and collision avoidance",
            2: "IoT Greengrass on every robot adds operational complexity, MSK is overkill for robot communication",
            3: "EC2 Auto Scaling doesn't provide robot-specific connectivity, RDS doesn't optimize for real-time robot coordination"
        },
        examStrategy: "Autonomous robotics: IoT Core + Kinesis + SageMaker + Lambda + Timestream for industrial robot coordination."
    }
},
{
    id: 'sap_354',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national weather service needs to migrate their hurricane prediction models from legacy supercomputers to AWS, requiring real-time atmospheric simulations, ensemble forecasting, public safety alerting, and international meteorological data sharing while maintaining forecast accuracy for emergency management.",
    question: "Which modernization approach provides OPTIMAL weather prediction with emergency response capabilities?",
    options: [
        "Deploy AWS ParallelCluster with HPC-optimized instances for atmospheric modeling, Amazon FSx for Lustre for weather data processing, AWS Batch for ensemble forecasting, Amazon SNS for emergency alerts, and Amazon S3 with Cross-Region Replication for international data sharing",
        "Implement Amazon EMR with Spark clusters, Amazon Redshift for weather data warehousing, AWS Glue for data processing, Amazon QuickSight for forecast visualization, and AWS Direct Connect for international collaboration",
        "Use Amazon ECS with GPU instances, Amazon EFS for shared storage, AWS Step Functions for forecast workflows, Amazon DocumentDB for weather metadata, and AWS VPN for collaborative access",
        "Deploy AWS Fargate for containerized forecasting, Amazon DynamoDB for weather records, Amazon EventBridge for forecast coordination, AWS Lambda for alert processing, and Amazon API Gateway for international APIs"
    ],
    correct: 0,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for hurricane modeling. FSx Lustre handles massive weather datasets with parallel processing. Batch schedules ensemble forecasting efficiently. SNS delivers emergency alerts to public safety systems. S3 with CRR enables international meteorological data sharing.",
        whyWrong: {
            1: "EMR is optimized for big data analytics, not intensive atmospheric modeling computations",
            2: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for weather computing",
            3: "Fargate has resource limitations for weather modeling, Lambda has execution time limits for complex forecasting"
        },
        examStrategy: "Weather prediction: ParallelCluster + HPC + FSx Lustre + Batch + SNS for emergency weather forecasting."
    }
},
{
    id: 'sap_355',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online education platform serves 50 million students, experiences video streaming issues during peak hours, has content delivery delays, faces assessment processing bottlenecks, and needs real-time collaboration features while optimizing bandwidth usage for emerging markets.",
    question: "Which optimization strategy provides the BEST educational platform performance and global accessibility?",
    options: [
        "Implement Amazon CloudFront with origin shield for video delivery, AWS Elemental MediaPackage for adaptive streaming, Amazon DynamoDB for student progress tracking, AWS Lambda for assessment processing, and Amazon Chime SDK for real-time collaboration",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for video processing, Amazon API Gateway for student access, and Amazon ElastiCache for session management",
        "Use Amazon S3 Transfer Acceleration, Amazon ECS for video processing, AWS Step Functions for assessment workflows, Amazon EventBridge for collaboration coordination, and Amazon CloudWatch for monitoring",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Lambda for video processing, Amazon SQS for assessment queuing, and Amazon Connect for student support"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFront with origin shield optimizes video delivery globally and reduces bandwidth costs. MediaPackage provides adaptive streaming for emerging markets with limited bandwidth. DynamoDB scales for 50M student progress tracking. Lambda handles assessment processing with automatic scaling. Chime SDK enables real-time collaboration features.",
        whyWrong: {
            1: "Global Accelerator doesn't provide video streaming optimization, EFS doesn't optimize for educational content delivery",
            2: "Transfer Acceleration doesn't provide video streaming capabilities, Step Functions add latency for real-time assessments",
            3: "Direct Connect is expensive for global content delivery, Glacier has retrieval delays inappropriate for active content"
        },
        examStrategy: "Education platform: CloudFront + origin shield + MediaPackage + DynamoDB + Lambda + Chime SDK for global learning."
    }
},
{
    id: 'sap_356',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global defense contractor manages classified weapons systems across 25+ military programs, requires strict security clearance access controls, maintains air-gapped development environments, enables secure multi-national collaboration, and ensures ITAR compliance while supporting rapid prototyping and testing.",
    question: "Which governance framework ensures MAXIMUM security for classified defense systems development?",
    options: [
        "AWS GovCloud with program-isolated VPCs per classification level, AWS Organizations with ITAR-compliant SCPs, AWS Lake Formation for classified data governance, AWS PrivateLink for secure multi-national collaboration, and AWS CloudTrail with tamper detection for defense audit compliance",
        "AWS Control Tower in commercial regions, AWS Config for ITAR monitoring, AWS PrivateLink for program communication, AWS Backup for data protection, and AWS Client VPN for contractor access",
        "AWS Organizations with contractor-based accounts, AWS Security Hub for clearance monitoring, AWS Direct Connect for government connectivity, AWS KMS for encryption, and Amazon WorkSpaces for secure development",
        "AWS Landing Zone with defense templates, AWS Directory Service for clearance authentication, AWS GuardDuty for threat detection, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 0,
    explanation: {
        correct: "GovCloud with program-isolated VPCs ensures proper classification separation for 25+ programs. Organizations with ITAR-compliant SCPs enforce export control restrictions automatically. Lake Formation provides classified data governance with fine-grained access controls. PrivateLink enables secure multi-national collaboration without internet exposure. CloudTrail with tamper detection provides defense-compliant audit trails.",
        whyWrong: {
            1: "Commercial regions don't meet ITAR requirements for classified weapons systems development",
            2: "Contractor-based accounts don't provide classification-level security isolation needed",
            3: "Landing Zone is deprecated, Directory Service alone doesn't provide the security controls needed for classified development"
        },
        examStrategy: "Defense systems: GovCloud + program-isolated VPCs + ITAR SCPs + Lake Formation + PrivateLink for classified development."
    }
},
{
    id: 'sap_357',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart city energy grid manages 500,000+ IoT devices across power generation, distribution, and consumption, optimizes renewable energy integration, predicts demand patterns, manages grid stability, and provides real-time energy trading while ensuring grid security and resilience.",
    question: "Which architecture provides OPTIMAL smart grid management and energy optimization?",
    options: [
        "AWS IoT Core for grid device management, Amazon Kinesis Data Analytics for real-time energy optimization, Amazon SageMaker for demand prediction, AWS Lambda for grid stability management, and Amazon Timestream for energy metrics tracking",
        "Amazon EventBridge for grid events, AWS Batch for energy calculations, Amazon DocumentDB for device data, Amazon API Gateway for utility integration, and Amazon CloudWatch for grid monitoring",
        "AWS IoT Greengrass for grid edge processing, Amazon MSK for high-volume messaging, Amazon EMR for energy analytics, AWS Step Functions for grid coordination, and Amazon S3 for energy data storage",
        "Amazon EC2 Auto Scaling for grid processing, Amazon ElastiCache for energy caching, Amazon RDS for grid records, AWS Direct Connect for utility connectivity, and Amazon QuickSight for energy dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure management for 500,000+ grid devices with security controls. Kinesis Data Analytics enables real-time energy optimization and renewable integration. SageMaker predicts demand patterns and optimizes energy trading. Lambda manages grid stability with automatic scaling. Timestream tracks energy metrics and grid performance efficiently.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time grid stability management",
            2: "IoT Greengrass on every grid device adds operational complexity, MSK is overkill for grid communication",
            3: "EC2 Auto Scaling doesn't provide grid-specific device management, RDS doesn't optimize for real-time energy data"
        },
        examStrategy: "Smart grid: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for intelligent energy management."
    }
},
{
    id: 'sap_358',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A space exploration agency needs to migrate their mission planning and spacecraft control systems from ground-based infrastructure to AWS, supporting deep space missions, real-time telemetry from Mars rovers, mission-critical decision making, and international space collaboration while maintaining zero-downtime requirements.",
    question: "Which modernization strategy ensures MAXIMUM reliability for space mission operations?",
    options: [
        "Deploy AWS Ground Station for spacecraft communication, Amazon Kinesis Data Streams for real-time telemetry, AWS Lambda for mission-critical processing with provisioned concurrency, Amazon DynamoDB Global Tables for mission state, and AWS Auto Scaling with multi-region failover for zero-downtime requirements",
        "Implement Amazon EC2 in multiple regions, Amazon RDS Aurora for mission data, AWS Step Functions for mission workflows, Amazon API Gateway for ground station integration, and AWS CloudFormation for infrastructure automation",
        "Use AWS Outposts for mission control rooms, Amazon MSK for telemetry streams, AWS Batch for mission calculations, Amazon ElastiCache for performance, and AWS Direct Connect for ground station connectivity",
        "Deploy AWS Wavelength for low-latency processing, Amazon EventBridge for mission coordination, Amazon DocumentDB for spacecraft data, Amazon Redshift for mission analytics, and AWS Transit Gateway for ground station networking"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Ground Station provides managed spacecraft communication infrastructure. Kinesis Data Streams handles real-time telemetry from Mars rovers with automatic scaling. Lambda with provisioned concurrency ensures mission-critical processing without cold starts. DynamoDB Global Tables provides mission state availability across regions. Auto Scaling with multi-region failover ensures zero-downtime requirements.",
        whyWrong: {
            1: "Standard EC2 deployment doesn't provide spacecraft communication capabilities, Step Functions add latency for mission-critical decisions",
            2: "Outposts requires on-premises infrastructure management, Batch adds latency for real-time telemetry processing",
            3: "Wavelength is for mobile edge computing not space missions, EventBridge adds latency for mission-critical operations"
        },
        examStrategy: "Space missions: Ground Station + Kinesis + Lambda provisioned + DynamoDB Global Tables + Auto Scaling for mission-critical reliability."
    }
},
{
    id: 'sap_359',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial trading firm's algorithmic trading platform processes 10 million orders per second, experiences latency spikes during market volatility, has risk management calculation delays, faces regulatory reporting bottlenecks across 40+ jurisdictions, and needs microsecond-level trade execution optimization.",
    question: "Which optimization strategy provides the BEST ultra-high frequency trading performance with regulatory compliance?",
    options: [
        "Implement Amazon ElastiCache Redis cluster mode for order books, AWS Lambda with provisioned concurrency for trade execution, Amazon Kinesis Data Streams for market data, Amazon SageMaker for risk calculations, and AWS Config for multi-jurisdiction regulatory monitoring",
        "Deploy Amazon MemoryDB for trading state persistence, AWS Batch for risk processing, Amazon Aurora Global for trade records, Amazon EventBridge for market events, and AWS Security Hub for compliance dashboards",
        "Use AWS Local Zones for ultra-low latency, Amazon DocumentDB for flexible trading schemas, AWS Step Functions for trade workflows, Amazon MSK for market messaging, and AWS CloudTrail for regulatory auditing",
        "Implement AWS Outposts for on-premises performance, Amazon RDS with read replicas, AWS Glue for regulatory reporting, Amazon EMR for market analysis, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 10M orders/second with automatic sharding. Lambda with provisioned concurrency eliminates cold starts for microsecond-level execution. Kinesis Data Streams handles high-volume market data efficiently. SageMaker provides real-time risk calculations. Config monitors regulatory compliance across 40+ jurisdictions continuously.",
        whyWrong: {
            1: "MemoryDB has higher latency than ElastiCache cluster mode, Batch adds latency for real-time risk calculations",
            2: "Local Zones alone don't provide the caching and processing optimization needed, Step Functions add latency",
            3: "Outposts requires infrastructure management, RDS with read replicas can't handle 10M orders/second write volume"
        },
        examStrategy: "Ultra-high frequency trading: ElastiCache cluster + Lambda provisioned + Kinesis + SageMaker + Config for microsecond optimization."
    }
},
{
    id: 'sap_360',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational mining corporation operates in 60+ countries with varying environmental regulations, needs real-time equipment monitoring, environmental compliance reporting, worker safety management, and operational coordination while maintaining independence during connectivity disruptions in remote locations.",
    question: "Which governance architecture provides OPTIMAL mining operations with environmental compliance and safety?",
    options: [
        "AWS Organizations with country-specific OUs for environmental compliance, AWS IoT Core for equipment and safety monitoring, Amazon Kinesis Data Analytics for real-time operational optimization, AWS Outposts for remote site independence, and AWS Config for continuous environmental regulatory monitoring",
        "AWS Control Tower for mining standardization, AWS Security Hub for compliance monitoring, Amazon MSK for equipment communication, AWS Lambda for safety processing, and AWS Direct Connect for site connectivity",
        "AWS Organizations with commodity-based OUs, AWS Directory Service for worker authentication, Amazon ElastiCache for operations caching, AWS Batch for environmental analysis, and AWS Certificate Manager for secure communications",
        "AWS Landing Zone with mining templates, AWS GuardDuty for site security, Amazon Redshift for operations analytics, AWS Glue for compliance data integration, and Amazon QuickSight for safety dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 60+ different environmental regulations. IoT Core manages equipment and safety monitoring systems securely. Kinesis Data Analytics provides real-time operational optimization and safety analysis. Outposts enables remote mining sites to operate independently during connectivity disruptions. Config monitors environmental regulatory compliance continuously.",
        whyWrong: {
            1: "Control Tower doesn't address country-specific environmental regulations, MSK adds complexity for mining equipment communication",
            2: "Commodity-based OUs don't address environmental regulatory requirements, Directory Service doesn't provide environmental governance",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the environmental compliance framework needed"
        },
        examStrategy: "Mining governance: Organizations + country OUs + IoT Core + Kinesis Analytics + Outposts + Config for environmental compliance."
    }
},
{
    id: 'sap_361',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A molecular drug discovery platform enables 5,000+ researchers to simulate protein interactions, design new drug compounds, predict molecular behavior, manage clinical trial data, and collaborate on breakthrough discoveries while protecting intellectual property and ensuring regulatory compliance.",
    question: "Which architecture provides OPTIMAL drug discovery research with comprehensive IP protection?",
    options: [
        "Amazon SageMaker for AI-driven drug discovery, AWS ParallelCluster for molecular simulations, AWS Lake Formation for IP protection and research governance, Amazon FSx for Lustre for computational data, and AWS PrivateLink for secure research collaboration",
        "Amazon EC2 with GPU instances for molecular modeling, Amazon S3 for research data, AWS Lambda for compound analysis, Amazon DynamoDB for research records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for discovery coordination, Amazon DocumentDB for compound metadata, and AWS VPN for researcher access",
        "Amazon EMR for drug analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional collaboration"
    ],
    correct: 0,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for drug discovery and compound design. ParallelCluster delivers HPC performance for molecular simulations. Lake Formation ensures IP protection through fine-grained access controls and regulatory compliance. FSx Lustre provides high-performance storage for computational workloads. PrivateLink enables secure research collaboration while protecting IP.",
        whyWrong: {
            1: "Standard EC2 doesn't provide AI/ML drug discovery capabilities, basic IAM doesn't provide IP protection needed",
            2: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide IP protection for research data",
            3: "EMR doesn't provide AI/ML or HPC capabilities for drug discovery, Redshift is for traditional analytics"
        },
        examStrategy: "Drug discovery: SageMaker + ParallelCluster + Lake Formation + FSx Lustre + PrivateLink for AI-driven IP-protected research."
    }
},
{
    id: 'sap_362',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national tax administration needs to migrate their taxpayer processing systems from legacy mainframes to AWS, handling 200 million tax returns annually, ensuring data privacy, maintaining audit trails, supporting real-time fraud detection, and enabling citizen self-service while meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL taxpayer services with government security compliance?",
    options: [
        "Deploy AWS Control Tower in GovCloud, Amazon DynamoDB for taxpayer records, AWS Lambda for tax processing, Amazon Kinesis Data Analytics for fraud detection, and AWS Config for government security compliance monitoring",
        "Implement AWS Mainframe Modernization for legacy system migration, Amazon RDS for tax data, AWS Step Functions for processing workflows, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Use AWS Elastic Beanstalk for tax applications, Amazon Aurora for taxpayer database, AWS Batch for return processing, Amazon CloudFront for citizen portal, and AWS Security Hub for compliance management",
        "Deploy Amazon EKS for containerized services, Amazon DocumentDB for flexible tax data, AWS EventBridge for processing coordination, AWS Lambda for microservices, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant landing zones and governance. DynamoDB scales to handle 200M tax returns with high availability. Lambda provides serverless tax processing with automatic scaling. Kinesis Data Analytics enables real-time fraud detection. Config ensures continuous government security compliance monitoring.",
        whyWrong: {
            1: "Mainframe Modernization alone doesn't provide the scale needed for 200M returns, Step Functions add latency for high-volume processing",
            2: "Elastic Beanstalk doesn't provide government security controls, Aurora may not scale to 200M taxpayer records efficiently",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for tax processing patterns"
        },
        examStrategy: "Government tax systems: Control Tower GovCloud + DynamoDB + Lambda + Kinesis Analytics + Config for secure citizen services."
    }
},

// Questions 363-375: correct: 1 (B)
{
    id: 'sap_363',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global streaming music platform serves 300 million users, experiences audio quality issues during peak hours, has music recommendation delays, faces content licensing complexities across regions, and needs real-time playlist synchronization while optimizing bandwidth costs for mobile users.",
    question: "Which optimization strategy provides the BEST music streaming performance and cost efficiency?",
    options: [
        "Implement Amazon CloudFront for audio delivery, AWS Lambda for recommendation processing, Amazon DynamoDB for user preferences, Amazon API Gateway for mobile apps, and Amazon S3 for music storage",
        "Deploy Amazon CloudFront with Lambda@Edge for adaptive audio streaming, Amazon SageMaker for real-time music recommendations, Amazon DynamoDB Global Tables for playlist synchronization, AWS Elemental MediaConvert for audio optimization, and Amazon ElastiCache for user session management",
        "Use AWS Global Accelerator for performance, Amazon EFS for music storage, AWS Batch for recommendation processing, Amazon EventBridge for playlist coordination, and Amazon CloudWatch for monitoring",
        "Implement AWS Direct Connect for audio delivery, Amazon Glacier for music archival, AWS Step Functions for recommendation workflows, Amazon DocumentDB for user data, and Amazon Route 53 for traffic routing"
    ],
    correct: 1,
    explanation: {
        correct: "CloudFront with Lambda@Edge enables adaptive audio streaming based on bandwidth conditions, reducing costs for mobile users. SageMaker provides real-time music recommendations with low latency. DynamoDB Global Tables ensures playlist synchronization across regions. MediaConvert optimizes audio for different devices and bandwidth. ElastiCache provides fast user session access.",
        whyWrong: {
            0: "Basic Lambda processing doesn't provide real-time recommendation capabilities, lacks adaptive streaming for bandwidth optimization",
            2: "Global Accelerator doesn't provide audio streaming optimization, EFS doesn't optimize for music content delivery",
            3: "Direct Connect is expensive for global music delivery, Glacier has retrieval delays inappropriate for active music content"
        },
        examStrategy: "Music streaming: CloudFront Lambda@Edge + SageMaker + DynamoDB Global Tables + MediaConvert + ElastiCache for optimal audio delivery."
    }
},
{
    id: 'sap_364',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global aerospace manufacturer designs aircraft across 15+ international programs, requires ITAR compliance for military projects, manages proprietary design data, enables secure supplier collaboration, and maintains strict export control while supporting concurrent engineering across time zones.",
    question: "Which governance framework ensures COMPREHENSIVE aerospace design security with international collaboration?",
    options: [
        "AWS Organizations with program-based OUs, AWS Config for ITAR monitoring, AWS PrivateLink for supplier access, AWS CloudTrail for design audit trails, and AWS WorkSpaces for secure engineering collaboration",
        "AWS Organizations with ITAR classification OUs, AWS Lake Formation for design data governance and export control, AWS PrivateLink for secure supplier collaboration, AWS GovCloud for military programs, and AWS CloudTrail with tamper detection for compliance audit trails",
        "AWS Control Tower for aerospace standardization, AWS Security Hub for compliance monitoring, Amazon S3 for design storage, AWS Lambda for data processing, and AWS Direct Connect for supplier connectivity",
        "AWS Landing Zone with aerospace templates, AWS Directory Service for engineer authentication, Amazon ElastiCache for design caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with ITAR classification OUs ensure proper export control separation. Lake Formation provides design data governance with fine-grained access controls for proprietary data. PrivateLink enables secure supplier collaboration without internet exposure. GovCloud provides ITAR-compliant infrastructure for military programs. CloudTrail with tamper detection ensures compliance audit trails.",
        whyWrong: {
            0: "Program-based OUs don't address ITAR classification requirements, basic WorkSpaces doesn't provide export control protection",
            2: "Control Tower doesn't address ITAR classification requirements, basic S3 doesn't provide export control governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the export control governance needed for aerospace design"
        },
        examStrategy: "Aerospace governance: Organizations + ITAR OUs + Lake Formation + PrivateLink + GovCloud + CloudTrail tamper detection."
    }
},
{
    id: 'sap_365',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A precision agriculture research platform analyzes crop genetics, soil composition, weather patterns, and farming techniques across 10,000+ research plots globally, optimizes yield predictions, manages field experiments, and enables agricultural research collaboration while protecting proprietary seed data.",
    question: "Which architecture provides OPTIMAL agricultural research with proprietary data protection?",
    options: [
        "AWS IoT Core for field sensor management, Amazon MSK for high-volume agricultural data, Amazon EMR for crop analytics, AWS Lambda for yield processing, and Amazon S3 for research data storage",
        "Amazon Kinesis Data Streams for sensor ingestion, Amazon SageMaker for yield prediction and genetic analysis, AWS Lake Formation for proprietary seed data protection, AWS IoT Core for field device management, and Amazon Timestream for agricultural time-series data",
        "Amazon EventBridge for field events, AWS Batch for genetic calculations, Amazon DocumentDB for crop data, Amazon API Gateway for research integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for field edge processing, Amazon DynamoDB for crop records, AWS Step Functions for research workflows, Amazon Redshift for analytics, and Amazon QuickSight for agricultural dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles sensor data from 10,000+ research plots. SageMaker provides sophisticated yield prediction and genetic analysis capabilities. Lake Formation protects proprietary seed data through fine-grained access controls. IoT Core manages field devices securely. Timestream optimizes agricultural time-series data storage and analysis.",
        whyWrong: {
            0: "MSK adds complexity for agricultural sensor data, EMR is for batch analytics not real-time agricultural optimization",
            2: "EventBridge and Batch add latency inappropriate for real-time agricultural monitoring",
            3: "IoT Greengrass on every plot adds operational complexity, Step Functions add latency for research coordination"
        },
        examStrategy: "Agricultural research: Kinesis + SageMaker + Lake Formation + IoT Core + Timestream for intelligent farming research."
    }
},
{
    id: 'sap_366',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A central bank needs to migrate their monetary policy modeling and financial stability monitoring systems from legacy infrastructure to AWS, supporting real-time economic analysis, international coordination, regulatory reporting, and maintaining the highest security standards for national financial data.",
    question: "Which modernization approach ensures MAXIMUM security for central banking operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for financial data, use AWS Lambda for economic analysis, Amazon API Gateway for international coordination, and AWS CloudTrail for regulatory audit compliance",
        "Deploy AWS GovCloud with isolated VPCs for financial systems, AWS ParallelCluster for economic modeling, AWS Lake Formation for financial data governance, Amazon Kinesis Data Analytics for real-time economic monitoring, and AWS CloudTrail with tamper detection for central bank audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for financial records, AWS Batch for economic calculations, Amazon S3 for policy data, and AWS Config for regulatory compliance",
        "Use AWS Outposts for secure facilities, Amazon FSx for financial data, AWS Step Functions for policy workflows, Amazon ElastiCache for performance, and AWS Direct Connect for international connectivity"
    ],
    correct: 1,
    explanation: {
        correct: "GovCloud with isolated VPCs provides maximum security for national financial systems. ParallelCluster delivers HPC performance for complex economic modeling. Lake Formation provides financial data governance with strict access controls. Kinesis Data Analytics enables real-time economic monitoring. CloudTrail with tamper detection provides central bank audit compliance.",
        whyWrong: {
            0: "Standard deployment doesn't provide the security isolation needed for central bank financial systems",
            2: "Control Tower doesn't provide the security isolation needed for central banking operations",
            3: "Outposts requires on-premises management and doesn't provide the security isolation needed for national financial data"
        },
        examStrategy: "Central banking: GovCloud isolated VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection."
    }
},
{
    id: 'sap_367',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global ride-hailing platform coordinates 5 million drivers across 1,000+ cities, experiences driver-passenger matching delays during surge periods, has dynamic pricing calculation issues, faces real-time location tracking problems at scale, and needs fraud detection while optimizing operational costs.",
    question: "Which optimization strategy provides the BEST ride-hailing platform performance with fraud prevention?",
    options: [
        "Implement Amazon ElastiCache for driver location caching, AWS Lambda for matching algorithms, Amazon DynamoDB for ride data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for monitoring",
        "Deploy Amazon ElastiCache Redis cluster mode for real-time location data, Amazon SageMaker for driver-passenger matching optimization and fraud detection, Amazon DynamoDB Global Tables for ride state management, Amazon Kinesis for location streams, and AWS Lambda for dynamic pricing algorithms",
        "Use Amazon Aurora Global Database for ride records, Amazon MSK for real-time messaging, AWS Step Functions for ride workflows, Amazon EventBridge for surge coordination, and AWS Batch for fraud analysis",
        "Implement Amazon DocumentDB for flexible ride schemas, Amazon EventBridge for location events, AWS Batch for matching calculations, Amazon SQS for ride requests, and Amazon EMR for fraud analytics"
    ],
    correct: 1,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond location access for 5M drivers with automatic sharding. SageMaker optimizes driver-passenger matching and provides real-time fraud detection. DynamoDB Global Tables manages ride state across 1,000+ cities. Kinesis handles real-time location streams efficiently. Lambda scales automatically for dynamic pricing.",
        whyWrong: {
            0: "Basic ElastiCache setup doesn't provide clustering needed for 5M drivers, CloudWatch doesn't provide fraud detection",
            2: "Aurora Global Database has write limitations for high-frequency location updates, Batch adds latency for fraud detection",
            3: "DocumentDB doesn't optimize for ride-hailing patterns, EventBridge adds latency for real-time location processing"
        },
        examStrategy: "Ride-hailing optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for scale and fraud prevention."
    }
},
{
    id: 'sap_368',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational oil and gas company operates drilling platforms and refineries across 40+ countries with varying environmental regulations, needs real-time safety monitoring, environmental compliance reporting, operational coordination, and emergency response capabilities while maintaining operational independence during connectivity issues.",
    question: "Which governance architecture provides OPTIMAL oil and gas operations with environmental compliance and safety?",
    options: [
        "AWS Organizations with facility-based OUs, AWS Config for safety compliance, AWS IoT Core for equipment monitoring, Amazon Kinesis for operational data, and AWS Storage Gateway for operational independence",
        "AWS Organizations with country-specific OUs for environmental compliance, AWS IoT Core for safety and equipment monitoring, Amazon Kinesis Data Analytics for real-time operational optimization, AWS Outposts for remote facility independence, and AWS Config for continuous environmental regulatory monitoring",
        "AWS Control Tower for industry standardization, AWS Security Hub for compliance monitoring, Amazon MSK for facility communication, AWS Lambda for safety processing, and AWS Direct Connect for facility connectivity",
        "AWS Landing Zone with energy templates, AWS Directory Service for worker authentication, Amazon ElastiCache for operations caching, AWS Batch for environmental analysis, and AWS Certificate Manager for secure communications"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 40+ different environmental regulations. IoT Core manages safety and equipment monitoring systems across drilling platforms and refineries. Kinesis Data Analytics provides real-time operational optimization and safety analysis. Outposts enables remote facilities to operate independently during connectivity issues. Config monitors environmental regulatory compliance continuously.",
        whyWrong: {
            0: "Facility-based OUs don't address country-specific environmental regulations, Storage Gateway doesn't provide full operational independence",
            2: "Control Tower doesn't address country-specific environmental regulations, MSK adds complexity for facility communication",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the environmental governance needed for oil and gas operations"
        },
        examStrategy: "Oil and gas governance: Organizations + country OUs + IoT Core + Kinesis Analytics + Outposts + Config for environmental compliance."
    }
},
{
    id: 'sap_369',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A quantum computing research consortium enables 1,000+ physicists globally to access quantum simulators, develop quantum algorithms, collaborate on quantum machine learning, manage experimental results, and advance quantum research while protecting intellectual property and ensuring fair resource allocation.",
    question: "Which architecture provides OPTIMAL quantum research collaboration with comprehensive IP protection?",
    options: [
        "Amazon EC2 with specialized instances for quantum simulation, Amazon S3 for research data, AWS Lambda for algorithm processing, Amazon DynamoDB for experimental records, and AWS IAM for access control",
        "Amazon Braket for quantum computing access, AWS Lake Formation for IP protection and research governance, Amazon SageMaker for quantum ML research, AWS Batch for fair resource scheduling among researchers, and AWS PrivateLink for secure global research collaboration",
        "AWS Batch for quantum job scheduling, Amazon EFS for shared research data, AWS Step Functions for quantum workflows, Amazon DocumentDB for algorithm metadata, and AWS VPN for researcher access",
        "Amazon EMR for quantum analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Amazon Braket provides managed quantum computing access with fair resource allocation among 1,000+ researchers. Lake Formation ensures IP protection through fine-grained access controls and research governance. SageMaker enables quantum machine learning research. Batch provides fair scheduling for quantum jobs. PrivateLink ensures secure global research collaboration while protecting IP.",
        whyWrong: {
            0: "Standard EC2 doesn't provide quantum computing capabilities, basic IAM doesn't provide IP protection needed for research",
            2: "Batch alone doesn't provide quantum computing access, EFS doesn't provide IP protection for quantum research",
            3: "EMR doesn't provide quantum computing capabilities, Redshift is for traditional analytics not quantum research"
        },
        examStrategy: "Quantum research: Braket + Lake Formation + SageMaker + Batch + PrivateLink for collaborative quantum computing research."
    }
},
{
    id: 'sap_370',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national healthcare system needs to migrate their patient records and medical imaging systems from legacy infrastructure to AWS, supporting 100 million patient records, ensuring HIPAA compliance, enabling provider collaboration, and maintaining high availability for emergency care while protecting patient privacy.",
    question: "Which modernization approach provides OPTIMAL healthcare system performance with comprehensive privacy protection?",
    options: [
        "Deploy Amazon EC2 for healthcare applications, Amazon RDS for patient data, AWS Lambda for medical processing, Amazon API Gateway for provider integration, and AWS CloudTrail for HIPAA audit compliance",
        "Implement AWS Control Tower for healthcare governance, Amazon HealthLake for patient records management, Amazon DynamoDB for medical data, AWS Lake Formation for HIPAA compliance and patient privacy protection, and AWS PrivateLink for secure provider collaboration",
        "Use AWS Elastic Beanstalk for medical applications, Amazon Aurora for patient database, AWS Step Functions for healthcare workflows, Amazon S3 for medical imaging, and AWS Config for compliance monitoring",
        "Deploy Amazon EKS for containerized healthcare services, Amazon DocumentDB for flexible patient data, AWS EventBridge for healthcare coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower provides healthcare-specific governance frameworks. HealthLake is purpose-built for healthcare data management with FHIR support for 100M patient records. DynamoDB scales for medical data with high availability. Lake Formation ensures HIPAA compliance and patient privacy protection. PrivateLink enables secure provider collaboration without internet exposure.",
        whyWrong: {
            0: "Standard deployment doesn't provide healthcare-specific data management and HIPAA compliance frameworks",
            2: "Elastic Beanstalk doesn't provide healthcare-specific controls, Aurora may not optimize for 100M patient records",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for healthcare data patterns and HIPAA requirements"
        },
        examStrategy: "Healthcare systems: Control Tower + HealthLake + DynamoDB + Lake Formation + PrivateLink for HIPAA-compliant patient care."
    }
},
{
    id: 'sap_371',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global e-commerce marketplace with 500 million products experiences search performance degradation, product recommendation delays, inventory synchronization issues across regions, seller performance tracking problems, and payment processing bottlenecks during major sales events.",
    question: "Which optimization strategy provides the BEST marketplace performance and seller experience?",
    options: [
        "Implement Amazon ElastiCache for product caching, AWS Lambda for search processing, Amazon DynamoDB for inventory data, Amazon API Gateway for seller APIs, and Amazon SQS for payment processing",
        "Deploy Amazon OpenSearch for intelligent product search, Amazon SageMaker for real-time recommendations, Amazon DynamoDB Global Tables for inventory synchronization, AWS Lambda for seller analytics, and Amazon Kinesis for payment event processing",
        "Use Amazon Aurora Global Database for product catalog, Amazon MSK for real-time updates, AWS Step Functions for seller workflows, Amazon EventBridge for marketplace coordination, and AWS Batch for payment processing",
        "Implement Amazon DocumentDB for flexible product schemas, Amazon EventBridge for marketplace events, AWS Batch for recommendation calculations, Amazon SQS for seller communication, and Amazon EMR for payment analytics"
    ],
    correct: 1,
    explanation: {
        correct: "OpenSearch provides sophisticated product search capabilities for 500M products with real-time indexing. SageMaker enables real-time recommendation inference. DynamoDB Global Tables ensures inventory synchronization across regions. Lambda provides seller analytics with automatic scaling. Kinesis handles payment events during sales spikes efficiently.",
        whyWrong: {
            0: "Basic Lambda search processing can't match OpenSearch capabilities for 500M products, SQS adds latency for payments",
            2: "Aurora Global Database has write limitations for high-frequency inventory updates, Batch adds latency for real-time payments",
            3: "DocumentDB doesn't optimize for marketplace search patterns, EventBridge adds latency for real-time inventory updates"
        },
        examStrategy: "Marketplace optimization: OpenSearch + SageMaker + DynamoDB Global Tables + Lambda + Kinesis for optimal e-commerce performance."
    }
},
{
    id: 'sap_372',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global consulting firm manages 500+ client engagements across 80+ countries with strict confidentiality requirements, implements Chinese walls between competing clients, enables secure collaboration on joint projects, maintains audit trails for compliance, and supports remote workforce while protecting client intellectual property.",
    question: "Which governance framework ensures COMPREHENSIVE client confidentiality and IP protection with operational flexibility?",
    options: [
        "AWS Organizations with client-based OUs, AWS Config for confidentiality compliance, AWS PrivateLink for secure communication, AWS CloudTrail for audit trails, and AWS WorkSpaces for remote collaboration",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for Chinese wall enforcement and client IP protection, AWS PrivateLink for secure project collaboration, AWS CloudTrail with legal hold for audit compliance, and AWS WorkSpaces with DLP for secure remote access",
        "AWS Control Tower for consulting standardization, AWS Security Hub for compliance monitoring, Amazon S3 for client data storage, AWS Lambda for data processing, and AWS Direct Connect for office connectivity",
        "AWS Landing Zone with consulting templates, AWS Directory Service for consultant authentication, Amazon ElastiCache for client caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 80+ different confidentiality regulations. Lake Formation enforces Chinese walls through fine-grained access controls and protects client IP. PrivateLink enables secure project collaboration without internet exposure. CloudTrail with legal hold provides compliant audit trails. WorkSpaces with DLP ensures secure remote access with data loss prevention.",
        whyWrong: {
            0: "Client-based OUs don't address jurisdiction-specific regulations, basic WorkSpaces doesn't provide Chinese wall enforcement",
            2: "Control Tower doesn't address jurisdiction-specific confidentiality requirements, basic S3 doesn't provide Chinese wall controls",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the fine-grained access controls needed for Chinese walls"
        },
        examStrategy: "Consulting governance: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + WorkSpaces DLP."
    }
},
{
    id: 'sap_373',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart transportation system coordinates 50,000+ connected vehicles, 10,000+ traffic signals, and public transit across a major metropolitan area, optimizes traffic flow, manages emergency vehicle routing, provides real-time passenger information, and reduces carbon emissions through intelligent routing.",
    question: "Which architecture provides OPTIMAL smart transportation coordination and environmental optimization?",
    options: [
        "AWS IoT Core for vehicle and infrastructure connectivity, Amazon MSK for high-volume transportation data, Amazon EMR for traffic analytics, AWS Lambda for routing optimization, and Amazon S3 for transportation data storage",
        "Amazon Kinesis Data Streams for real-time vehicle and signal data, Amazon Kinesis Data Analytics for traffic optimization, Amazon SageMaker for route prediction and carbon emission reduction, AWS Lambda for emergency vehicle coordination, and Amazon Timestream for transportation metrics",
        "Amazon EventBridge for transportation events, AWS Batch for route calculations, Amazon DocumentDB for vehicle data, Amazon API Gateway for passenger information, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for vehicle edge processing, Amazon DynamoDB for transportation records, AWS Step Functions for traffic coordination, Amazon Redshift for analytics, and Amazon QuickSight for transportation dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles data from 50,000+ vehicles and 10,000+ signals. Kinesis Data Analytics provides real-time traffic optimization. SageMaker enables intelligent routing for carbon emission reduction and route prediction. Lambda coordinates emergency vehicle routing with automatic scaling. Timestream optimizes transportation time-series data and metrics.",
        whyWrong: {
            0: "MSK adds complexity for transportation data, EMR is for batch analytics not real-time traffic optimization",
            2: "EventBridge and Batch add latency inappropriate for real-time traffic management and emergency routing",
            3: "IoT Greengrass on every vehicle adds operational complexity, Step Functions add latency for traffic coordination"
        },
        examStrategy: "Smart transportation: Kinesis ecosystem + SageMaker + Lambda + Timestream for intelligent urban mobility."
    }
},
{
    id: 'sap_374',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national emergency management agency needs to migrate their disaster response coordination systems from legacy infrastructure to AWS, supporting real-time crisis communication, resource allocation, inter-agency coordination, public alerting, and maintaining operations during natural disasters while ensuring government security compliance.",
    question: "Which modernization strategy ensures MAXIMUM resilience for emergency management operations?",
    options: [
        "Deploy Amazon EC2 in multiple regions, implement Amazon RDS for emergency data, use AWS Lambda for alert processing, Amazon API Gateway for agency coordination, and AWS CloudFormation for infrastructure automation",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB Global Tables for crisis data resilience, AWS IoT Core for emergency sensor networks, Amazon SNS for public alerting, and AWS Outposts for field operations during disasters",
        "Implement AWS Outposts for emergency centers, Amazon MSK for crisis communication, AWS Batch for resource calculations, Amazon ElastiCache for performance, and AWS Direct Connect for agency connectivity",
        "Use AWS Wavelength for emergency response, Amazon EventBridge for crisis coordination, Amazon DocumentDB for emergency data, Amazon Redshift for analytics, and AWS Transit Gateway for agency networking"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for emergency operations. DynamoDB Global Tables ensures crisis data resilience across regions during disasters. IoT Core manages emergency sensor networks securely. SNS delivers public alerts reliably at scale. Outposts enables field operations to continue during infrastructure disruptions.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance and disaster resilience needed",
            2: "Outposts alone doesn't provide the scalability and government compliance needed for national emergency management",
            3: "Wavelength is for mobile edge computing not emergency management, EventBridge doesn't provide disaster resilience"
        },
        examStrategy: "Emergency management: Control Tower GovCloud + DynamoDB Global Tables + IoT Core + SNS + Outposts for disaster resilience."
    }
},
{
    id: 'sap_375',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global logistics network coordinates 2 million packages daily across 5,000+ distribution centers, experiences package tracking delays, has route optimization inefficiencies, faces capacity planning challenges during peak seasons, and needs real-time visibility for customers while reducing operational costs and carbon footprint.",
    question: "Which optimization strategy provides the BEST logistics performance with environmental sustainability?",
    options: [
        "Implement Amazon ElastiCache for package caching, AWS Lambda for tracking updates, Amazon DynamoDB for logistics data, Amazon API Gateway for customer visibility, and Amazon CloudWatch for monitoring",
        "Deploy Amazon DynamoDB Global Tables for package tracking, Amazon SageMaker for route optimization and capacity planning, Amazon Kinesis Data Streams for real-time logistics events, AWS Lambda for carbon footprint calculation, and Amazon API Gateway for customer visibility APIs",
        "Use Amazon Aurora Global Database for package records, Amazon MSK for real-time messaging, AWS Step Functions for logistics workflows, Amazon EventBridge for distribution coordination, and AWS Batch for route calculations",
        "Implement Amazon DocumentDB for flexible logistics schemas, Amazon EventBridge for package events, AWS Batch for optimization calculations, Amazon SQS for distribution communication, and Amazon EMR for analytics"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB Global Tables provides conflict-free package tracking for 2M daily packages across 5,000+ centers. SageMaker optimizes routes and predicts capacity needs for peak seasons while minimizing carbon footprint. Kinesis Data Streams handles real-time logistics events efficiently. Lambda calculates environmental impact automatically. API Gateway provides scalable customer visibility.",
        whyWrong: {
            0: "Basic caching doesn't solve complex logistics optimization, CloudWatch doesn't provide route optimization or sustainability metrics",
            2: "Aurora Global Database has write limitations for high-frequency package updates, Step Functions add latency for real-time logistics",
            3: "DocumentDB doesn't optimize for logistics tracking patterns, EventBridge adds latency for real-time package processing"
        },
        examStrategy: "Logistics optimization: DynamoDB Global Tables + SageMaker + Kinesis + Lambda + API Gateway for sustainable logistics."
    }
},

// Questions 376-387: correct: 2 (C)
{
    id: 'sap_376',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational pharmaceutical research company collaborates with 200+ academic institutions globally on drug development, requires secure data sharing, intellectual property protection, regulatory compliance across jurisdictions, and fair access to research resources while maintaining competitive advantages.",
    question: "Which governance framework ensures OPTIMAL research collaboration with comprehensive IP protection?",
    options: [
        "AWS Organizations with institution-based OUs, AWS Config for research compliance, AWS PrivateLink for academic collaboration, AWS CloudTrail for IP audit trails, and AWS WorkSpaces for secure research access",
        "AWS Control Tower for research standardization, AWS Security Hub for compliance monitoring, Amazon S3 for research data storage, AWS Lambda for data processing, and AWS Direct Connect for institutional connectivity",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for IP protection and research data governance, AWS PrivateLink for secure academic collaboration, AWS CloudTrail with legal hold for patent audit trails, and AWS Resource Access Manager for controlled resource sharing",
        "AWS Landing Zone with research templates, AWS Directory Service for researcher authentication, Amazon ElastiCache for research caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with varying drug development regulations globally. Lake Formation provides IP protection and research data governance with fine-grained access controls. PrivateLink enables secure academic collaboration without internet exposure. CloudTrail with legal hold provides patent-compliant audit trails. RAM enables controlled sharing of research resources among 200+ institutions.",
        whyWrong: {
            0: "Institution-based OUs don't address jurisdiction-specific regulatory requirements, basic WorkSpaces doesn't provide IP protection",
            1: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide IP governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the IP protection needed for pharmaceutical research"
        },
        examStrategy: "Pharmaceutical research: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + RAM for secure collaboration."
    }
},
{
    id: 'sap_377',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global climate monitoring network processes data from 100,000+ weather stations, satellites, and ocean buoys, predicts climate patterns, models environmental changes, supports international climate research, and provides early warning systems for extreme weather events while enabling scientific collaboration.",
    question: "Which architecture provides OPTIMAL climate monitoring and research collaboration capabilities?",
    options: [
        "AWS IoT Core for sensor connectivity, Amazon MSK for high-volume climate data, Amazon EMR for climate analytics, AWS Lambda for weather processing, and Amazon S3 for environmental data storage",
        "Amazon EventBridge for climate events, AWS Batch for modeling calculations, Amazon DocumentDB for climate data, Amazon API Gateway for research integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Core for global sensor management, Amazon Kinesis Data Analytics for real-time climate analysis, Amazon SageMaker for climate prediction modeling, AWS Lambda for extreme weather alerting, and Amazon S3 with Cross-Region Replication for international research collaboration",
        "AWS IoT Greengrass for sensor edge processing, Amazon DynamoDB for climate records, AWS Step Functions for research workflows, Amazon Redshift for analytics, and Amazon QuickSight for climate dashboards"
    ],
    correct: 2,
    explanation: {
        correct: "IoT Core provides secure management for 100,000+ global sensors with device security. Kinesis Data Analytics processes real-time climate data for pattern detection. SageMaker enables sophisticated climate prediction and environmental modeling. Lambda provides extreme weather early warning systems. S3 with CRR enables international climate research collaboration and data sharing.",
        whyWrong: {
            0: "MSK adds complexity for climate sensor data, EMR is for batch analytics not real-time climate monitoring",
            1: "EventBridge and Batch add latency inappropriate for real-time climate monitoring and early warning systems",
            3: "IoT Greengrass on every sensor adds operational complexity, Step Functions add latency for real-time climate analysis"
        },
        examStrategy: "Climate monitoring: IoT Core + Kinesis Analytics + SageMaker + Lambda + S3 CRR for global climate research."
    }
},
{
    id: 'sap_378',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global food delivery platform coordinates 1 million drivers across 2,000+ cities, experiences delivery time prediction errors, has driver-restaurant coordination issues, faces dynamic pricing calculation delays, and needs real-time order optimization while reducing operational costs and environmental impact.",
    question: "Which optimization strategy provides the BEST food delivery performance with sustainability focus?",
    options: [
        "Implement Amazon ElastiCache for restaurant caching, AWS Lambda for delivery processing, Amazon DynamoDB for order data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for order records, Amazon MSK for real-time messaging, AWS Step Functions for delivery workflows, Amazon EventBridge for coordination, and AWS Batch for pricing calculations",
        "Use Amazon ElastiCache Redis cluster mode for real-time location data, Amazon SageMaker for delivery optimization and carbon footprint reduction, Amazon DynamoDB Global Tables for order state management, Amazon Kinesis for location streams, and AWS Lambda for driver-restaurant coordination",
        "Implement Amazon DocumentDB for flexible order schemas, Amazon EventBridge for delivery events, AWS Batch for route calculations, Amazon SQS for coordination, and Amazon EMR for sustainability analytics"
    ],
    correct: 2,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond location access for 1M drivers across 2,000+ cities. SageMaker optimizes delivery routes for efficiency and carbon footprint reduction. DynamoDB Global Tables manages order state globally. Kinesis handles real-time location streams efficiently. Lambda coordinates driver-restaurant communication automatically.",
        whyWrong: {
            0: "Basic caching doesn't solve delivery optimization, CloudWatch doesn't provide route optimization or sustainability features",
            1: "Aurora Global Database has write limitations for high-frequency location updates, Batch adds latency for real-time pricing",
            3: "DocumentDB doesn't optimize for food delivery patterns, EventBridge adds latency for real-time location processing"
        },
        examStrategy: "Food delivery optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for sustainable delivery."
    }
},
{
    id: 'sap_379',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national intelligence agency needs to migrate their threat analysis and counter-intelligence systems from air-gapped networks to AWS, maintaining the highest classification levels, enabling real-time threat assessment, supporting inter-agency intelligence sharing, and ensuring absolute security isolation while advancing analytical capabilities.",
    question: "Which modernization approach ensures MAXIMUM security for classified intelligence operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for intelligence data, use AWS Lambda for threat analysis, Amazon API Gateway for agency integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for threat records, AWS Batch for analysis workflows, Amazon S3 for intelligence data, and AWS Config for classification compliance",
        "Deploy AWS GovCloud with classification-isolated VPCs and air-gapped networking, AWS ParallelCluster for advanced threat analysis computing, AWS Lake Formation for intelligence data governance, Amazon Kinesis Data Analytics for real-time threat assessment, and AWS CloudTrail with tamper detection for audit compliance",
        "Use AWS Outposts in secure facilities, Amazon FSx for intelligence data, AWS Step Functions for analysis workflows, Amazon ElastiCache for performance, and AWS Direct Connect for inter-agency connectivity"
    ],
    correct: 2,
    explanation: {
        correct: "GovCloud with classification-isolated VPCs and air-gapped networking provides maximum security separation for classified intelligence. ParallelCluster delivers advanced computing for sophisticated threat analysis. Lake Formation provides intelligence data governance with strict access controls. Kinesis Data Analytics enables real-time threat assessment. CloudTrail with tamper detection ensures audit compliance for intelligence operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide classification-level security isolation needed for intelligence operations",
            1: "Control Tower doesn't provide classification-isolated security needed for counter-intelligence",
            3: "Outposts requires on-premises management, Direct Connect may compromise air-gapped security requirements"
        },
        examStrategy: "Intelligence operations: GovCloud air-gapped VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection."
    }
},
{
    id: 'sap_380',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment management firm with $5 trillion assets under management operates across 70+ jurisdictions, requires real-time risk assessment, algorithmic trading capabilities, regulatory reporting per jurisdiction, client data protection, and secure communication with fund managers while maintaining competitive trading advantages.",
    question: "Which governance framework ensures COMPREHENSIVE investment management with competitive protection?",
    options: [
        "AWS Organizations with asset class OUs, AWS Config for investment compliance, AWS Lambda for trading algorithms, Amazon DynamoDB for portfolio data, and AWS PrivateLink for manager communication",
        "AWS Control Tower for investment standardization, AWS Security Hub for compliance monitoring, Amazon Kinesis for trading data, AWS Step Functions for investment workflows, and AWS Direct Connect for regulatory connectivity",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for client data protection and competitive intelligence governance, Amazon ElastiCache for real-time risk assessment, AWS Lambda for algorithmic trading, and AWS CloudTrail with legal hold for regulatory audit trails",
        "AWS Landing Zone with investment templates, AWS Directory Service for trader authentication, Amazon ElastiCache for portfolio caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for fund management"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 70+ different financial regulations. Lake Formation provides client data protection and competitive intelligence governance through fine-grained access controls. ElastiCache enables real-time risk assessment for $5T AUM. Lambda scales for algorithmic trading with competitive advantages. CloudTrail with legal hold provides regulatory audit trails.",
        whyWrong: {
            0: "Asset class OUs don't address jurisdiction-specific regulatory requirements",
            1: "Control Tower doesn't address jurisdiction-specific investment regulations",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the data governance needed for competitive protection"
        },
        examStrategy: "Investment management: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for competitive protection."
    }
},
{
    id: 'sap_381',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A precision medicine platform analyzes genetic data from 10 million patients, develops personalized treatment plans, predicts drug responses, matches patients to clinical trials, and enables medical research collaboration while maintaining strict patient privacy and regulatory compliance across multiple countries.",
    question: "Which architecture provides OPTIMAL precision medicine capabilities with comprehensive privacy protection?",
    options: [
        "Amazon EC2 with GPU instances for genetic analysis, Amazon S3 for patient data, AWS Lambda for treatment planning, Amazon DynamoDB for medical records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for treatment coordination, Amazon DocumentDB for patient metadata, and AWS VPN for researcher access",
        "Amazon SageMaker for AI-driven treatment prediction and drug response analysis, AWS ParallelCluster for genetic sequencing analysis, AWS Lake Formation for patient privacy governance and regulatory compliance, Amazon HealthLake for medical data management, and AWS PrivateLink for secure research collaboration",
        "Amazon EMR for genetic analytics, Amazon Redshift for medical data warehousing, AWS Glue for data processing, Amazon QuickSight for medical visualization, and AWS Direct Connect for healthcare collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "SageMaker provides AI/ML for personalized treatment prediction and drug response analysis. ParallelCluster delivers HPC performance for genetic sequencing of 10M patients. Lake Formation ensures patient privacy governance and multi-country regulatory compliance. HealthLake provides purpose-built medical data management and clinical trial matching. PrivateLink enables secure research collaboration while protecting patient data.",
        whyWrong: {
            0: "Standard EC2 doesn't provide AI/ML precision medicine capabilities, basic IAM doesn't provide patient privacy protection",
            1: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide privacy protection for patient genetic data",
            3: "EMR doesn't provide AI/ML or specialized genetic analysis capabilities, Redshift doesn't optimize for precision medicine workflows"
        },
        examStrategy: "Precision medicine: SageMaker + ParallelCluster + Lake Formation + HealthLake + PrivateLink for AI-driven personalized healthcare."
    }
},
{
    id: 'sap_382',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global telecommunications network serves 500 million subscribers across 80+ countries, experiences network congestion during peak hours, has 5G rollout optimization challenges, faces real-time fault detection requirements, and needs predictive maintenance for network infrastructure while reducing operational costs and energy consumption.",
    question: "Which optimization strategy provides the BEST telecommunications network performance with sustainability focus?",
    options: [
        "Implement Amazon ElastiCache for network caching, AWS Lambda for fault processing, Amazon DynamoDB for network data, Amazon API Gateway for subscriber services, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for network records, Amazon MSK for real-time messaging, AWS Step Functions for network workflows, Amazon EventBridge for fault coordination, and AWS Batch for maintenance calculations",
        "Use Amazon ElastiCache Redis cluster mode for real-time network state, Amazon SageMaker for predictive maintenance and 5G optimization, Amazon DynamoDB Global Tables for subscriber management, Amazon Kinesis for network telemetry, and AWS Lambda for fault detection and energy optimization",
        "Implement Amazon DocumentDB for flexible network schemas, Amazon EventBridge for network events, AWS Batch for optimization calculations, Amazon SQS for fault handling, and Amazon EMR for sustainability analytics"
    ],
    correct: 2,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides real-time network state access for 500M subscribers with automatic scaling. SageMaker enables predictive maintenance and 5G rollout optimization while reducing energy consumption. DynamoDB Global Tables manages subscribers across 80+ countries. Kinesis handles network telemetry efficiently. Lambda provides real-time fault detection and energy optimization.",
        whyWrong: {
            0: "Basic caching doesn't solve network optimization, CloudWatch doesn't provide predictive maintenance or 5G optimization",
            1: "Aurora Global Database has limitations for high-frequency network updates, Batch adds latency for real-time fault detection",
            3: "DocumentDB doesn't optimize for telecommunications patterns, EventBridge adds latency for real-time network management"
        },
        examStrategy: "Telecom optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for sustainable networks."
    }
},
{
    id: 'sap_383',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national social security administration needs to migrate their benefits processing systems from legacy mainframes to AWS, handling 100 million beneficiaries, ensuring data privacy, maintaining fraud detection capabilities, supporting citizen self-service, and ensuring high availability for critical benefit payments while meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL citizen services with comprehensive security and fraud protection?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for beneficiary data, use AWS Lambda for benefits processing, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Implement AWS Elastic Beanstalk for benefits applications, Amazon Aurora for beneficiary database, AWS Step Functions for processing workflows, Amazon CloudFront for citizen portal, and AWS Config for compliance monitoring",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB for beneficiary records, Amazon SageMaker for fraud detection, AWS Lambda for benefits processing, and Amazon Kinesis Data Analytics for real-time payment monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible beneficiary data, AWS EventBridge for processing coordination, AWS Batch for benefits calculations, and AWS Security Hub for compliance management"
    ],
    correct: 2,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance and security controls. DynamoDB scales to handle 100M beneficiaries with high availability for critical payments. SageMaker provides sophisticated fraud detection capabilities. Lambda handles benefits processing with automatic scaling. Kinesis Data Analytics enables real-time payment monitoring for fraud prevention.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance and fraud detection needed for social security",
            1: "Elastic Beanstalk doesn't provide government security controls, Aurora may not scale optimally for 100M beneficiaries",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for social security processing patterns"
        },
        examStrategy: "Social security modernization: Control Tower GovCloud + DynamoDB + SageMaker + Lambda + Kinesis Analytics for secure citizen services."
    }
},
{
    id: 'sap_384',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational aerospace consortium collaborates on space exploration projects across 25+ countries with different export control regulations, requires secure technology sharing, intellectual property protection, international coordination, and compliance with space treaties while enabling rapid innovation and development.",
    question: "Which governance framework ensures OPTIMAL space project collaboration with export control compliance?",
    options: [
        "AWS Organizations with project-based OUs, AWS Config for space compliance, AWS PrivateLink for international collaboration, AWS CloudTrail for technology audit trails, and AWS WorkSpaces for secure development",
        "AWS Control Tower for aerospace standardization, AWS Security Hub for compliance monitoring, Amazon S3 for space data storage, AWS Lambda for data processing, and AWS Direct Connect for international connectivity",
        "AWS Organizations with country-specific OUs for export control compliance, AWS Lake Formation for technology sharing governance and IP protection, AWS PrivateLink for secure international collaboration, AWS CloudTrail with legal hold for space treaty audit trails, and AWS GovCloud for sensitive space technology",
        "AWS Landing Zone with aerospace templates, AWS Directory Service for engineer authentication, Amazon ElastiCache for space data caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 25+ different export control regulations for space technology. Lake Formation provides technology sharing governance and IP protection through fine-grained access controls. PrivateLink enables secure international collaboration without internet exposure. CloudTrail with legal hold provides space treaty audit trails. GovCloud provides additional security for sensitive space technology.",
        whyWrong: {
            0: "Project-based OUs don't address country-specific export control requirements, basic WorkSpaces doesn't provide IP protection",
            1: "Control Tower doesn't address country-specific export control regulations, basic S3 doesn't provide technology governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the export control governance needed for space technology"
        },
        examStrategy: "Space collaboration: Organizations + country OUs + Lake Formation + PrivateLink + CloudTrail legal hold + GovCloud for export control."
    }
},
{
    id: 'sap_385',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An autonomous shipping fleet management platform coordinates 5,000+ cargo vessels globally, optimizes maritime routes, manages port scheduling, monitors weather conditions, ensures maritime safety compliance, and enables predictive maintenance while reducing fuel consumption and environmental impact.",
    question: "Which architecture provides OPTIMAL autonomous maritime operations with environmental sustainability?",
    options: [
        "AWS IoT Core for vessel connectivity, Amazon MSK for maritime data, Amazon EMR for route analytics, AWS Lambda for navigation processing, and Amazon S3 for voyage data storage",
        "Amazon EventBridge for maritime events, AWS Batch for route calculations, Amazon DocumentDB for vessel data, Amazon API Gateway for port integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for vessel edge computing, Amazon Kinesis Data Streams for real-time vessel telemetry, Amazon SageMaker for route optimization and fuel efficiency, AWS Lambda for collision avoidance and safety compliance, and Amazon Timestream for maritime performance metrics",
        "AWS Satellite for vessel communication, Amazon MSK for data streaming, Amazon Redshift for voyage analytics, AWS Step Functions for port coordination, and Amazon QuickSight for fleet dashboards"
    ],
    correct: 2,
    explanation: {
        correct: "IoT Greengrass enables autonomous decision-making at sea when connectivity is limited for 5,000+ vessels. Kinesis Data Streams handles real-time vessel telemetry efficiently. SageMaker provides route optimization for fuel efficiency and environmental impact reduction. Lambda ensures collision avoidance and maritime safety compliance. Timestream optimizes maritime performance metrics and sustainability tracking.",
        whyWrong: {
            0: "MSK adds complexity for vessel communication, EMR is for batch analytics not real-time maritime operations",
            1: "EventBridge and Batch add latency inappropriate for real-time maritime navigation and safety",
            3: "Satellite alone doesn't provide comprehensive fleet management, Step Functions add latency for real-time port coordination"
        },
        examStrategy: "Autonomous shipping: IoT Greengrass + Kinesis + SageMaker + Lambda + Timestream for sustainable maritime operations."
    }
},
{
    id: 'sap_386',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global video streaming platform serves 800 million subscribers, experiences quality degradation during major live events, has content recommendation algorithm delays, faces regional content licensing complexities, and needs real-time viewer analytics while optimizing bandwidth costs and content delivery efficiency.",
    question: "Which optimization strategy provides the BEST streaming performance with cost efficiency and personalization?",
    options: [
        "Implement Amazon CloudFront for content delivery, AWS Lambda for recommendation processing, Amazon DynamoDB for viewer data, Amazon API Gateway for mobile apps, and Amazon S3 for video storage",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for video processing, Amazon EventBridge for viewer coordination, and Amazon ElastiCache for session management",
        "Use Amazon CloudFront with Lambda@Edge for adaptive streaming and regional content control, Amazon SageMaker for real-time personalized recommendations, Amazon DynamoDB Global Tables for viewer state management, AWS Elemental MediaServices for content optimization, and Amazon Kinesis for real-time viewer analytics",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Step Functions for recommendation workflows, Amazon DocumentDB for viewer data, and Amazon Route 53 for traffic routing"
    ],
    correct: 2,
    explanation: {
        correct: "CloudFront with Lambda@Edge enables adaptive streaming based on bandwidth and regional content licensing controls. SageMaker provides real-time personalized recommendations for 800M subscribers. DynamoDB Global Tables manages viewer state globally. Elemental MediaServices optimizes content delivery efficiency. Kinesis provides real-time viewer analytics for content optimization.",
        whyWrong: {
            0: "Basic Lambda processing doesn't provide real-time recommendation capabilities or regional content control",
            1: "Global Accelerator doesn't provide video streaming optimization, EFS doesn't optimize for video content delivery",
            3: "Direct Connect is expensive for global content delivery, Glacier has retrieval delays inappropriate for active streaming content"
        },
        examStrategy: "Video streaming: CloudFront Lambda@Edge + SageMaker + DynamoDB Global Tables + Elemental MediaServices + Kinesis for optimized streaming."
    }
},
{
    id: 'sap_387',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national meteorological service needs to migrate their weather prediction supercomputing systems from legacy infrastructure to AWS, supporting global climate modeling, severe weather forecasting, international data exchange, and public safety alerting while maintaining forecast accuracy and enabling climate research collaboration.",
    question: "Which modernization strategy ensures OPTIMAL weather prediction with international collaboration capabilities?",
    options: [
        "Deploy Amazon EMR with Spark clusters, implement Amazon S3 for weather data, use AWS Glue for data processing, Amazon Redshift for climate analytics, and AWS Direct Connect for international collaboration",
        "Implement Amazon ECS with GPU instances, use Amazon EFS for shared storage, AWS Step Functions for forecast workflows, Amazon DocumentDB for weather metadata, and AWS VPN for collaborative access",
        "Deploy AWS ParallelCluster with HPC-optimized instances for atmospheric modeling, Amazon FSx for Lustre for weather data processing, AWS Batch for ensemble forecasting, Amazon S3 with Cross-Region Replication for international data exchange, and AWS Ground Station for satellite weather data ingestion",
        "Use AWS Fargate for containerized forecasting, Amazon DynamoDB for weather records, Amazon EventBridge for forecast coordination, AWS Lambda for alert processing, and Amazon API Gateway for international APIs"
    ],
    correct: 2,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for global climate modeling. FSx Lustre handles massive weather datasets with parallel processing. Batch schedules ensemble forecasting efficiently. S3 with CRR enables international meteorological data exchange. Ground Station ingests satellite weather data directly for improved forecast accuracy.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive atmospheric modeling computations",
            1: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for weather computing",
            3: "Fargate has resource limitations for weather modeling, Lambda has execution time limits for complex atmospheric simulations"
        },
        examStrategy: "Weather prediction: ParallelCluster + HPC + FSx Lustre + Batch + S3 CRR + Ground Station for meteorological collaboration."
    }
},

// Questions 388-400: correct: 3 (D)
{
    id: 'sap_388',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global humanitarian organization coordinates disaster relief operations across 120+ countries with varying political situations, requires rapid deployment capabilities, secure communication in crisis zones, resource coordination, donor transparency, and compliance with international aid regulations while maintaining operational continuity during emergencies and conflicts.",
    question: "Which governance framework ensures COMPREHENSIVE humanitarian operations with crisis resilience?",
    options: [
        "AWS Organizations with program-based OUs, AWS Config for aid compliance, AWS IoT Core for field monitoring, Amazon Kinesis for crisis data, and AWS Storage Gateway for operational continuity",
        "AWS Control Tower for humanitarian standardization, AWS Security Hub for compliance monitoring, Amazon MSK for crisis communication, AWS Lambda for resource coordination, and AWS Direct Connect for field connectivity",
        "AWS Organizations with regional OUs, AWS Lake Formation for donor transparency, Amazon ElastiCache for resource caching, AWS Batch for aid distribution analysis, and AWS Certificate Manager for secure communications",
        "AWS Organizations with country-specific OUs for regulatory compliance, AWS Outposts for rapid crisis deployment, AWS Lake Formation for donor transparency and resource governance, Amazon Kinesis Data Analytics for real-time crisis coordination, and AWS Satellite for communication in remote disaster areas"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 120+ different aid regulations and political requirements. Outposts enables rapid deployment in crisis zones with local AWS infrastructure. Lake Formation provides donor transparency and resource governance with audit trails. Kinesis Data Analytics coordinates real-time crisis response and resource allocation. AWS Satellite ensures communication in remote disaster areas without existing infrastructure.",
        whyWrong: {
            0: "Program-based OUs don't address country-specific aid regulations, Storage Gateway doesn't provide crisis deployment capabilities",
            1: "Control Tower doesn't address country-specific humanitarian regulations, MSK doesn't provide crisis zone communication",
            2: "Regional OUs don't provide the granular country compliance needed for aid regulations and political situations"
        },
        examStrategy: "Humanitarian operations: Organizations + country OUs + Outposts + Lake Formation + Kinesis Analytics + Satellite for crisis response."
    }
},
{
    id: 'sap_389',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart city waste management platform monitors 2 million waste bins across a major metropolitan area using IoT sensors, optimizes collection routes, predicts bin fill levels, coordinates fleet management, and provides environmental impact analytics while reducing operational costs and carbon emissions.",
    question: "Which architecture provides OPTIMAL smart waste management with environmental sustainability focus?",
    options: [
        "AWS IoT Core for bin connectivity, Amazon MSK for high-volume waste data, Amazon EMR for route analytics, AWS Lambda for collection optimization, and Amazon S3 for environmental data storage",
        "Amazon EventBridge for waste events, AWS Batch for route calculations, Amazon DocumentDB for bin data, Amazon API Gateway for fleet integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for bin edge processing, Amazon DynamoDB for waste records, AWS Step Functions for collection workflows, Amazon Redshift for analytics, and Amazon QuickSight for environmental dashboards",
        "AWS IoT Core for sensor management, Amazon Kinesis Data Analytics for real-time bin monitoring, Amazon SageMaker for route optimization and fill prediction, AWS Lambda for fleet coordination, and Amazon Timestream for environmental impact tracking and carbon emission analytics"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure, scalable management for 2M waste bin sensors with device management. Kinesis Data Analytics processes real-time bin fill level data for optimization. SageMaker enables sophisticated route optimization and fill level prediction to reduce carbon emissions. Lambda coordinates fleet management automatically. Timestream tracks environmental impact metrics and carbon emission reduction analytics.",
        whyWrong: {
            0: "MSK adds complexity for waste sensor data, EMR is for batch analytics not real-time waste optimization",
            1: "EventBridge and Batch add latency inappropriate for real-time waste collection optimization",
            2: "IoT Greengrass on every bin adds operational complexity, Step Functions add latency for collection coordination"
        },
        examStrategy: "Smart waste management: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for sustainable city operations."
    }
},
{
    id: 'sap_390',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global cryptocurrency exchange processes 5 million transactions per second, experiences order execution delays during market volatility, has liquidity management challenges across trading pairs, faces regulatory compliance requirements in 60+ jurisdictions, and needs real-time risk management while maintaining sub-millisecond latency for high-frequency traders.",
    question: "Which optimization strategy provides the BEST cryptocurrency exchange performance with comprehensive compliance?",
    options: [
        "Implement Amazon ElastiCache for order book caching, AWS Lambda for trade processing, Amazon DynamoDB for transaction data, Amazon API Gateway for trading APIs, and Amazon CloudWatch for monitoring",
        "Deploy Amazon MemoryDB for trading state, Amazon MSK for high-throughput messaging, AWS Step Functions for trading workflows, Amazon Aurora Global for transaction history, and AWS Security Hub for compliance dashboards",
        "Use Amazon DocumentDB for flexible trading schemas, Amazon EventBridge for market events, AWS Batch for liquidity calculations, Amazon RDS for transaction storage, and AWS Config for regulatory compliance",
        "Implement Amazon ElastiCache Redis cluster mode for order books, AWS Lambda with provisioned concurrency for sub-millisecond execution, Amazon DynamoDB Global Tables for transaction records, Amazon Kinesis for real-time market data, and AWS Config for multi-jurisdiction compliance monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 5M TPS with automatic sharding and high availability. Lambda with provisioned concurrency eliminates cold starts for consistent sub-millisecond execution. DynamoDB Global Tables handles global transaction records with automatic scaling. Kinesis processes real-time market data efficiently. Config monitors compliance across 60+ jurisdictions continuously.",
        whyWrong: {
            0: "Basic caching doesn't provide clustering needed for 5M TPS, CloudWatch doesn't provide compliance monitoring",
            1: "MemoryDB has higher latency than ElastiCache cluster mode for sub-millisecond requirements, Step Functions add latency",
            2: "DocumentDB doesn't optimize for trading performance, EventBridge adds latency for real-time order execution"
        },
        examStrategy: "Crypto exchange optimization: ElastiCache cluster + Lambda provisioned + DynamoDB Global Tables + Kinesis + Config for compliant trading."
    }
},
{
    id: 'sap_391',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national education system needs to migrate their student information and learning management systems from legacy infrastructure to AWS, supporting 20 million students, ensuring data privacy, enabling remote learning capabilities, and maintaining high availability for critical educational services while meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL educational services with comprehensive privacy protection?",
    options: [
        "Deploy Amazon EC2 for education applications, Amazon RDS for student data, AWS Lambda for learning processing, Amazon API Gateway for student services, and AWS CloudTrail for privacy audit compliance",
        "Implement AWS Elastic Beanstalk for learning applications, Amazon Aurora for student database, AWS Step Functions for educational workflows, Amazon S3 for learning content, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible student data, AWS EventBridge for educational coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB for student records, Amazon Chime SDK for remote learning capabilities, AWS Lambda for educational processing, and AWS Lake Formation for student data privacy governance"
    ],
    correct: 3,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for educational data. DynamoDB scales to handle 20M students with high availability for critical services. Chime SDK enables remote learning with video and collaboration capabilities. Lambda handles educational processing with automatic scaling. Lake Formation ensures student data privacy governance and FERPA compliance.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance and privacy protection needed for student data",
            1: "Elastic Beanstalk doesn't provide government security controls, Aurora may not scale optimally for 20M students",
            2: "EKS adds operational complexity, DocumentDB doesn't optimize for educational data patterns and privacy requirements"
        },
        examStrategy: "Education modernization: Control Tower GovCloud + DynamoDB + Chime SDK + Lambda + Lake Formation for secure learning."
    }
},
{
    id: 'sap_392',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational energy trading company operates across 45+ countries with different energy regulations, requires real-time commodity trading, cross-border energy transactions, regulatory reporting per jurisdiction, and secure communication between trading desks while maintaining audit trails for compliance and competitive intelligence protection.",
    question: "Which governance framework ensures OPTIMAL energy trading with competitive protection and compliance?",
    options: [
        "AWS Organizations with trading desk OUs, AWS Config for energy compliance, AWS Lambda for trade processing, Amazon DynamoDB for trade records, and AWS PrivateLink for secure communication",
        "AWS Control Tower for energy standardization, AWS Security Hub for compliance monitoring, Amazon MSK for trading messages, AWS Step Functions for trade workflows, and AWS Direct Connect for desk connectivity",
        "AWS Organizations with commodity-based OUs, AWS Directory Service for trader authentication, Amazon ElastiCache for trade caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for competitive intelligence protection and trade governance, Amazon ElastiCache for real-time trading data, AWS Lambda for algorithmic trading, and AWS CloudTrail with legal hold for regulatory audit trails"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 45+ different energy regulations. Lake Formation provides competitive intelligence protection and trade governance through fine-grained access controls. ElastiCache enables real-time commodity trading with microsecond latency. Lambda scales for algorithmic trading strategies. CloudTrail with legal hold provides regulatory audit trails for energy trading compliance.",
        whyWrong: {
            0: "Trading desk OUs don't address jurisdiction-specific regulatory requirements",
            1: "Control Tower doesn't address jurisdiction-specific energy trading regulations",
            2: "Commodity-based OUs don't address jurisdiction-specific compliance requirements, Directory Service doesn't provide competitive protection"
        },
        examStrategy: "Energy trading: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for competitive compliance."
    }
},
{
    id: 'sap_393',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global carbon trading and environmental monitoring platform tracks emissions from 100,000+ industrial facilities, verifies carbon credits, manages environmental compliance, enables carbon offset trading, and provides real-time environmental impact analytics while ensuring data integrity and regulatory transparency.",
    question: "Which architecture provides OPTIMAL carbon trading with environmental compliance and data integrity?",
    options: [
        "AWS IoT Core for facility connectivity, Amazon MSK for high-volume emissions data, Amazon EMR for carbon analytics, AWS Lambda for offset processing, and Amazon S3 for environmental reporting",
        "Amazon EventBridge for facility events, AWS Batch for emissions calculations, Amazon DocumentDB for carbon data, Amazon API Gateway for trading integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for facility edge processing, Amazon DynamoDB for carbon records, AWS Step Functions for trading workflows, Amazon Redshift for analytics, and Amazon QuickSight for environmental dashboards",
        "AWS IoT Core for emissions monitoring, Amazon Kinesis Data Analytics for real-time carbon tracking, Amazon SageMaker for carbon credit verification and offset optimization, AWS Lambda for trading automation, and Amazon QLDB for immutable carbon trading records with AWS Lake Formation for regulatory transparency"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure monitoring for 100,000+ industrial facility emissions with device management. Kinesis Data Analytics tracks carbon emissions in real-time for compliance. SageMaker verifies carbon credits and optimizes offset trading strategies. Lambda automates carbon trading processes. QLDB ensures immutable carbon trading records for data integrity, and Lake Formation provides regulatory transparency and compliance governance.",
        whyWrong: {
            0: "MSK adds complexity for emissions monitoring, EMR is for batch analytics not real-time carbon tracking",
            1: "EventBridge and Batch add latency inappropriate for real-time carbon trading and compliance monitoring",
            2: "IoT Greengrass on every facility adds operational complexity, Step Functions add latency for trading workflows"
        },
        examStrategy: "Carbon trading: IoT Core + Kinesis Analytics + SageMaker + Lambda + QLDB + Lake Formation for environmental compliance."
    }
},
{
    id: 'sap_394',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online gaming platform serves 200 million players across multiple game titles, experiences matchmaking delays during peak hours, has in-game purchase processing issues, faces regional content regulation challenges, and needs real-time player analytics while optimizing server costs and player experience.",
    question: "Which optimization strategy provides the BEST gaming platform performance with cost efficiency and player satisfaction?",
    options: [
        "Implement Amazon ElastiCache for player session caching, AWS Lambda for matchmaking processing, Amazon DynamoDB for player data, Amazon API Gateway for game services, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for player records, Amazon MSK for real-time messaging, AWS Step Functions for game workflows, Amazon EventBridge for player coordination, and AWS Batch for analytics processing",
        "Use Amazon DocumentDB for flexible player schemas, Amazon EventBridge for game events, AWS Batch for matchmaking calculations, Amazon SQS for player communication, and Amazon EMR for player analytics",
        "Implement Amazon GameLift for managed game hosting and matchmaking, Amazon ElastiCache Redis cluster mode for player state, Amazon DynamoDB Global Tables for player progress, Amazon Kinesis for real-time player analytics, and AWS Lambda for in-game purchase processing"
    ],
    correct: 3,
    explanation: {
        correct: "GameLift provides managed game hosting with automatic scaling and intelligent matchmaking for 200M players. ElastiCache Redis cluster mode delivers sub-millisecond player state access with cost optimization. DynamoDB Global Tables manages player progress across regions. Kinesis provides real-time player analytics for game optimization. Lambda handles in-game purchases with automatic scaling and cost efficiency.",
        whyWrong: {
            0: "Basic caching doesn't solve matchmaking optimization for 200M players, CloudWatch doesn't provide game-specific analytics",
            1: "Aurora Global Database has limitations for high-frequency player updates, Step Functions add latency for real-time gaming",
            2: "DocumentDB doesn't optimize for gaming patterns, EventBridge adds latency for real-time player interactions"
        },
        examStrategy: "Gaming optimization: GameLift + ElastiCache cluster + DynamoDB Global Tables + Kinesis + Lambda for optimal player experience."
    }
},
{
    id: 'sap_395',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A defense intelligence agency needs to migrate their signals intelligence and cyber warfare systems from classified networks to AWS, maintaining the highest security clearances, enabling real-time threat analysis, supporting multi-domain operations, and ensuring air-gapped security while advancing analytical capabilities for national security.",
    question: "Which modernization strategy ensures MAXIMUM security for classified defense intelligence operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for intelligence data, use AWS Lambda for threat analysis, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for intelligence records, AWS Batch for analysis workflows, Amazon S3 for classified data, and AWS Config for security compliance",
        "Use AWS Outposts in secure facilities, Amazon FSx for intelligence data, AWS Step Functions for analysis workflows, Amazon ElastiCache for performance, and AWS Direct Connect for agency connectivity",
        "Deploy AWS GovCloud with air-gapped VPCs per security classification, AWS ParallelCluster for advanced threat analysis, AWS Lake Formation for classified data governance, Amazon Kinesis Data Analytics for real-time signals intelligence, and AWS CloudTrail with tamper detection for defense audit compliance"
    ],
    correct: 3,
    explanation: {
        correct: "GovCloud with air-gapped VPCs per security classification provides maximum security separation for classified intelligence. ParallelCluster delivers advanced computing for sophisticated threat analysis and cyber warfare capabilities. Lake Formation provides classified data governance with strict access controls. Kinesis Data Analytics enables real-time signals intelligence processing. CloudTrail with tamper detection ensures defense audit compliance for national security operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide classification-level security isolation needed for defense intelligence",
            1: "Control Tower doesn't provide air-gapped security isolation needed for classified cyber warfare systems",
            2: "Outposts requires on-premises management, Direct Connect may compromise air-gapped security requirements for classified operations"
        },
        examStrategy: "Defense intelligence: GovCloud air-gapped VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection."
    }
},
{
    id: 'sap_396',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global pharmaceutical consortium coordinates drug development across 50+ companies and 200+ research institutions, requires secure intellectual property sharing, regulatory compliance across jurisdictions, competitive collaboration controls, and clinical trial coordination while protecting proprietary research and maintaining fair access to breakthrough discoveries.",
    question: "Which governance framework ensures COMPREHENSIVE pharmaceutical collaboration with IP protection and competitive controls?",
    options: [
        "AWS Organizations with company-based OUs, AWS Config for pharma compliance, AWS PrivateLink for research collaboration, AWS CloudTrail for IP audit trails, and AWS WorkSpaces for secure development",
        "AWS Control Tower for pharmaceutical standardization, AWS Security Hub for compliance monitoring, Amazon S3 for research data storage, AWS Lambda for data processing, and AWS Direct Connect for institutional connectivity",
        "AWS Organizations with therapeutic area OUs, AWS Directory Service for researcher authentication, Amazon ElastiCache for research caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for IP protection and competitive collaboration controls, AWS PrivateLink for secure consortium collaboration, AWS CloudTrail with legal hold for pharmaceutical audit trails, and AWS Resource Access Manager for controlled breakthrough discovery sharing"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with varying pharmaceutical regulations globally. Lake Formation provides IP protection and competitive collaboration controls through fine-grained access controls. PrivateLink enables secure consortium collaboration without internet exposure. CloudTrail with legal hold provides pharmaceutical audit trails for regulatory compliance. RAM enables controlled sharing of breakthrough discoveries among consortium members while protecting proprietary research.",
        whyWrong: {
            0: "Company-based OUs don't address jurisdiction-specific regulatory requirements, basic WorkSpaces doesn't provide competitive controls",
            1: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide IP protection",
            2: "Therapeutic area OUs don't address regulatory jurisdiction requirements, Directory Service doesn't provide competitive collaboration controls"
        },
        examStrategy: "Pharmaceutical consortium: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + RAM for competitive collaboration."
    }
},
{
    id: 'sap_397',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global renewable energy management platform coordinates 500,000+ solar panels and wind turbines across smart grids, optimizes energy distribution, predicts renewable output, manages energy storage, and enables peer-to-peer energy trading while ensuring grid stability and minimizing carbon footprint.",
    question: "Which architecture provides OPTIMAL renewable energy management with grid stability and carbon reduction?",
    options: [
        "AWS IoT Core for renewable device connectivity, Amazon MSK for high-volume energy data, Amazon EMR for grid analytics, AWS Lambda for energy optimization, and Amazon S3 for energy data storage",
        "Amazon EventBridge for energy events, AWS Batch for grid calculations, Amazon DocumentDB for device data, Amazon API Gateway for utility integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for renewable edge processing, Amazon DynamoDB for energy records, AWS Step Functions for grid workflows, Amazon Redshift for analytics, and Amazon QuickSight for energy dashboards",
        "AWS IoT Core for renewable device management, Amazon Kinesis Data Analytics for real-time grid optimization, Amazon SageMaker for renewable output prediction and carbon footprint minimization, AWS Lambda for peer-to-peer trading automation, and Amazon Timestream for energy performance metrics and grid stability monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure management for 500,000+ renewable energy devices with grid integration. Kinesis Data Analytics enables real-time grid optimization and stability management. SageMaker predicts renewable output and optimizes for carbon footprint reduction. Lambda automates peer-to-peer energy trading with automatic scaling. Timestream tracks energy performance metrics and grid stability monitoring for optimal renewable integration.",
        whyWrong: {
            0: "MSK adds complexity for renewable device communication, EMR is for batch analytics not real-time grid optimization",
            1: "EventBridge and Batch add latency inappropriate for real-time grid stability management",
            2: "IoT Greengrass on every device adds operational complexity, Step Functions add latency for grid optimization"
        },
        examStrategy: "Renewable energy: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for intelligent grid management."
    }
},
{
    id: 'sap_398',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial services platform processes 50 million transactions daily across banking, insurance, and investment services, experiences latency spikes during market volatility, has cross-service data synchronization issues, faces regulatory reporting bottlenecks across 70+ jurisdictions, and needs real-time fraud detection while maintaining sub-second response times for customer transactions.",
    question: "Which optimization strategy provides the BEST financial services performance with comprehensive fraud protection and compliance?",
    options: [
        "Implement Amazon ElastiCache for transaction caching, AWS Lambda for financial processing, Amazon DynamoDB for customer data, Amazon API Gateway for service APIs, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for financial records, Amazon MSK for real-time messaging, AWS Step Functions for financial workflows, Amazon EventBridge for service coordination, and AWS Batch for fraud analysis",
        "Use Amazon DocumentDB for flexible financial schemas, Amazon EventBridge for transaction events, AWS Batch for compliance calculations, Amazon SQS for service communication, and Amazon EMR for fraud analytics",
        "Implement Amazon ElastiCache Redis cluster mode for real-time transaction state, Amazon SageMaker for real-time fraud detection and risk assessment, Amazon DynamoDB Global Tables for customer data synchronization, Amazon Kinesis for transaction event streaming, and AWS Config for multi-jurisdiction regulatory compliance monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-second transaction access for 50M daily transactions with automatic scaling. SageMaker enables real-time fraud detection and risk assessment across banking, insurance, and investment services. DynamoDB Global Tables ensures customer data synchronization across services and regions. Kinesis handles transaction event streaming efficiently. Config monitors regulatory compliance across 70+ jurisdictions continuously.",
        whyWrong: {
            0: "Basic caching doesn't solve cross-service synchronization, CloudWatch doesn't provide fraud detection or compliance monitoring",
            1: "Aurora Global Database has limitations for high-frequency transaction updates, Batch adds latency for real-time fraud detection",
            2: "DocumentDB doesn't optimize for financial transaction patterns, EventBridge adds latency for real-time transaction processing"
        },
        examStrategy: "Financial services optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Config for fraud protection and compliance."
    }
},
{
    id: 'sap_399',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national transportation authority needs to migrate their traffic management and public transit systems from legacy infrastructure to AWS, supporting smart city initiatives, real-time traffic optimization, public safety integration, and citizen services while ensuring high availability for critical transportation infrastructure and meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL smart transportation with comprehensive safety and security?",
    options: [
        "Deploy Amazon EC2 for transportation applications, Amazon RDS for traffic data, AWS Lambda for optimization processing, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Implement AWS Elastic Beanstalk for traffic applications, Amazon Aurora for transportation database, AWS Step Functions for traffic workflows, Amazon CloudFront for citizen portal, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible traffic data, AWS EventBridge for transportation coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management",
        "Deploy AWS Control Tower in GovCloud for government compliance, AWS IoT Core for traffic infrastructure monitoring, Amazon Kinesis Data Analytics for real-time traffic optimization, Amazon DynamoDB for transportation data, and AWS Lambda for public safety integration and citizen services"
    ],
    correct: 3,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for critical transportation infrastructure. IoT Core manages traffic signals, sensors, and transit systems securely. Kinesis Data Analytics enables real-time traffic optimization for smart city initiatives. DynamoDB scales for transportation data with high availability. Lambda handles public safety integration and citizen services with automatic scaling.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance needed for critical transportation infrastructure",
            1: "Elastic Beanstalk doesn't provide government security controls or smart city IoT capabilities",
            2: "EKS adds operational complexity, DocumentDB doesn't optimize for transportation data patterns and government security requirements"
        },
        examStrategy: "Smart transportation: Control Tower GovCloud + IoT Core + Kinesis Analytics + DynamoDB + Lambda for secure city infrastructure."
    }
},
{
    id: 'sap_400',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global biotechnology research platform enables 10,000+ scientists to collaborate on genetic engineering, protein design, and synthetic biology projects, requires massive computational resources for molecular simulations, manages sensitive research data, and enables secure international collaboration while protecting intellectual property and ensuring biosafety compliance.",
    question: "Which architecture provides OPTIMAL biotechnology research with comprehensive IP protection and biosafety compliance?",
    options: [
        "Amazon EC2 with GPU instances for molecular modeling, Amazon S3 for research data, AWS Lambda for genetic analysis, Amazon DynamoDB for research records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for research coordination, Amazon DocumentDB for genetic metadata, and AWS VPN for scientist access",
        "Amazon EMR for biotech analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional collaboration",
        "Amazon SageMaker for AI-driven protein design and genetic analysis, AWS ParallelCluster for massive molecular simulations, AWS Lake Formation for IP protection and biosafety compliance governance, Amazon FSx for Lustre for computational data, and AWS PrivateLink for secure international research collaboration"
    ],
    correct: 3,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for protein design and genetic analysis automation. ParallelCluster delivers massive computational resources for molecular simulations at biotechnology scale. Lake Formation ensures IP protection and biosafety compliance governance through fine-grained access controls. FSx Lustre provides high-performance storage for computational workloads. PrivateLink enables secure international research collaboration while protecting sensitive biotechnology IP.",
        whyWrong: {
            0: "Standard EC2 doesn't provide AI/ML biotechnology capabilities, basic IAM doesn't provide IP protection and biosafety compliance",
            1: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide IP protection for sensitive biotechnology research",
            2: "EMR doesn't provide AI/ML or HPC capabilities for biotechnology research, Redshift doesn't optimize for molecular simulation workflows"
        },
        examStrategy: "Biotechnology research: SageMaker + ParallelCluster + Lake Formation + FSx Lustre + PrivateLink for AI-driven biosafety-compliant research."
    }
},

// SAP-C02 Questions 351-400 (Batch 8) - 50 Questions - PROPERLY RANDOMIZED ANSWERS
// Distribution: 12 A's, 13 B's, 12 C's, 13 D's

// Questions 351-362: correct: 0 (A)
{
    id: 'sap_351',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global cryptocurrency trading platform processes 2 million transactions per second, experiences slippage during high volatility, has liquidity fragmentation across exchanges, faces regulatory compliance challenges in 50+ jurisdictions, and needs sub-millisecond order execution while maintaining audit trails.",
    question: "Which optimization strategy provides the BEST cryptocurrency trading performance with regulatory compliance?",
    options: [
        "Implement Amazon ElastiCache Redis cluster mode for order books, AWS Lambda for trade execution, Amazon DynamoDB Global Tables for transaction records, Amazon Kinesis for real-time market data, and AWS Config for multi-jurisdiction compliance monitoring",
        "Deploy Amazon MemoryDB for trading state, Amazon MSK for high-throughput messaging, AWS Step Functions for trading workflows, Amazon Aurora Global for transaction history, and AWS Security Hub for compliance dashboards",
        "Use Amazon DocumentDB for flexible trading schemas, Amazon EventBridge for market events, AWS Batch for liquidity calculations, Amazon RDS for transaction storage, and AWS CloudTrail for regulatory auditing",
        "Implement Amazon Redshift for trading analytics, Amazon SQS for order queuing, AWS Glue for data processing, Amazon EMR for market analysis, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 2M TPS with automatic sharding. Lambda scales instantly for trade execution. DynamoDB Global Tables handles global transaction records with automatic scaling. Kinesis processes real-time market data efficiently. Config monitors compliance across 50+ jurisdictions continuously.",
        whyWrong: {
            1: "MemoryDB has higher latency than ElastiCache cluster mode for ultra-high frequency trading requirements",
            2: "DocumentDB doesn't optimize for trading performance, EventBridge adds latency for real-time order execution",
            3: "Redshift is for analytics not real-time trading, SQS adds latency violating sub-millisecond requirements"
        },
        examStrategy: "Crypto trading optimization: ElastiCache cluster + Lambda + DynamoDB Global Tables + Kinesis + Config for regulatory compliance."
    }
},
{
    id: 'sap_352',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational pharmaceutical company conducts clinical trials across 90+ countries with varying drug approval regulations, needs patient data protection, research collaboration security, FDA/EMA submission requirements, and intellectual property protection while enabling global research coordination.",
    question: "Which governance architecture ensures COMPREHENSIVE pharmaceutical research compliance and IP protection?",
    options: [
        "AWS Organizations with regulatory jurisdiction OUs, AWS Lake Formation for patient data governance and IP protection, AWS PrivateLink for secure research collaboration, AWS CloudTrail with tamper detection for regulatory audit trails, and AWS Config for continuous FDA/EMA compliance monitoring",
        "AWS Control Tower for research standardization, AWS Security Hub for compliance monitoring, Amazon S3 for research data storage, AWS Lambda for data processing, and AWS Direct Connect for institutional connectivity",
        "AWS Organizations with therapeutic area OUs, AWS Directory Service for researcher authentication, Amazon ElastiCache for research caching, AWS Batch for trial analysis, and AWS Certificate Manager for secure communications",
        "AWS Landing Zone with pharma templates, AWS GuardDuty for threat detection, Amazon Redshift for trial analytics, AWS Glue for data integration, and Amazon QuickSight for research dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with regulatory jurisdiction OUs ensure compliance with 90+ different drug approval regulations. Lake Formation provides patient data governance and IP protection through fine-grained access controls. PrivateLink enables secure research collaboration without internet exposure. CloudTrail with tamper detection provides FDA/EMA-compliant audit trails. Config ensures continuous regulatory compliance monitoring.",
        whyWrong: {
            1: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide IP protection",
            2: "Therapeutic area OUs don't address regulatory jurisdiction requirements, Directory Service doesn't provide IP governance",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the regulatory compliance framework needed"
        },
        examStrategy: "Pharmaceutical governance: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail tamper detection for IP protection."
    }
},
{
    id: 'sap_353',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An advanced robotics platform coordinates 100,000+ autonomous robots across manufacturing, logistics, and service industries, requiring real-time coordination, collision avoidance, task optimization, machine learning adaptation, and fleet management while ensuring safety protocols and operational efficiency.",
    question: "Which architecture provides OPTIMAL autonomous robotics coordination at industrial scale?",
    options: [
        "AWS IoT Core for robot connectivity, Amazon Kinesis Data Streams for real-time telemetry, Amazon SageMaker for ML-driven task optimization, AWS Lambda for collision avoidance algorithms, and Amazon Timestream for robotics performance metrics",
        "Amazon EventBridge for robot coordination, AWS Batch for task calculations, Amazon DocumentDB for robot data, Amazon API Gateway for fleet integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for robot edge computing, Amazon MSK for high-throughput messaging, Amazon EMR for fleet analytics, AWS Step Functions for task workflows, and Amazon S3 for operational data",
        "Amazon EC2 Auto Scaling for robot management, Amazon ElastiCache for coordination caching, Amazon RDS for robot records, AWS Direct Connect for fleet connectivity, and Amazon QuickSight for fleet dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure, scalable connectivity for 100,000+ robots with device management. Kinesis Data Streams handles real-time robot telemetry efficiently. SageMaker enables ML-driven task optimization and adaptation. Lambda provides real-time collision avoidance with automatic scaling. Timestream optimizes robotics performance metrics and safety monitoring.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time robot coordination and collision avoidance",
            2: "IoT Greengrass on every robot adds operational complexity, MSK is overkill for robot communication",
            3: "EC2 Auto Scaling doesn't provide robot-specific connectivity, RDS doesn't optimize for real-time robot coordination"
        },
        examStrategy: "Autonomous robotics: IoT Core + Kinesis + SageMaker + Lambda + Timestream for industrial robot coordination."
    }
},
{
    id: 'sap_354',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national weather service needs to migrate their hurricane prediction models from legacy supercomputers to AWS, requiring real-time atmospheric simulations, ensemble forecasting, public safety alerting, and international meteorological data sharing while maintaining forecast accuracy for emergency management.",
    question: "Which modernization approach provides OPTIMAL weather prediction with emergency response capabilities?",
    options: [
        "Deploy AWS ParallelCluster with HPC-optimized instances for atmospheric modeling, Amazon FSx for Lustre for weather data processing, AWS Batch for ensemble forecasting, Amazon SNS for emergency alerts, and Amazon S3 with Cross-Region Replication for international data sharing",
        "Implement Amazon EMR with Spark clusters, Amazon Redshift for weather data warehousing, AWS Glue for data processing, Amazon QuickSight for forecast visualization, and AWS Direct Connect for international collaboration",
        "Use Amazon ECS with GPU instances, Amazon EFS for shared storage, AWS Step Functions for forecast workflows, Amazon DocumentDB for weather metadata, and AWS VPN for collaborative access",
        "Deploy AWS Fargate for containerized forecasting, Amazon DynamoDB for weather records, Amazon EventBridge for forecast coordination, AWS Lambda for alert processing, and Amazon API Gateway for international APIs"
    ],
    correct: 0,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for hurricane modeling. FSx Lustre handles massive weather datasets with parallel processing. Batch schedules ensemble forecasting efficiently. SNS delivers emergency alerts to public safety systems. S3 with CRR enables international meteorological data sharing.",
        whyWrong: {
            1: "EMR is optimized for big data analytics, not intensive atmospheric modeling computations",
            2: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for weather computing",
            3: "Fargate has resource limitations for weather modeling, Lambda has execution time limits for complex forecasting"
        },
        examStrategy: "Weather prediction: ParallelCluster + HPC + FSx Lustre + Batch + SNS for emergency weather forecasting."
    }
},
{
    id: 'sap_355',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online education platform serves 50 million students, experiences video streaming issues during peak hours, has content delivery delays, faces assessment processing bottlenecks, and needs real-time collaboration features while optimizing bandwidth usage for emerging markets.",
    question: "Which optimization strategy provides the BEST educational platform performance and global accessibility?",
    options: [
        "Implement Amazon CloudFront with origin shield for video delivery, AWS Elemental MediaPackage for adaptive streaming, Amazon DynamoDB for student progress tracking, AWS Lambda for assessment processing, and Amazon Chime SDK for real-time collaboration",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for video processing, Amazon API Gateway for student access, and Amazon ElastiCache for session management",
        "Use Amazon S3 Transfer Acceleration, Amazon ECS for video processing, AWS Step Functions for assessment workflows, Amazon EventBridge for collaboration coordination, and Amazon CloudWatch for monitoring",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Lambda for video processing, Amazon SQS for assessment queuing, and Amazon Connect for student support"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFront with origin shield optimizes video delivery globally and reduces bandwidth costs. MediaPackage provides adaptive streaming for emerging markets with limited bandwidth. DynamoDB scales for 50M student progress tracking. Lambda handles assessment processing with automatic scaling. Chime SDK enables real-time collaboration features.",
        whyWrong: {
            1: "Global Accelerator doesn't provide video streaming optimization, EFS doesn't optimize for educational content delivery",
            2: "Transfer Acceleration doesn't provide video streaming capabilities, Step Functions add latency for real-time assessments",
            3: "Direct Connect is expensive for global content delivery, Glacier has retrieval delays inappropriate for active content"
        },
        examStrategy: "Education platform: CloudFront + origin shield + MediaPackage + DynamoDB + Lambda + Chime SDK for global learning."
    }
},
{
    id: 'sap_356',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global defense contractor manages classified weapons systems across 25+ military programs, requires strict security clearance access controls, maintains air-gapped development environments, enables secure multi-national collaboration, and ensures ITAR compliance while supporting rapid prototyping and testing.",
    question: "Which governance framework ensures MAXIMUM security for classified defense systems development?",
    options: [
        "AWS GovCloud with program-isolated VPCs per classification level, AWS Organizations with ITAR-compliant SCPs, AWS Lake Formation for classified data governance, AWS PrivateLink for secure multi-national collaboration, and AWS CloudTrail with tamper detection for defense audit compliance",
        "AWS Control Tower in commercial regions, AWS Config for ITAR monitoring, AWS PrivateLink for program communication, AWS Backup for data protection, and AWS Client VPN for contractor access",
        "AWS Organizations with contractor-based accounts, AWS Security Hub for clearance monitoring, AWS Direct Connect for government connectivity, AWS KMS for encryption, and Amazon WorkSpaces for secure development",
        "AWS Landing Zone with defense templates, AWS Directory Service for clearance authentication, AWS GuardDuty for threat detection, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 0,
    explanation: {
        correct: "GovCloud with program-isolated VPCs ensures proper classification separation for 25+ programs. Organizations with ITAR-compliant SCPs enforce export control restrictions automatically. Lake Formation provides classified data governance with fine-grained access controls. PrivateLink enables secure multi-national collaboration without internet exposure. CloudTrail with tamper detection provides defense-compliant audit trails.",
        whyWrong: {
            1: "Commercial regions don't meet ITAR requirements for classified weapons systems development",
            2: "Contractor-based accounts don't provide classification-level security isolation needed",
            3: "Landing Zone is deprecated, Directory Service alone doesn't provide the security controls needed for classified development"
        },
        examStrategy: "Defense systems: GovCloud + program-isolated VPCs + ITAR SCPs + Lake Formation + PrivateLink for classified development."
    }
},
{
    id: 'sap_357',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart city energy grid manages 500,000+ IoT devices across power generation, distribution, and consumption, optimizes renewable energy integration, predicts demand patterns, manages grid stability, and provides real-time energy trading while ensuring grid security and resilience.",
    question: "Which architecture provides OPTIMAL smart grid management and energy optimization?",
    options: [
        "AWS IoT Core for grid device management, Amazon Kinesis Data Analytics for real-time energy optimization, Amazon SageMaker for demand prediction, AWS Lambda for grid stability management, and Amazon Timestream for energy metrics tracking",
        "Amazon EventBridge for grid events, AWS Batch for energy calculations, Amazon DocumentDB for device data, Amazon API Gateway for utility integration, and Amazon CloudWatch for grid monitoring",
        "AWS IoT Greengrass for grid edge processing, Amazon MSK for high-volume messaging, Amazon EMR for energy analytics, AWS Step Functions for grid coordination, and Amazon S3 for energy data storage",
        "Amazon EC2 Auto Scaling for grid processing, Amazon ElastiCache for energy caching, Amazon RDS for grid records, AWS Direct Connect for utility connectivity, and Amazon QuickSight for energy dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure management for 500,000+ grid devices with security controls. Kinesis Data Analytics enables real-time energy optimization and renewable integration. SageMaker predicts demand patterns and optimizes energy trading. Lambda manages grid stability with automatic scaling. Timestream tracks energy metrics and grid performance efficiently.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time grid stability management",
            2: "IoT Greengrass on every grid device adds operational complexity, MSK is overkill for grid communication",
            3: "EC2 Auto Scaling doesn't provide grid-specific device management, RDS doesn't optimize for real-time energy data"
        },
        examStrategy: "Smart grid: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for intelligent energy management."
    }
},
{
    id: 'sap_358',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A space exploration agency needs to migrate their mission planning and spacecraft control systems from ground-based infrastructure to AWS, supporting deep space missions, real-time telemetry from Mars rovers, mission-critical decision making, and international space collaboration while maintaining zero-downtime requirements.",
    question: "Which modernization strategy ensures MAXIMUM reliability for space mission operations?",
    options: [
        "Deploy AWS Ground Station for spacecraft communication, Amazon Kinesis Data Streams for real-time telemetry, AWS Lambda for mission-critical processing with provisioned concurrency, Amazon DynamoDB Global Tables for mission state, and AWS Auto Scaling with multi-region failover for zero-downtime requirements",
        "Implement Amazon EC2 in multiple regions, Amazon RDS Aurora for mission data, AWS Step Functions for mission workflows, Amazon API Gateway for ground station integration, and AWS CloudFormation for infrastructure automation",
        "Use AWS Outposts for mission control rooms, Amazon MSK for telemetry streams, AWS Batch for mission calculations, Amazon ElastiCache for performance, and AWS Direct Connect for ground station connectivity",
        "Deploy AWS Wavelength for low-latency processing, Amazon EventBridge for mission coordination, Amazon DocumentDB for spacecraft data, Amazon Redshift for mission analytics, and AWS Transit Gateway for ground station networking"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Ground Station provides managed spacecraft communication infrastructure. Kinesis Data Streams handles real-time telemetry from Mars rovers with automatic scaling. Lambda with provisioned concurrency ensures mission-critical processing without cold starts. DynamoDB Global Tables provides mission state availability across regions. Auto Scaling with multi-region failover ensures zero-downtime requirements.",
        whyWrong: {
            1: "Standard EC2 deployment doesn't provide spacecraft communication capabilities, Step Functions add latency for mission-critical decisions",
            2: "Outposts requires on-premises infrastructure management, Batch adds latency for real-time telemetry processing",
            3: "Wavelength is for mobile edge computing not space missions, EventBridge adds latency for mission-critical operations"
        },
        examStrategy: "Space missions: Ground Station + Kinesis + Lambda provisioned + DynamoDB Global Tables + Auto Scaling for mission-critical reliability."
    }
},
{
    id: 'sap_359',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial trading firm's algorithmic trading platform processes 10 million orders per second, experiences latency spikes during market volatility, has risk management calculation delays, faces regulatory reporting bottlenecks across 40+ jurisdictions, and needs microsecond-level trade execution optimization.",
    question: "Which optimization strategy provides the BEST ultra-high frequency trading performance with regulatory compliance?",
    options: [
        "Implement Amazon ElastiCache Redis cluster mode for order books, AWS Lambda with provisioned concurrency for trade execution, Amazon Kinesis Data Streams for market data, Amazon SageMaker for risk calculations, and AWS Config for multi-jurisdiction regulatory monitoring",
        "Deploy Amazon MemoryDB for trading state persistence, AWS Batch for risk processing, Amazon Aurora Global for trade records, Amazon EventBridge for market events, and AWS Security Hub for compliance dashboards",
        "Use AWS Local Zones for ultra-low latency, Amazon DocumentDB for flexible trading schemas, AWS Step Functions for trade workflows, Amazon MSK for market messaging, and AWS CloudTrail for regulatory auditing",
        "Implement AWS Outposts for on-premises performance, Amazon RDS with read replicas, AWS Glue for regulatory reporting, Amazon EMR for market analysis, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 10M orders/second with automatic sharding. Lambda with provisioned concurrency eliminates cold starts for microsecond-level execution. Kinesis Data Streams handles high-volume market data efficiently. SageMaker provides real-time risk calculations. Config monitors regulatory compliance across 40+ jurisdictions continuously.",
        whyWrong: {
            1: "MemoryDB has higher latency than ElastiCache cluster mode, Batch adds latency for real-time risk calculations",
            2: "Local Zones alone don't provide the caching and processing optimization needed, Step Functions add latency",
            3: "Outposts requires infrastructure management, RDS with read replicas can't handle 10M orders/second write volume"
        },
        examStrategy: "Ultra-high frequency trading: ElastiCache cluster + Lambda provisioned + Kinesis + SageMaker + Config for microsecond optimization."
    }
},
{
    id: 'sap_360',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational mining corporation operates in 60+ countries with varying environmental regulations, needs real-time equipment monitoring, environmental compliance reporting, worker safety management, and operational coordination while maintaining independence during connectivity disruptions in remote locations.",
    question: "Which governance architecture provides OPTIMAL mining operations with environmental compliance and safety?",
    options: [
        "AWS Organizations with country-specific OUs for environmental compliance, AWS IoT Core for equipment and safety monitoring, Amazon Kinesis Data Analytics for real-time operational optimization, AWS Outposts for remote site independence, and AWS Config for continuous environmental regulatory monitoring",
        "AWS Control Tower for mining standardization, AWS Security Hub for compliance monitoring, Amazon MSK for equipment communication, AWS Lambda for safety processing, and AWS Direct Connect for site connectivity",
        "AWS Organizations with commodity-based OUs, AWS Directory Service for worker authentication, Amazon ElastiCache for operations caching, AWS Batch for environmental analysis, and AWS Certificate Manager for secure communications",
        "AWS Landing Zone with mining templates, AWS GuardDuty for site security, Amazon Redshift for operations analytics, AWS Glue for compliance data integration, and Amazon QuickSight for safety dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 60+ different environmental regulations. IoT Core manages equipment and safety monitoring systems securely. Kinesis Data Analytics provides real-time operational optimization and safety analysis. Outposts enables remote mining sites to operate independently during connectivity disruptions. Config monitors environmental regulatory compliance continuously.",
        whyWrong: {
            1: "Control Tower doesn't address country-specific environmental regulations, MSK adds complexity for mining equipment communication",
            2: "Commodity-based OUs don't address environmental regulatory requirements, Directory Service doesn't provide environmental governance",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the environmental compliance framework needed"
        },
        examStrategy: "Mining governance: Organizations + country OUs + IoT Core + Kinesis Analytics + Outposts + Config for environmental compliance."
    }
},
{
    id: 'sap_361',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A molecular drug discovery platform enables 5,000+ researchers to simulate protein interactions, design new drug compounds, predict molecular behavior, manage clinical trial data, and collaborate on breakthrough discoveries while protecting intellectual property and ensuring regulatory compliance.",
    question: "Which architecture provides OPTIMAL drug discovery research with comprehensive IP protection?",
    options: [
        "Amazon SageMaker for AI-driven drug discovery, AWS ParallelCluster for molecular simulations, AWS Lake Formation for IP protection and research governance, Amazon FSx for Lustre for computational data, and AWS PrivateLink for secure research collaboration",
        "Amazon EC2 with GPU instances for molecular modeling, Amazon S3 for research data, AWS Lambda for compound analysis, Amazon DynamoDB for research records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for discovery coordination, Amazon DocumentDB for compound metadata, and AWS VPN for researcher access",
        "Amazon EMR for drug analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional collaboration"
    ],
    correct: 0,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for drug discovery and compound design. ParallelCluster delivers HPC performance for molecular simulations. Lake Formation ensures IP protection through fine-grained access controls and regulatory compliance. FSx Lustre provides high-performance storage for computational workloads. PrivateLink enables secure research collaboration while protecting IP.",
        whyWrong: {
            1: "Standard EC2 doesn't provide AI/ML drug discovery capabilities, basic IAM doesn't provide IP protection needed",
            2: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide IP protection for research data",
            3: "EMR doesn't provide AI/ML or HPC capabilities for drug discovery, Redshift is for traditional analytics"
        },
        examStrategy: "Drug discovery: SageMaker + ParallelCluster + Lake Formation + FSx Lustre + PrivateLink for AI-driven IP-protected research."
    }
},
{
    id: 'sap_362',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national tax administration needs to migrate their taxpayer processing systems from legacy mainframes to AWS, handling 200 million tax returns annually, ensuring data privacy, maintaining audit trails, supporting real-time fraud detection, and enabling citizen self-service while meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL taxpayer services with government security compliance?",
    options: [
        "Deploy AWS Control Tower in GovCloud, Amazon DynamoDB for taxpayer records, AWS Lambda for tax processing, Amazon Kinesis Data Analytics for fraud detection, and AWS Config for government security compliance monitoring",
        "Implement AWS Mainframe Modernization for legacy system migration, Amazon RDS for tax data, AWS Step Functions for processing workflows, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Use AWS Elastic Beanstalk for tax applications, Amazon Aurora for taxpayer database, AWS Batch for return processing, Amazon CloudFront for citizen portal, and AWS Security Hub for compliance management",
        "Deploy Amazon EKS for containerized services, Amazon DocumentDB for flexible tax data, AWS EventBridge for processing coordination, AWS Lambda for microservices, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant landing zones and governance. DynamoDB scales to handle 200M tax returns with high availability. Lambda provides serverless tax processing with automatic scaling. Kinesis Data Analytics enables real-time fraud detection. Config ensures continuous government security compliance monitoring.",
        whyWrong: {
            1: "Mainframe Modernization alone doesn't provide the scale needed for 200M returns, Step Functions add latency for high-volume processing",
            2: "Elastic Beanstalk doesn't provide government security controls, Aurora may not scale to 200M taxpayer records efficiently",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for tax processing patterns"
        },
        examStrategy: "Government tax systems: Control Tower GovCloud + DynamoDB + Lambda + Kinesis Analytics + Config for secure citizen services."
    }
},

// Questions 363-375: correct: 1 (B)
{
    id: 'sap_363',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global streaming music platform serves 300 million users, experiences audio quality issues during peak hours, has music recommendation delays, faces content licensing complexities across regions, and needs real-time playlist synchronization while optimizing bandwidth costs for mobile users.",
    question: "Which optimization strategy provides the BEST music streaming performance and cost efficiency?",
    options: [
        "Implement Amazon CloudFront for audio delivery, AWS Lambda for recommendation processing, Amazon DynamoDB for user preferences, Amazon API Gateway for mobile apps, and Amazon S3 for music storage",
        "Deploy Amazon CloudFront with Lambda@Edge for adaptive audio streaming, Amazon SageMaker for real-time music recommendations, Amazon DynamoDB Global Tables for playlist synchronization, AWS Elemental MediaConvert for audio optimization, and Amazon ElastiCache for user session management",
        "Use AWS Global Accelerator for performance, Amazon EFS for music storage, AWS Batch for recommendation processing, Amazon EventBridge for playlist coordination, and Amazon CloudWatch for monitoring",
        "Implement AWS Direct Connect for audio delivery, Amazon Glacier for music archival, AWS Step Functions for recommendation workflows, Amazon DocumentDB for user data, and Amazon Route 53 for traffic routing"
    ],
    correct: 1,
    explanation: {
        correct: "CloudFront with Lambda@Edge enables adaptive audio streaming based on bandwidth conditions, reducing costs for mobile users. SageMaker provides real-time music recommendations with low latency. DynamoDB Global Tables ensures playlist synchronization across regions. MediaConvert optimizes audio for different devices and bandwidth. ElastiCache provides fast user session access.",
        whyWrong: {
            0: "Basic Lambda processing doesn't provide real-time recommendation capabilities, lacks adaptive streaming for bandwidth optimization",
            2: "Global Accelerator doesn't provide audio streaming optimization, EFS doesn't optimize for music content delivery",
            3: "Direct Connect is expensive for global music delivery, Glacier has retrieval delays inappropriate for active music content"
        },
        examStrategy: "Music streaming: CloudFront Lambda@Edge + SageMaker + DynamoDB Global Tables + MediaConvert + ElastiCache for optimal audio delivery."
    }
},
{
    id: 'sap_364',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global aerospace manufacturer designs aircraft across 15+ international programs, requires ITAR compliance for military projects, manages proprietary design data, enables secure supplier collaboration, and maintains strict export control while supporting concurrent engineering across time zones.",
    question: "Which governance framework ensures COMPREHENSIVE aerospace design security with international collaboration?",
    options: [
        "AWS Organizations with program-based OUs, AWS Config for ITAR monitoring, AWS PrivateLink for supplier access, AWS CloudTrail for design audit trails, and AWS WorkSpaces for secure engineering collaboration",
        "AWS Organizations with ITAR classification OUs, AWS Lake Formation for design data governance and export control, AWS PrivateLink for secure supplier collaboration, AWS GovCloud for military programs, and AWS CloudTrail with tamper detection for compliance audit trails",
        "AWS Control Tower for aerospace standardization, AWS Security Hub for compliance monitoring, Amazon S3 for design storage, AWS Lambda for data processing, and AWS Direct Connect for supplier connectivity",
        "AWS Landing Zone with aerospace templates, AWS Directory Service for engineer authentication, Amazon ElastiCache for design caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with ITAR classification OUs ensure proper export control separation. Lake Formation provides design data governance with fine-grained access controls for proprietary data. PrivateLink enables secure supplier collaboration without internet exposure. GovCloud provides ITAR-compliant infrastructure for military programs. CloudTrail with tamper detection ensures compliance audit trails.",
        whyWrong: {
            0: "Program-based OUs don't address ITAR classification requirements, basic WorkSpaces doesn't provide export control protection",
            2: "Control Tower doesn't address ITAR classification requirements, basic S3 doesn't provide export control governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the export control governance needed for aerospace design"
        },
        examStrategy: "Aerospace governance: Organizations + ITAR OUs + Lake Formation + PrivateLink + GovCloud + CloudTrail tamper detection."
    }
},
{
    id: 'sap_365',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A precision agriculture research platform analyzes crop genetics, soil composition, weather patterns, and farming techniques across 10,000+ research plots globally, optimizes yield predictions, manages field experiments, and enables agricultural research collaboration while protecting proprietary seed data.",
    question: "Which architecture provides OPTIMAL agricultural research with proprietary data protection?",
    options: [
        "AWS IoT Core for field sensor management, Amazon MSK for high-volume agricultural data, Amazon EMR for crop analytics, AWS Lambda for yield processing, and Amazon S3 for research data storage",
        "Amazon Kinesis Data Streams for sensor ingestion, Amazon SageMaker for yield prediction and genetic analysis, AWS Lake Formation for proprietary seed data protection, AWS IoT Core for field device management, and Amazon Timestream for agricultural time-series data",
        "Amazon EventBridge for field events, AWS Batch for genetic calculations, Amazon DocumentDB for crop data, Amazon API Gateway for research integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for field edge processing, Amazon DynamoDB for crop records, AWS Step Functions for research workflows, Amazon Redshift for analytics, and Amazon QuickSight for agricultural dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles sensor data from 10,000+ research plots. SageMaker provides sophisticated yield prediction and genetic analysis capabilities. Lake Formation protects proprietary seed data through fine-grained access controls. IoT Core manages field devices securely. Timestream optimizes agricultural time-series data storage and analysis.",
        whyWrong: {
            0: "MSK adds complexity for agricultural sensor data, EMR is for batch analytics not real-time agricultural optimization",
            2: "EventBridge and Batch add latency inappropriate for real-time agricultural monitoring",
            3: "IoT Greengrass on every plot adds operational complexity, Step Functions add latency for research coordination"
        },
        examStrategy: "Agricultural research: Kinesis + SageMaker + Lake Formation + IoT Core + Timestream for intelligent farming research."
    }
},
{
    id: 'sap_366',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A central bank needs to migrate their monetary policy modeling and financial stability monitoring systems from legacy infrastructure to AWS, supporting real-time economic analysis, international coordination, regulatory reporting, and maintaining the highest security standards for national financial data.",
    question: "Which modernization approach ensures MAXIMUM security for central banking operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for financial data, use AWS Lambda for economic analysis, Amazon API Gateway for international coordination, and AWS CloudTrail for regulatory audit compliance",
        "Deploy AWS GovCloud with isolated VPCs for financial systems, AWS ParallelCluster for economic modeling, AWS Lake Formation for financial data governance, Amazon Kinesis Data Analytics for real-time economic monitoring, and AWS CloudTrail with tamper detection for central bank audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for financial records, AWS Batch for economic calculations, Amazon S3 for policy data, and AWS Config for regulatory compliance",
        "Use AWS Outposts for secure facilities, Amazon FSx for financial data, AWS Step Functions for policy workflows, Amazon ElastiCache for performance, and AWS Direct Connect for international connectivity"
    ],
    correct: 1,
    explanation: {
        correct: "GovCloud with isolated VPCs provides maximum security for national financial systems. ParallelCluster delivers HPC performance for complex economic modeling. Lake Formation provides financial data governance with strict access controls. Kinesis Data Analytics enables real-time economic monitoring. CloudTrail with tamper detection provides central bank audit compliance.",
        whyWrong: {
            0: "Standard deployment doesn't provide the security isolation needed for central bank financial systems",
            2: "Control Tower doesn't provide the security isolation needed for central banking operations",
            3: "Outposts requires on-premises management and doesn't provide the security isolation needed for national financial data"
        },
        examStrategy: "Central banking: GovCloud isolated VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection."
    }
},
{
    id: 'sap_367',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global ride-hailing platform coordinates 5 million drivers across 1,000+ cities, experiences driver-passenger matching delays during surge periods, has dynamic pricing calculation issues, faces real-time location tracking problems at scale, and needs fraud detection while optimizing operational costs.",
    question: "Which optimization strategy provides the BEST ride-hailing platform performance with fraud prevention?",
    options: [
        "Implement Amazon ElastiCache for driver location caching, AWS Lambda for matching algorithms, Amazon DynamoDB for ride data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for monitoring",
        "Deploy Amazon ElastiCache Redis cluster mode for real-time location data, Amazon SageMaker for driver-passenger matching optimization and fraud detection, Amazon DynamoDB Global Tables for ride state management, Amazon Kinesis for location streams, and AWS Lambda for dynamic pricing algorithms",
        "Use Amazon Aurora Global Database for ride records, Amazon MSK for real-time messaging, AWS Step Functions for ride workflows, Amazon EventBridge for surge coordination, and AWS Batch for fraud analysis",
        "Implement Amazon DocumentDB for flexible ride schemas, Amazon EventBridge for location events, AWS Batch for matching calculations, Amazon SQS for ride requests, and Amazon EMR for fraud analytics"
    ],
    correct: 1,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond location access for 5M drivers with automatic sharding. SageMaker optimizes driver-passenger matching and provides real-time fraud detection. DynamoDB Global Tables manages ride state across 1,000+ cities. Kinesis handles real-time location streams efficiently. Lambda scales automatically for dynamic pricing.",
        whyWrong: {
            0: "Basic ElastiCache setup doesn't provide clustering needed for 5M drivers, CloudWatch doesn't provide fraud detection",
            2: "Aurora Global Database has write limitations for high-frequency location updates, Batch adds latency for fraud detection",
            3: "DocumentDB doesn't optimize for ride-hailing patterns, EventBridge adds latency for real-time location processing"
        },
        examStrategy: "Ride-hailing optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for scale and fraud prevention."
    }
},
{
    id: 'sap_368',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational oil and gas company operates drilling platforms and refineries across 40+ countries with varying environmental regulations, needs real-time safety monitoring, environmental compliance reporting, operational coordination, and emergency response capabilities while maintaining operational independence during connectivity issues.",
    question: "Which governance architecture provides OPTIMAL oil and gas operations with environmental compliance and safety?",
    options: [
        "AWS Organizations with facility-based OUs, AWS Config for safety compliance, AWS IoT Core for equipment monitoring, Amazon Kinesis for operational data, and AWS Storage Gateway for operational independence",
        "AWS Organizations with country-specific OUs for environmental compliance, AWS IoT Core for safety and equipment monitoring, Amazon Kinesis Data Analytics for real-time operational optimization, AWS Outposts for remote facility independence, and AWS Config for continuous environmental regulatory monitoring",
        "AWS Control Tower for industry standardization, AWS Security Hub for compliance monitoring, Amazon MSK for facility communication, AWS Lambda for safety processing, and AWS Direct Connect for facility connectivity",
        "AWS Landing Zone with energy templates, AWS Directory Service for worker authentication, Amazon ElastiCache for operations caching, AWS Batch for environmental analysis, and AWS Certificate Manager for secure communications"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 40+ different environmental regulations. IoT Core manages safety and equipment monitoring systems across drilling platforms and refineries. Kinesis Data Analytics provides real-time operational optimization and safety analysis. Outposts enables remote facilities to operate independently during connectivity issues. Config monitors environmental regulatory compliance continuously.",
        whyWrong: {
            0: "Facility-based OUs don't address country-specific environmental regulations, Storage Gateway doesn't provide full operational independence",
            2: "Control Tower doesn't address country-specific environmental regulations, MSK adds complexity for facility communication",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the environmental governance needed for oil and gas operations"
        },
        examStrategy: "Oil and gas governance: Organizations + country OUs + IoT Core + Kinesis Analytics + Outposts + Config for environmental compliance."
    }
},
{
    id: 'sap_369',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A quantum computing research consortium enables 1,000+ physicists globally to access quantum simulators, develop quantum algorithms, collaborate on quantum machine learning, manage experimental results, and advance quantum research while protecting intellectual property and ensuring fair resource allocation.",
    question: "Which architecture provides OPTIMAL quantum research collaboration with comprehensive IP protection?",
    options: [
        "Amazon EC2 with specialized instances for quantum simulation, Amazon S3 for research data, AWS Lambda for algorithm processing, Amazon DynamoDB for experimental records, and AWS IAM for access control",
        "Amazon Braket for quantum computing access, AWS Lake Formation for IP protection and research governance, Amazon SageMaker for quantum ML research, AWS Batch for fair resource scheduling among researchers, and AWS PrivateLink for secure global research collaboration",
        "AWS Batch for quantum job scheduling, Amazon EFS for shared research data, AWS Step Functions for quantum workflows, Amazon DocumentDB for algorithm metadata, and AWS VPN for researcher access",
        "Amazon EMR for quantum analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Amazon Braket provides managed quantum computing access with fair resource allocation among 1,000+ researchers. Lake Formation ensures IP protection through fine-grained access controls and research governance. SageMaker enables quantum machine learning research. Batch provides fair scheduling for quantum jobs. PrivateLink ensures secure global research collaboration while protecting IP.",
        whyWrong: {
            0: "Standard EC2 doesn't provide quantum computing capabilities, basic IAM doesn't provide IP protection needed for research",
            2: "Batch alone doesn't provide quantum computing access, EFS doesn't provide IP protection for quantum research",
            3: "EMR doesn't provide quantum computing capabilities, Redshift is for traditional analytics not quantum research"
        },
        examStrategy: "Quantum research: Braket + Lake Formation + SageMaker + Batch + PrivateLink for collaborative quantum computing research."
    }
},
{
    id: 'sap_370',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national healthcare system needs to migrate their patient records and medical imaging systems from legacy infrastructure to AWS, supporting 100 million patient records, ensuring HIPAA compliance, enabling provider collaboration, and maintaining high availability for emergency care while protecting patient privacy.",
    question: "Which modernization approach provides OPTIMAL healthcare system performance with comprehensive privacy protection?",
    options: [
        "Deploy Amazon EC2 for healthcare applications, Amazon RDS for patient data, AWS Lambda for medical processing, Amazon API Gateway for provider integration, and AWS CloudTrail for HIPAA audit compliance",
        "Implement AWS Control Tower for healthcare governance, Amazon HealthLake for patient records management, Amazon DynamoDB for medical data, AWS Lake Formation for HIPAA compliance and patient privacy protection, and AWS PrivateLink for secure provider collaboration",
        "Use AWS Elastic Beanstalk for medical applications, Amazon Aurora for patient database, AWS Step Functions for healthcare workflows, Amazon S3 for medical imaging, and AWS Config for compliance monitoring",
        "Deploy Amazon EKS for containerized healthcare services, Amazon DocumentDB for flexible patient data, AWS EventBridge for healthcare coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower provides healthcare-specific governance frameworks. HealthLake is purpose-built for healthcare data management with FHIR support for 100M patient records. DynamoDB scales for medical data with high availability. Lake Formation ensures HIPAA compliance and patient privacy protection. PrivateLink enables secure provider collaboration without internet exposure.",
        whyWrong: {
            0: "Standard deployment doesn't provide healthcare-specific data management and HIPAA compliance frameworks",
            2: "Elastic Beanstalk doesn't provide healthcare-specific controls, Aurora may not optimize for 100M patient records",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for healthcare data patterns and HIPAA requirements"
        },
        examStrategy: "Healthcare systems: Control Tower + HealthLake + DynamoDB + Lake Formation + PrivateLink for HIPAA-compliant patient care."
    }
},
{
    id: 'sap_371',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global e-commerce marketplace with 500 million products experiences search performance degradation, product recommendation delays, inventory synchronization issues across regions, seller performance tracking problems, and payment processing bottlenecks during major sales events.",
    question: "Which optimization strategy provides the BEST marketplace performance and seller experience?",
    options: [
        "Implement Amazon ElastiCache for product caching, AWS Lambda for search processing, Amazon DynamoDB for inventory data, Amazon API Gateway for seller APIs, and Amazon SQS for payment processing",
        "Deploy Amazon OpenSearch for intelligent product search, Amazon SageMaker for real-time recommendations, Amazon DynamoDB Global Tables for inventory synchronization, AWS Lambda for seller analytics, and Amazon Kinesis for payment event processing",
        "Use Amazon Aurora Global Database for product catalog, Amazon MSK for real-time updates, AWS Step Functions for seller workflows, Amazon EventBridge for marketplace coordination, and AWS Batch for payment processing",
        "Implement Amazon DocumentDB for flexible product schemas, Amazon EventBridge for marketplace events, AWS Batch for recommendation calculations, Amazon SQS for seller communication, and Amazon EMR for payment analytics"
    ],
    correct: 1,
    explanation: {
        correct: "OpenSearch provides sophisticated product search capabilities for 500M products with real-time indexing. SageMaker enables real-time recommendation inference. DynamoDB Global Tables ensures inventory synchronization across regions. Lambda provides seller analytics with automatic scaling. Kinesis handles payment events during sales spikes efficiently.",
        whyWrong: {
            0: "Basic Lambda search processing can't match OpenSearch capabilities for 500M products, SQS adds latency for payments",
            2: "Aurora Global Database has write limitations for high-frequency inventory updates, Batch adds latency for real-time payments",
            3: "DocumentDB doesn't optimize for marketplace search patterns, EventBridge adds latency for real-time inventory updates"
        },
        examStrategy: "Marketplace optimization: OpenSearch + SageMaker + DynamoDB Global Tables + Lambda + Kinesis for optimal e-commerce performance."
    }
},
{
    id: 'sap_372',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global consulting firm manages 500+ client engagements across 80+ countries with strict confidentiality requirements, implements Chinese walls between competing clients, enables secure collaboration on joint projects, maintains audit trails for compliance, and supports remote workforce while protecting client intellectual property.",
    question: "Which governance framework ensures COMPREHENSIVE client confidentiality and IP protection with operational flexibility?",
    options: [
        "AWS Organizations with client-based OUs, AWS Config for confidentiality compliance, AWS PrivateLink for secure communication, AWS CloudTrail for audit trails, and AWS WorkSpaces for remote collaboration",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for Chinese wall enforcement and client IP protection, AWS PrivateLink for secure project collaboration, AWS CloudTrail with legal hold for audit compliance, and AWS WorkSpaces with DLP for secure remote access",
        "AWS Control Tower for consulting standardization, AWS Security Hub for compliance monitoring, Amazon S3 for client data storage, AWS Lambda for data processing, and AWS Direct Connect for office connectivity",
        "AWS Landing Zone with consulting templates, AWS Directory Service for consultant authentication, Amazon ElastiCache for client caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 80+ different confidentiality regulations. Lake Formation enforces Chinese walls through fine-grained access controls and protects client IP. PrivateLink enables secure project collaboration without internet exposure. CloudTrail with legal hold provides compliant audit trails. WorkSpaces with DLP ensures secure remote access with data loss prevention.",
        whyWrong: {
            0: "Client-based OUs don't address jurisdiction-specific regulations, basic WorkSpaces doesn't provide Chinese wall enforcement",
            2: "Control Tower doesn't address jurisdiction-specific confidentiality requirements, basic S3 doesn't provide Chinese wall controls",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the fine-grained access controls needed for Chinese walls"
        },
        examStrategy: "Consulting governance: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + WorkSpaces DLP."
    }
},
{
    id: 'sap_373',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart transportation system coordinates 50,000+ connected vehicles, 10,000+ traffic signals, and public transit across a major metropolitan area, optimizes traffic flow, manages emergency vehicle routing, provides real-time passenger information, and reduces carbon emissions through intelligent routing.",
    question: "Which architecture provides OPTIMAL smart transportation coordination and environmental optimization?",
    options: [
        "AWS IoT Core for vehicle and infrastructure connectivity, Amazon MSK for high-volume transportation data, Amazon EMR for traffic analytics, AWS Lambda for routing optimization, and Amazon S3 for transportation data storage",
        "Amazon Kinesis Data Streams for real-time vehicle and signal data, Amazon Kinesis Data Analytics for traffic optimization, Amazon SageMaker for route prediction and carbon emission reduction, AWS Lambda for emergency vehicle coordination, and Amazon Timestream for transportation metrics",
        "Amazon EventBridge for transportation events, AWS Batch for route calculations, Amazon DocumentDB for vehicle data, Amazon API Gateway for passenger information, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for vehicle edge processing, Amazon DynamoDB for transportation records, AWS Step Functions for traffic coordination, Amazon Redshift for analytics, and Amazon QuickSight for transportation dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles data from 50,000+ vehicles and 10,000+ signals. Kinesis Data Analytics provides real-time traffic optimization. SageMaker enables intelligent routing for carbon emission reduction and route prediction. Lambda coordinates emergency vehicle routing with automatic scaling. Timestream optimizes transportation time-series data and metrics.",
        whyWrong: {
            0: "MSK adds complexity for transportation data, EMR is for batch analytics not real-time traffic optimization",
            2: "EventBridge and Batch add latency inappropriate for real-time traffic management and emergency routing",
            3: "IoT Greengrass on every vehicle adds operational complexity, Step Functions add latency for traffic coordination"
        },
        examStrategy: "Smart transportation: Kinesis ecosystem + SageMaker + Lambda + Timestream for intelligent urban mobility."
    }
},
{
    id: 'sap_374',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national emergency management agency needs to migrate their disaster response coordination systems from legacy infrastructure to AWS, supporting real-time crisis communication, resource allocation, inter-agency coordination, public alerting, and maintaining operations during natural disasters while ensuring government security compliance.",
    question: "Which modernization strategy ensures MAXIMUM resilience for emergency management operations?",
    options: [
        "Deploy Amazon EC2 in multiple regions, implement Amazon RDS for emergency data, use AWS Lambda for alert processing, Amazon API Gateway for agency coordination, and AWS CloudFormation for infrastructure automation",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB Global Tables for crisis data resilience, AWS IoT Core for emergency sensor networks, Amazon SNS for public alerting, and AWS Outposts for field operations during disasters",
        "Implement AWS Outposts for emergency centers, Amazon MSK for crisis communication, AWS Batch for resource calculations, Amazon ElastiCache for performance, and AWS Direct Connect for agency connectivity",
        "Use AWS Wavelength for emergency response, Amazon EventBridge for crisis coordination, Amazon DocumentDB for emergency data, Amazon Redshift for analytics, and AWS Transit Gateway for agency networking"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for emergency operations. DynamoDB Global Tables ensures crisis data resilience across regions during disasters. IoT Core manages emergency sensor networks securely. SNS delivers public alerts reliably at scale. Outposts enables field operations to continue during infrastructure disruptions.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance and disaster resilience needed",
            2: "Outposts alone doesn't provide the scalability and government compliance needed for national emergency management",
            3: "Wavelength is for mobile edge computing not emergency management, EventBridge doesn't provide disaster resilience"
        },
        examStrategy: "Emergency management: Control Tower GovCloud + DynamoDB Global Tables + IoT Core + SNS + Outposts for disaster resilience."
    }
},
{
    id: 'sap_375',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global logistics network coordinates 2 million packages daily across 5,000+ distribution centers, experiences package tracking delays, has route optimization inefficiencies, faces capacity planning challenges during peak seasons, and needs real-time visibility for customers while reducing operational costs and carbon footprint.",
    question: "Which optimization strategy provides the BEST logistics performance with environmental sustainability?",
    options: [
        "Implement Amazon ElastiCache for package caching, AWS Lambda for tracking updates, Amazon DynamoDB for logistics data, Amazon API Gateway for customer visibility, and Amazon CloudWatch for monitoring",
        "Deploy Amazon DynamoDB Global Tables for package tracking, Amazon SageMaker for route optimization and capacity planning, Amazon Kinesis Data Streams for real-time logistics events, AWS Lambda for carbon footprint calculation, and Amazon API Gateway for customer visibility APIs",
        "Use Amazon Aurora Global Database for package records, Amazon MSK for real-time messaging, AWS Step Functions for logistics workflows, Amazon EventBridge for distribution coordination, and AWS Batch for route calculations",
        "Implement Amazon DocumentDB for flexible logistics schemas, Amazon EventBridge for package events, AWS Batch for optimization calculations, Amazon SQS for distribution communication, and Amazon EMR for analytics"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB Global Tables provides conflict-free package tracking for 2M daily packages across 5,000+ centers. SageMaker optimizes routes and predicts capacity needs for peak seasons while minimizing carbon footprint. Kinesis Data Streams handles real-time logistics events efficiently. Lambda calculates environmental impact automatically. API Gateway provides scalable customer visibility.",
        whyWrong: {
            0: "Basic caching doesn't solve complex logistics optimization, CloudWatch doesn't provide route optimization or sustainability metrics",
            2: "Aurora Global Database has write limitations for high-frequency package updates, Step Functions add latency for real-time logistics",
            3: "DocumentDB doesn't optimize for logistics tracking patterns, EventBridge adds latency for real-time package processing"
        },
        examStrategy: "Logistics optimization: DynamoDB Global Tables + SageMaker + Kinesis + Lambda + API Gateway for sustainable logistics."
    }
},

// Questions 376-387: correct: 2 (C)
{
    id: 'sap_376',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational pharmaceutical research company collaborates with 200+ academic institutions globally on drug development, requires secure data sharing, intellectual property protection, regulatory compliance across jurisdictions, and fair access to research resources while maintaining competitive advantages.",
    question: "Which governance framework ensures OPTIMAL research collaboration with comprehensive IP protection?",
    options: [
        "AWS Organizations with institution-based OUs, AWS Config for research compliance, AWS PrivateLink for academic collaboration, AWS CloudTrail for IP audit trails, and AWS WorkSpaces for secure research access",
        "AWS Control Tower for research standardization, AWS Security Hub for compliance monitoring, Amazon S3 for research data storage, AWS Lambda for data processing, and AWS Direct Connect for institutional connectivity",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for IP protection and research data governance, AWS PrivateLink for secure academic collaboration, AWS CloudTrail with legal hold for patent audit trails, and AWS Resource Access Manager for controlled resource sharing",
        "AWS Landing Zone with research templates, AWS Directory Service for researcher authentication, Amazon ElastiCache for research caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with varying drug development regulations globally. Lake Formation provides IP protection and research data governance with fine-grained access controls. PrivateLink enables secure academic collaboration without internet exposure. CloudTrail with legal hold provides patent-compliant audit trails. RAM enables controlled sharing of research resources among 200+ institutions.",
        whyWrong: {
            0: "Institution-based OUs don't address jurisdiction-specific regulatory requirements, basic WorkSpaces doesn't provide IP protection",
            1: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide IP governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the IP protection needed for pharmaceutical research"
        },
        examStrategy: "Pharmaceutical research: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + RAM for secure collaboration."
    }
},
{
    id: 'sap_377',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global climate monitoring network processes data from 100,000+ weather stations, satellites, and ocean buoys, predicts climate patterns, models environmental changes, supports international climate research, and provides early warning systems for extreme weather events while enabling scientific collaboration.",
    question: "Which architecture provides OPTIMAL climate monitoring and research collaboration capabilities?",
    options: [
        "AWS IoT Core for sensor connectivity, Amazon MSK for high-volume climate data, Amazon EMR for climate analytics, AWS Lambda for weather processing, and Amazon S3 for environmental data storage",
        "Amazon EventBridge for climate events, AWS Batch for modeling calculations, Amazon DocumentDB for climate data, Amazon API Gateway for research integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Core for global sensor management, Amazon Kinesis Data Analytics for real-time climate analysis, Amazon SageMaker for climate prediction modeling, AWS Lambda for extreme weather alerting, and Amazon S3 with Cross-Region Replication for international research collaboration",
        "AWS IoT Greengrass for sensor edge processing, Amazon DynamoDB for climate records, AWS Step Functions for research workflows, Amazon Redshift for analytics, and Amazon QuickSight for climate dashboards"
    ],
    correct: 2,
    explanation: {
        correct: "IoT Core provides secure management for 100,000+ global sensors with device security. Kinesis Data Analytics processes real-time climate data for pattern detection. SageMaker enables sophisticated climate prediction and environmental modeling. Lambda provides extreme weather early warning systems. S3 with CRR enables international climate research collaboration and data sharing.",
        whyWrong: {
            0: "MSK adds complexity for climate sensor data, EMR is for batch analytics not real-time climate monitoring",
            1: "EventBridge and Batch add latency inappropriate for real-time climate monitoring and early warning systems",
            3: "IoT Greengrass on every sensor adds operational complexity, Step Functions add latency for real-time climate analysis"
        },
        examStrategy: "Climate monitoring: IoT Core + Kinesis Analytics + SageMaker + Lambda + S3 CRR for global climate research."
    }
},
{
    id: 'sap_378',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global food delivery platform coordinates 1 million drivers across 2,000+ cities, experiences delivery time prediction errors, has driver-restaurant coordination issues, faces dynamic pricing calculation delays, and needs real-time order optimization while reducing operational costs and environmental impact.",
    question: "Which optimization strategy provides the BEST food delivery performance with sustainability focus?",
    options: [
        "Implement Amazon ElastiCache for restaurant caching, AWS Lambda for delivery processing, Amazon DynamoDB for order data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for order records, Amazon MSK for real-time messaging, AWS Step Functions for delivery workflows, Amazon EventBridge for coordination, and AWS Batch for pricing calculations",
        "Use Amazon ElastiCache Redis cluster mode for real-time location data, Amazon SageMaker for delivery optimization and carbon footprint reduction, Amazon DynamoDB Global Tables for order state management, Amazon Kinesis for location streams, and AWS Lambda for driver-restaurant coordination",
        "Implement Amazon DocumentDB for flexible order schemas, Amazon EventBridge for delivery events, AWS Batch for route calculations, Amazon SQS for coordination, and Amazon EMR for sustainability analytics"
    ],
    correct: 2,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond location access for 1M drivers across 2,000+ cities. SageMaker optimizes delivery routes for efficiency and carbon footprint reduction. DynamoDB Global Tables manages order state globally. Kinesis handles real-time location streams efficiently. Lambda coordinates driver-restaurant communication automatically.",
        whyWrong: {
            0: "Basic caching doesn't solve delivery optimization, CloudWatch doesn't provide route optimization or sustainability features",
            1: "Aurora Global Database has write limitations for high-frequency location updates, Batch adds latency for real-time pricing",
            3: "DocumentDB doesn't optimize for food delivery patterns, EventBridge adds latency for real-time location processing"
        },
        examStrategy: "Food delivery optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for sustainable delivery."
    }
},
{
    id: 'sap_379',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national intelligence agency needs to migrate their threat analysis and counter-intelligence systems from air-gapped networks to AWS, maintaining the highest classification levels, enabling real-time threat assessment, supporting inter-agency intelligence sharing, and ensuring absolute security isolation while advancing analytical capabilities.",
    question: "Which modernization approach ensures MAXIMUM security for classified intelligence operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for intelligence data, use AWS Lambda for threat analysis, Amazon API Gateway for agency integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for threat records, AWS Batch for analysis workflows, Amazon S3 for intelligence data, and AWS Config for classification compliance",
        "Deploy AWS GovCloud with classification-isolated VPCs and air-gapped networking, AWS ParallelCluster for advanced threat analysis computing, AWS Lake Formation for intelligence data governance, Amazon Kinesis Data Analytics for real-time threat assessment, and AWS CloudTrail with tamper detection for audit compliance",
        "Use AWS Outposts in secure facilities, Amazon FSx for intelligence data, AWS Step Functions for analysis workflows, Amazon ElastiCache for performance, and AWS Direct Connect for inter-agency connectivity"
    ],
    correct: 2,
    explanation: {
        correct: "GovCloud with classification-isolated VPCs and air-gapped networking provides maximum security separation for classified intelligence. ParallelCluster delivers advanced computing for sophisticated threat analysis. Lake Formation provides intelligence data governance with strict access controls. Kinesis Data Analytics enables real-time threat assessment. CloudTrail with tamper detection ensures audit compliance for intelligence operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide classification-level security isolation needed for intelligence operations",
            1: "Control Tower doesn't provide classification-isolated security needed for counter-intelligence",
            3: "Outposts requires on-premises management, Direct Connect may compromise air-gapped security requirements"
        },
        examStrategy: "Intelligence operations: GovCloud air-gapped VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection."
    }
},
{
    id: 'sap_380',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment management firm with $5 trillion assets under management operates across 70+ jurisdictions, requires real-time risk assessment, algorithmic trading capabilities, regulatory reporting per jurisdiction, client data protection, and secure communication with fund managers while maintaining competitive trading advantages.",
    question: "Which governance framework ensures COMPREHENSIVE investment management with competitive protection?",
    options: [
        "AWS Organizations with asset class OUs, AWS Config for investment compliance, AWS Lambda for trading algorithms, Amazon DynamoDB for portfolio data, and AWS PrivateLink for manager communication",
        "AWS Control Tower for investment standardization, AWS Security Hub for compliance monitoring, Amazon Kinesis for trading data, AWS Step Functions for investment workflows, and AWS Direct Connect for regulatory connectivity",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for client data protection and competitive intelligence governance, Amazon ElastiCache for real-time risk assessment, AWS Lambda for algorithmic trading, and AWS CloudTrail with legal hold for regulatory audit trails",
        "AWS Landing Zone with investment templates, AWS Directory Service for trader authentication, Amazon ElastiCache for portfolio caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for fund management"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 70+ different financial regulations. Lake Formation provides client data protection and competitive intelligence governance through fine-grained access controls. ElastiCache enables real-time risk assessment for $5T AUM. Lambda scales for algorithmic trading with competitive advantages. CloudTrail with legal hold provides regulatory audit trails.",
        whyWrong: {
            0: "Asset class OUs don't address jurisdiction-specific regulatory requirements",
            1: "Control Tower doesn't address jurisdiction-specific investment regulations",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the data governance needed for competitive protection"
        },
        examStrategy: "Investment management: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for competitive protection."
    }
},
{
    id: 'sap_381',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A precision medicine platform analyzes genetic data from 10 million patients, develops personalized treatment plans, predicts drug responses, matches patients to clinical trials, and enables medical research collaboration while maintaining strict patient privacy and regulatory compliance across multiple countries.",
    question: "Which architecture provides OPTIMAL precision medicine capabilities with comprehensive privacy protection?",
    options: [
        "Amazon EC2 with GPU instances for genetic analysis, Amazon S3 for patient data, AWS Lambda for treatment planning, Amazon DynamoDB for medical records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for treatment coordination, Amazon DocumentDB for patient metadata, and AWS VPN for researcher access",
        "Amazon SageMaker for AI-driven treatment prediction and drug response analysis, AWS ParallelCluster for genetic sequencing analysis, AWS Lake Formation for patient privacy governance and regulatory compliance, Amazon HealthLake for medical data management, and AWS PrivateLink for secure research collaboration",
        "Amazon EMR for genetic analytics, Amazon Redshift for medical data warehousing, AWS Glue for data processing, Amazon QuickSight for medical visualization, and AWS Direct Connect for healthcare collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "SageMaker provides AI/ML for personalized treatment prediction and drug response analysis. ParallelCluster delivers HPC performance for genetic sequencing of 10M patients. Lake Formation ensures patient privacy governance and multi-country regulatory compliance. HealthLake provides purpose-built medical data management and clinical trial matching. PrivateLink enables secure research collaboration while protecting patient data.",
        whyWrong: {
            0: "Standard EC2 doesn't provide AI/ML precision medicine capabilities, basic IAM doesn't provide patient privacy protection",
            1: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide privacy protection for patient genetic data",
            3: "EMR doesn't provide AI/ML or specialized genetic analysis capabilities, Redshift doesn't optimize for precision medicine workflows"
        },
        examStrategy: "Precision medicine: SageMaker + ParallelCluster + Lake Formation + HealthLake + PrivateLink for AI-driven personalized healthcare."
    }
},
{
    id: 'sap_382',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global telecommunications network serves 500 million subscribers across 80+ countries, experiences network congestion during peak hours, has 5G rollout optimization challenges, faces real-time fault detection requirements, and needs predictive maintenance for network infrastructure while reducing operational costs and energy consumption.",
    question: "Which optimization strategy provides the BEST telecommunications network performance with sustainability focus?",
    options: [
        "Implement Amazon ElastiCache for network caching, AWS Lambda for fault processing, Amazon DynamoDB for network data, Amazon API Gateway for subscriber services, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for network records, Amazon MSK for real-time messaging, AWS Step Functions for network workflows, Amazon EventBridge for fault coordination, and AWS Batch for maintenance calculations",
        "Use Amazon ElastiCache Redis cluster mode for real-time network state, Amazon SageMaker for predictive maintenance and 5G optimization, Amazon DynamoDB Global Tables for subscriber management, Amazon Kinesis for network telemetry, and AWS Lambda for fault detection and energy optimization",
        "Implement Amazon DocumentDB for flexible network schemas, Amazon EventBridge for network events, AWS Batch for optimization calculations, Amazon SQS for fault handling, and Amazon EMR for sustainability analytics"
    ],
    correct: 2,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides real-time network state access for 500M subscribers with automatic scaling. SageMaker enables predictive maintenance and 5G rollout optimization while reducing energy consumption. DynamoDB Global Tables manages subscribers across 80+ countries. Kinesis handles network telemetry efficiently. Lambda provides real-time fault detection and energy optimization.",
        whyWrong: {
            0: "Basic caching doesn't solve network optimization, CloudWatch doesn't provide predictive maintenance or 5G optimization",
            1: "Aurora Global Database has limitations for high-frequency network updates, Batch adds latency for real-time fault detection",
            3: "DocumentDB doesn't optimize for telecommunications patterns, EventBridge adds latency for real-time network management"
        },
        examStrategy: "Telecom optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for sustainable networks."
    }
},
{
    id: 'sap_383',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national social security administration needs to migrate their benefits processing systems from legacy mainframes to AWS, handling 100 million beneficiaries, ensuring data privacy, maintaining fraud detection capabilities, supporting citizen self-service, and ensuring high availability for critical benefit payments while meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL citizen services with comprehensive security and fraud protection?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for beneficiary data, use AWS Lambda for benefits processing, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Implement AWS Elastic Beanstalk for benefits applications, Amazon Aurora for beneficiary database, AWS Step Functions for processing workflows, Amazon CloudFront for citizen portal, and AWS Config for compliance monitoring",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB for beneficiary records, Amazon SageMaker for fraud detection, AWS Lambda for benefits processing, and Amazon Kinesis Data Analytics for real-time payment monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible beneficiary data, AWS EventBridge for processing coordination, AWS Batch for benefits calculations, and AWS Security Hub for compliance management"
    ],
    correct: 2,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance and security controls. DynamoDB scales to handle 100M beneficiaries with high availability for critical payments. SageMaker provides sophisticated fraud detection capabilities. Lambda handles benefits processing with automatic scaling. Kinesis Data Analytics enables real-time payment monitoring for fraud prevention.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance and fraud detection needed for social security",
            1: "Elastic Beanstalk doesn't provide government security controls, Aurora may not scale optimally for 100M beneficiaries",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for social security processing patterns"
        },
        examStrategy: "Social security modernization: Control Tower GovCloud + DynamoDB + SageMaker + Lambda + Kinesis Analytics for secure citizen services."
    }
},
{
    id: 'sap_384',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational aerospace consortium collaborates on space exploration projects across 25+ countries with different export control regulations, requires secure technology sharing, intellectual property protection, international coordination, and compliance with space treaties while enabling rapid innovation and development.",
    question: "Which governance framework ensures OPTIMAL space project collaboration with export control compliance?",
    options: [
        "AWS Organizations with project-based OUs, AWS Config for space compliance, AWS PrivateLink for international collaboration, AWS CloudTrail for technology audit trails, and AWS WorkSpaces for secure development",
        "AWS Control Tower for aerospace standardization, AWS Security Hub for compliance monitoring, Amazon S3 for space data storage, AWS Lambda for data processing, and AWS Direct Connect for international connectivity",
        "AWS Organizations with country-specific OUs for export control compliance, AWS Lake Formation for technology sharing governance and IP protection, AWS PrivateLink for secure international collaboration, AWS CloudTrail with legal hold for space treaty audit trails, and AWS GovCloud for sensitive space technology",
        "AWS Landing Zone with aerospace templates, AWS Directory Service for engineer authentication, Amazon ElastiCache for space data caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 25+ different export control regulations for space technology. Lake Formation provides technology sharing governance and IP protection through fine-grained access controls. PrivateLink enables secure international collaboration without internet exposure. CloudTrail with legal hold provides space treaty audit trails. GovCloud provides additional security for sensitive space technology.",
        whyWrong: {
            0: "Project-based OUs don't address country-specific export control requirements, basic WorkSpaces doesn't provide IP protection",
            1: "Control Tower doesn't address country-specific export control regulations, basic S3 doesn't provide technology governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the export control governance needed for space technology"
        },
        examStrategy: "Space collaboration: Organizations + country OUs + Lake Formation + PrivateLink + CloudTrail legal hold + GovCloud for export control."
    }
},
{
    id: 'sap_385',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An autonomous shipping fleet management platform coordinates 5,000+ cargo vessels globally, optimizes maritime routes, manages port scheduling, monitors weather conditions, ensures maritime safety compliance, and enables predictive maintenance while reducing fuel consumption and environmental impact.",
    question: "Which architecture provides OPTIMAL autonomous maritime operations with environmental sustainability?",
    options: [
        "AWS IoT Core for vessel connectivity, Amazon MSK for maritime data, Amazon EMR for route analytics, AWS Lambda for navigation processing, and Amazon S3 for voyage data storage",
        "Amazon EventBridge for maritime events, AWS Batch for route calculations, Amazon DocumentDB for vessel data, Amazon API Gateway for port integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for vessel edge computing, Amazon Kinesis Data Streams for real-time vessel telemetry, Amazon SageMaker for route optimization and fuel efficiency, AWS Lambda for collision avoidance and safety compliance, and Amazon Timestream for maritime performance metrics",
        "AWS Satellite for vessel communication, Amazon MSK for data streaming, Amazon Redshift for voyage analytics, AWS Step Functions for port coordination, and Amazon QuickSight for fleet dashboards"
    ],
    correct: 2,
    explanation: {
        correct: "IoT Greengrass enables autonomous decision-making at sea when connectivity is limited for 5,000+ vessels. Kinesis Data Streams handles real-time vessel telemetry efficiently. SageMaker provides route optimization for fuel efficiency and environmental impact reduction. Lambda ensures collision avoidance and maritime safety compliance. Timestream optimizes maritime performance metrics and sustainability tracking.",
        whyWrong: {
            0: "MSK adds complexity for vessel communication, EMR is for batch analytics not real-time maritime operations",
            1: "EventBridge and Batch add latency inappropriate for real-time maritime navigation and safety",
            3: "Satellite alone doesn't provide comprehensive fleet management, Step Functions add latency for real-time port coordination"
        },
        examStrategy: "Autonomous shipping: IoT Greengrass + Kinesis + SageMaker + Lambda + Timestream for sustainable maritime operations."
    }
},
{
    id: 'sap_386',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global video streaming platform serves 800 million subscribers, experiences quality degradation during major live events, has content recommendation algorithm delays, faces regional content licensing complexities, and needs real-time viewer analytics while optimizing bandwidth costs and content delivery efficiency.",
    question: "Which optimization strategy provides the BEST streaming performance with cost efficiency and personalization?",
    options: [
        "Implement Amazon CloudFront for content delivery, AWS Lambda for recommendation processing, Amazon DynamoDB for viewer data, Amazon API Gateway for mobile apps, and Amazon S3 for video storage",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for video processing, Amazon EventBridge for viewer coordination, and Amazon ElastiCache for session management",
        "Use Amazon CloudFront with Lambda@Edge for adaptive streaming and regional content control, Amazon SageMaker for real-time personalized recommendations, Amazon DynamoDB Global Tables for viewer state management, AWS Elemental MediaServices for content optimization, and Amazon Kinesis for real-time viewer analytics",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Step Functions for recommendation workflows, Amazon DocumentDB for viewer data, and Amazon Route 53 for traffic routing"
    ],
    correct: 2,
    explanation: {
        correct: "CloudFront with Lambda@Edge enables adaptive streaming based on bandwidth and regional content licensing controls. SageMaker provides real-time personalized recommendations for 800M subscribers. DynamoDB Global Tables manages viewer state globally. Elemental MediaServices optimizes content delivery efficiency. Kinesis provides real-time viewer analytics for content optimization.",
        whyWrong: {
            0: "Basic Lambda processing doesn't provide real-time recommendation capabilities or regional content control",
            1: "Global Accelerator doesn't provide video streaming optimization, EFS doesn't optimize for video content delivery",
            3: "Direct Connect is expensive for global content delivery, Glacier has retrieval delays inappropriate for active streaming content"
        },
        examStrategy: "Video streaming: CloudFront Lambda@Edge + SageMaker + DynamoDB Global Tables + Elemental MediaServices + Kinesis for optimized streaming."
    }
},
{
    id: 'sap_387',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national meteorological service needs to migrate their weather prediction supercomputing systems from legacy infrastructure to AWS, supporting global climate modeling, severe weather forecasting, international data exchange, and public safety alerting while maintaining forecast accuracy and enabling climate research collaboration.",
    question: "Which modernization strategy ensures OPTIMAL weather prediction with international collaboration capabilities?",
    options: [
        "Deploy Amazon EMR with Spark clusters, implement Amazon S3 for weather data, use AWS Glue for data processing, Amazon Redshift for climate analytics, and AWS Direct Connect for international collaboration",
        "Implement Amazon ECS with GPU instances, use Amazon EFS for shared storage, AWS Step Functions for forecast workflows, Amazon DocumentDB for weather metadata, and AWS VPN for collaborative access",
        "Deploy AWS ParallelCluster with HPC-optimized instances for atmospheric modeling, Amazon FSx for Lustre for weather data processing, AWS Batch for ensemble forecasting, Amazon S3 with Cross-Region Replication for international data exchange, and AWS Ground Station for satellite weather data ingestion",
        "Use AWS Fargate for containerized forecasting, Amazon DynamoDB for weather records, Amazon EventBridge for forecast coordination, AWS Lambda for alert processing, and Amazon API Gateway for international APIs"
    ],
    correct: 2,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for global climate modeling. FSx Lustre handles massive weather datasets with parallel processing. Batch schedules ensemble forecasting efficiently. S3 with CRR enables international meteorological data exchange. Ground Station ingests satellite weather data directly for improved forecast accuracy.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive atmospheric modeling computations",
            1: "ECS doesn't provide HPC optimizations, EFS doesn't match FSx Lustre performance for weather computing",
            3: "Fargate has resource limitations for weather modeling, Lambda has execution time limits for complex atmospheric simulations"
        },
        examStrategy: "Weather prediction: ParallelCluster + HPC + FSx Lustre + Batch + S3 CRR + Ground Station for meteorological collaboration."
    }
},

// Questions 388-400: correct: 3 (D)
{
    id: 'sap_388',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global humanitarian organization coordinates disaster relief operations across 120+ countries with varying political situations, requires rapid deployment capabilities, secure communication in crisis zones, resource coordination, donor transparency, and compliance with international aid regulations while maintaining operational continuity during emergencies and conflicts.",
    question: "Which governance framework ensures COMPREHENSIVE humanitarian operations with crisis resilience?",
    options: [
        "AWS Organizations with program-based OUs, AWS Config for aid compliance, AWS IoT Core for field monitoring, Amazon Kinesis for crisis data, and AWS Storage Gateway for operational continuity",
        "AWS Control Tower for humanitarian standardization, AWS Security Hub for compliance monitoring, Amazon MSK for crisis communication, AWS Lambda for resource coordination, and AWS Direct Connect for field connectivity",
        "AWS Organizations with regional OUs, AWS Lake Formation for donor transparency, Amazon ElastiCache for resource caching, AWS Batch for aid distribution analysis, and AWS Certificate Manager for secure communications",
        "AWS Organizations with country-specific OUs for regulatory compliance, AWS Outposts for rapid crisis deployment, AWS Lake Formation for donor transparency and resource governance, Amazon Kinesis Data Analytics for real-time crisis coordination, and AWS Satellite for communication in remote disaster areas"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 120+ different aid regulations and political requirements. Outposts enables rapid deployment in crisis zones with local AWS infrastructure. Lake Formation provides donor transparency and resource governance with audit trails. Kinesis Data Analytics coordinates real-time crisis response and resource allocation. AWS Satellite ensures communication in remote disaster areas without existing infrastructure.",
        whyWrong: {
            0: "Program-based OUs don't address country-specific aid regulations, Storage Gateway doesn't provide crisis deployment capabilities",
            1: "Control Tower doesn't address country-specific humanitarian regulations, MSK doesn't provide crisis zone communication",
            2: "Regional OUs don't provide the granular country compliance needed for aid regulations and political situations"
        },
        examStrategy: "Humanitarian operations: Organizations + country OUs + Outposts + Lake Formation + Kinesis Analytics + Satellite for crisis response."
    }
},
{
    id: 'sap_389',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart city waste management platform monitors 2 million waste bins across a major metropolitan area using IoT sensors, optimizes collection routes, predicts bin fill levels, coordinates fleet management, and provides environmental impact analytics while reducing operational costs and carbon emissions.",
    question: "Which architecture provides OPTIMAL smart waste management with environmental sustainability focus?",
    options: [
        "AWS IoT Core for bin connectivity, Amazon MSK for high-volume waste data, Amazon EMR for route analytics, AWS Lambda for collection optimization, and Amazon S3 for environmental data storage",
        "Amazon EventBridge for waste events, AWS Batch for route calculations, Amazon DocumentDB for bin data, Amazon API Gateway for fleet integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for bin edge processing, Amazon DynamoDB for waste records, AWS Step Functions for collection workflows, Amazon Redshift for analytics, and Amazon QuickSight for environmental dashboards",
        "AWS IoT Core for sensor management, Amazon Kinesis Data Analytics for real-time bin monitoring, Amazon SageMaker for route optimization and fill prediction, AWS Lambda for fleet coordination, and Amazon Timestream for environmental impact tracking and carbon emission analytics"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure, scalable management for 2M waste bin sensors with device management. Kinesis Data Analytics processes real-time bin fill level data for optimization. SageMaker enables sophisticated route optimization and fill level prediction to reduce carbon emissions. Lambda coordinates fleet management automatically. Timestream tracks environmental impact metrics and carbon emission reduction analytics.",
        whyWrong: {
            0: "MSK adds complexity for waste sensor data, EMR is for batch analytics not real-time waste optimization",
            1: "EventBridge and Batch add latency inappropriate for real-time waste collection optimization",
            2: "IoT Greengrass on every bin adds operational complexity, Step Functions add latency for collection coordination"
        },
        examStrategy: "Smart waste management: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for sustainable city operations."
    }
},
{
    id: 'sap_390',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global cryptocurrency exchange processes 5 million transactions per second, experiences order execution delays during market volatility, has liquidity management challenges across trading pairs, faces regulatory compliance requirements in 60+ jurisdictions, and needs real-time risk management while maintaining sub-millisecond latency for high-frequency traders.",
    question: "Which optimization strategy provides the BEST cryptocurrency exchange performance with comprehensive compliance?",
    options: [
        "Implement Amazon ElastiCache for order book caching, AWS Lambda for trade processing, Amazon DynamoDB for transaction data, Amazon API Gateway for trading APIs, and Amazon CloudWatch for monitoring",
        "Deploy Amazon MemoryDB for trading state, Amazon MSK for high-throughput messaging, AWS Step Functions for trading workflows, Amazon Aurora Global for transaction history, and AWS Security Hub for compliance dashboards",
        "Use Amazon DocumentDB for flexible trading schemas, Amazon EventBridge for market events, AWS Batch for liquidity calculations, Amazon RDS for transaction storage, and AWS Config for regulatory compliance",
        "Implement Amazon ElastiCache Redis cluster mode for order books, AWS Lambda with provisioned concurrency for sub-millisecond execution, Amazon DynamoDB Global Tables for transaction records, Amazon Kinesis for real-time market data, and AWS Config for multi-jurisdiction compliance monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 5M TPS with automatic sharding and high availability. Lambda with provisioned concurrency eliminates cold starts for consistent sub-millisecond execution. DynamoDB Global Tables handles global transaction records with automatic scaling. Kinesis processes real-time market data efficiently. Config monitors compliance across 60+ jurisdictions continuously.",
        whyWrong: {
            0: "Basic caching doesn't provide clustering needed for 5M TPS, CloudWatch doesn't provide compliance monitoring",
            1: "MemoryDB has higher latency than ElastiCache cluster mode for sub-millisecond requirements, Step Functions add latency",
            2: "DocumentDB doesn't optimize for trading performance, EventBridge adds latency for real-time order execution"
        },
        examStrategy: "Crypto exchange optimization: ElastiCache cluster + Lambda provisioned + DynamoDB Global Tables + Kinesis + Config for compliant trading."
    }
},
{
    id: 'sap_391',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national education system needs to migrate their student information and learning management systems from legacy infrastructure to AWS, supporting 20 million students, ensuring data privacy, enabling remote learning capabilities, and maintaining high availability for critical educational services while meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL educational services with comprehensive privacy protection?",
    options: [
        "Deploy Amazon EC2 for education applications, Amazon RDS for student data, AWS Lambda for learning processing, Amazon API Gateway for student services, and AWS CloudTrail for privacy audit compliance",
        "Implement AWS Elastic Beanstalk for learning applications, Amazon Aurora for student database, AWS Step Functions for educational workflows, Amazon S3 for learning content, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible student data, AWS EventBridge for educational coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB for student records, Amazon Chime SDK for remote learning capabilities, AWS Lambda for educational processing, and AWS Lake Formation for student data privacy governance"
    ],
    correct: 3,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for educational data. DynamoDB scales to handle 20M students with high availability for critical services. Chime SDK enables remote learning with video and collaboration capabilities. Lambda handles educational processing with automatic scaling. Lake Formation ensures student data privacy governance and FERPA compliance.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance and privacy protection needed for student data",
            1: "Elastic Beanstalk doesn't provide government security controls, Aurora may not scale optimally for 20M students",
            2: "EKS adds operational complexity, DocumentDB doesn't optimize for educational data patterns and privacy requirements"
        },
        examStrategy: "Education modernization: Control Tower GovCloud + DynamoDB + Chime SDK + Lambda + Lake Formation for secure learning."
    }
},
{
    id: 'sap_392',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational energy trading company operates across 45+ countries with different energy regulations, requires real-time commodity trading, cross-border energy transactions, regulatory reporting per jurisdiction, and secure communication between trading desks while maintaining audit trails for compliance and competitive intelligence protection.",
    question: "Which governance framework ensures OPTIMAL energy trading with competitive protection and compliance?",
    options: [
        "AWS Organizations with trading desk OUs, AWS Config for energy compliance, AWS Lambda for trade processing, Amazon DynamoDB for trade records, and AWS PrivateLink for secure communication",
        "AWS Control Tower for energy standardization, AWS Security Hub for compliance monitoring, Amazon MSK for trading messages, AWS Step Functions for trade workflows, and AWS Direct Connect for desk connectivity",
        "AWS Organizations with commodity-based OUs, AWS Directory Service for trader authentication, Amazon ElastiCache for trade caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for competitive intelligence protection and trade governance, Amazon ElastiCache for real-time trading data, AWS Lambda for algorithmic trading, and AWS CloudTrail with legal hold for regulatory audit trails"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 45+ different energy regulations. Lake Formation provides competitive intelligence protection and trade governance through fine-grained access controls. ElastiCache enables real-time commodity trading with microsecond latency. Lambda scales for algorithmic trading strategies. CloudTrail with legal hold provides regulatory audit trails for energy trading compliance.",
        whyWrong: {
            0: "Trading desk OUs don't address jurisdiction-specific regulatory requirements",
            1: "Control Tower doesn't address jurisdiction-specific energy trading regulations",
            2: "Commodity-based OUs don't address jurisdiction-specific compliance requirements, Directory Service doesn't provide competitive protection"
        },
        examStrategy: "Energy trading: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for competitive compliance."
    }
},
{
    id: 'sap_393',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global carbon trading and environmental monitoring platform tracks emissions from 100,000+ industrial facilities, verifies carbon credits, manages environmental compliance, enables carbon offset trading, and provides real-time environmental impact analytics while ensuring data integrity and regulatory transparency.",
    question: "Which architecture provides OPTIMAL carbon trading with environmental compliance and data integrity?",
    options: [
        "AWS IoT Core for facility connectivity, Amazon MSK for high-volume emissions data, Amazon EMR for carbon analytics, AWS Lambda for offset processing, and Amazon S3 for environmental reporting",
        "Amazon EventBridge for facility events, AWS Batch for emissions calculations, Amazon DocumentDB for carbon data, Amazon API Gateway for trading integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for facility edge processing, Amazon DynamoDB for carbon records, AWS Step Functions for trading workflows, Amazon Redshift for analytics, and Amazon QuickSight for environmental dashboards",
        "AWS IoT Core for emissions monitoring, Amazon Kinesis Data Analytics for real-time carbon tracking, Amazon SageMaker for carbon credit verification and offset optimization, AWS Lambda for trading automation, and Amazon QLDB for immutable carbon trading records with AWS Lake Formation for regulatory transparency"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure monitoring for 100,000+ industrial facility emissions with device management. Kinesis Data Analytics tracks carbon emissions in real-time for compliance. SageMaker verifies carbon credits and optimizes offset trading strategies. Lambda automates carbon trading processes. QLDB ensures immutable carbon trading records for data integrity, and Lake Formation provides regulatory transparency and compliance governance.",
        whyWrong: {
            0: "MSK adds complexity for emissions monitoring, EMR is for batch analytics not real-time carbon tracking",
            1: "EventBridge and Batch add latency inappropriate for real-time carbon trading and compliance monitoring",
            2: "IoT Greengrass on every facility adds operational complexity, Step Functions add latency for trading workflows"
        },
        examStrategy: "Carbon trading: IoT Core + Kinesis Analytics + SageMaker + Lambda + QLDB + Lake Formation for environmental compliance."
    }
},
{
    id: 'sap_394',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online gaming platform serves 200 million players across multiple game titles, experiences matchmaking delays during peak hours, has in-game purchase processing issues, faces regional content regulation challenges, and needs real-time player analytics while optimizing server costs and player experience.",
    question: "Which optimization strategy provides the BEST gaming platform performance with cost efficiency and player satisfaction?",
    options: [
        "Implement Amazon ElastiCache for player session caching, AWS Lambda for matchmaking processing, Amazon DynamoDB for player data, Amazon API Gateway for game services, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for player records, Amazon MSK for real-time messaging, AWS Step Functions for game workflows, Amazon EventBridge for player coordination, and AWS Batch for analytics processing",
        "Use Amazon DocumentDB for flexible player schemas, Amazon EventBridge for game events, AWS Batch for matchmaking calculations, Amazon SQS for player communication, and Amazon EMR for player analytics",
        "Implement Amazon GameLift for managed game hosting and matchmaking, Amazon ElastiCache Redis cluster mode for player state, Amazon DynamoDB Global Tables for player progress, Amazon Kinesis for real-time player analytics, and AWS Lambda for in-game purchase processing"
    ],
    correct: 3,
    explanation: {
        correct: "GameLift provides managed game hosting with automatic scaling and intelligent matchmaking for 200M players. ElastiCache Redis cluster mode delivers sub-millisecond player state access with cost optimization. DynamoDB Global Tables manages player progress across regions. Kinesis provides real-time player analytics for game optimization. Lambda handles in-game purchases with automatic scaling and cost efficiency.",
        whyWrong: {
            0: "Basic caching doesn't solve matchmaking optimization for 200M players, CloudWatch doesn't provide game-specific analytics",
            1: "Aurora Global Database has limitations for high-frequency player updates, Step Functions add latency for real-time gaming",
            2: "DocumentDB doesn't optimize for gaming patterns, EventBridge adds latency for real-time player interactions"
        },
        examStrategy: "Gaming optimization: GameLift + ElastiCache cluster + DynamoDB Global Tables + Kinesis + Lambda for optimal player experience."
    }
},
{
    id: 'sap_395',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A defense intelligence agency needs to migrate their signals intelligence and cyber warfare systems from classified networks to AWS, maintaining the highest security clearances, enabling real-time threat analysis, supporting multi-domain operations, and ensuring air-gapped security while advancing analytical capabilities for national security.",
    question: "Which modernization strategy ensures MAXIMUM security for classified defense intelligence operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for intelligence data, use AWS Lambda for threat analysis, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for intelligence records, AWS Batch for analysis workflows, Amazon S3 for classified data, and AWS Config for security compliance",
        "Use AWS Outposts in secure facilities, Amazon FSx for intelligence data, AWS Step Functions for analysis workflows, Amazon ElastiCache for performance, and AWS Direct Connect for agency connectivity",
        "Deploy AWS GovCloud with air-gapped VPCs per security classification, AWS ParallelCluster for advanced threat analysis, AWS Lake Formation for classified data governance, Amazon Kinesis Data Analytics for real-time signals intelligence, and AWS CloudTrail with tamper detection for defense audit compliance"
    ],
    correct: 3,
    explanation: {
        correct: "GovCloud with air-gapped VPCs per security classification provides maximum security separation for classified intelligence. ParallelCluster delivers advanced computing for sophisticated threat analysis and cyber warfare capabilities. Lake Formation provides classified data governance with strict access controls. Kinesis Data Analytics enables real-time signals intelligence processing. CloudTrail with tamper detection ensures defense audit compliance for national security operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide classification-level security isolation needed for defense intelligence",
            1: "Control Tower doesn't provide air-gapped security isolation needed for classified cyber warfare systems",
            2: "Outposts requires on-premises management, Direct Connect may compromise air-gapped security requirements for classified operations"
        },
        examStrategy: "Defense intelligence: GovCloud air-gapped VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection."
    }
},
{
    id: 'sap_396',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global pharmaceutical consortium coordinates drug development across 50+ companies and 200+ research institutions, requires secure intellectual property sharing, regulatory compliance across jurisdictions, competitive collaboration controls, and clinical trial coordination while protecting proprietary research and maintaining fair access to breakthrough discoveries.",
    question: "Which governance framework ensures COMPREHENSIVE pharmaceutical collaboration with IP protection and competitive controls?",
    options: [
        "AWS Organizations with company-based OUs, AWS Config for pharma compliance, AWS PrivateLink for research collaboration, AWS CloudTrail for IP audit trails, and AWS WorkSpaces for secure development",
        "AWS Control Tower for pharmaceutical standardization, AWS Security Hub for compliance monitoring, Amazon S3 for research data storage, AWS Lambda for data processing, and AWS Direct Connect for institutional connectivity",
        "AWS Organizations with therapeutic area OUs, AWS Directory Service for researcher authentication, Amazon ElastiCache for research caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for IP protection and competitive collaboration controls, AWS PrivateLink for secure consortium collaboration, AWS CloudTrail with legal hold for pharmaceutical audit trails, and AWS Resource Access Manager for controlled breakthrough discovery sharing"
    ],
    correct: 3,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with varying pharmaceutical regulations globally. Lake Formation provides IP protection and competitive collaboration controls through fine-grained access controls. PrivateLink enables secure consortium collaboration without internet exposure. CloudTrail with legal hold provides pharmaceutical audit trails for regulatory compliance. RAM enables controlled sharing of breakthrough discoveries among consortium members while protecting proprietary research.",
        whyWrong: {
            0: "Company-based OUs don't address jurisdiction-specific regulatory requirements, basic WorkSpaces doesn't provide competitive controls",
            1: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide IP protection",
            2: "Therapeutic area OUs don't address regulatory jurisdiction requirements, Directory Service doesn't provide competitive collaboration controls"
        },
        examStrategy: "Pharmaceutical consortium: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + RAM for competitive collaboration."
    }
},
{
    id: 'sap_397',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global renewable energy management platform coordinates 500,000+ solar panels and wind turbines across smart grids, optimizes energy distribution, predicts renewable output, manages energy storage, and enables peer-to-peer energy trading while ensuring grid stability and minimizing carbon footprint.",
    question: "Which architecture provides OPTIMAL renewable energy management with grid stability and carbon reduction?",
    options: [
        "AWS IoT Core for renewable device connectivity, Amazon MSK for high-volume energy data, Amazon EMR for grid analytics, AWS Lambda for energy optimization, and Amazon S3 for energy data storage",
        "Amazon EventBridge for energy events, AWS Batch for grid calculations, Amazon DocumentDB for device data, Amazon API Gateway for utility integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for renewable edge processing, Amazon DynamoDB for energy records, AWS Step Functions for grid workflows, Amazon Redshift for analytics, and Amazon QuickSight for energy dashboards",
        "AWS IoT Core for renewable device management, Amazon Kinesis Data Analytics for real-time grid optimization, Amazon SageMaker for renewable output prediction and carbon footprint minimization, AWS Lambda for peer-to-peer trading automation, and Amazon Timestream for energy performance metrics and grid stability monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "IoT Core provides secure management for 500,000+ renewable energy devices with grid integration. Kinesis Data Analytics enables real-time grid optimization and stability management. SageMaker predicts renewable output and optimizes for carbon footprint reduction. Lambda automates peer-to-peer energy trading with automatic scaling. Timestream tracks energy performance metrics and grid stability monitoring for optimal renewable integration.",
        whyWrong: {
            0: "MSK adds complexity for renewable device communication, EMR is for batch analytics not real-time grid optimization",
            1: "EventBridge and Batch add latency inappropriate for real-time grid stability management",
            2: "IoT Greengrass on every device adds operational complexity, Step Functions add latency for grid optimization"
        },
        examStrategy: "Renewable energy: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for intelligent grid management."
    }
},
{
    id: 'sap_398',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial services platform processes 50 million transactions daily across banking, insurance, and investment services, experiences latency spikes during market volatility, has cross-service data synchronization issues, faces regulatory reporting bottlenecks across 70+ jurisdictions, and needs real-time fraud detection while maintaining sub-second response times for customer transactions.",
    question: "Which optimization strategy provides the BEST financial services performance with comprehensive fraud protection and compliance?",
    options: [
        "Implement Amazon ElastiCache for transaction caching, AWS Lambda for financial processing, Amazon DynamoDB for customer data, Amazon API Gateway for service APIs, and Amazon CloudWatch for monitoring",
        "Deploy Amazon Aurora Global Database for financial records, Amazon MSK for real-time messaging, AWS Step Functions for financial workflows, Amazon EventBridge for service coordination, and AWS Batch for fraud analysis",
        "Use Amazon DocumentDB for flexible financial schemas, Amazon EventBridge for transaction events, AWS Batch for compliance calculations, Amazon SQS for service communication, and Amazon EMR for fraud analytics",
        "Implement Amazon ElastiCache Redis cluster mode for real-time transaction state, Amazon SageMaker for real-time fraud detection and risk assessment, Amazon DynamoDB Global Tables for customer data synchronization, Amazon Kinesis for transaction event streaming, and AWS Config for multi-jurisdiction regulatory compliance monitoring"
    ],
    correct: 3,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-second transaction access for 50M daily transactions with automatic scaling. SageMaker enables real-time fraud detection and risk assessment across banking, insurance, and investment services. DynamoDB Global Tables ensures customer data synchronization across services and regions. Kinesis handles transaction event streaming efficiently. Config monitors regulatory compliance across 70+ jurisdictions continuously.",
        whyWrong: {
            0: "Basic caching doesn't solve cross-service synchronization, CloudWatch doesn't provide fraud detection or compliance monitoring",
            1: "Aurora Global Database has limitations for high-frequency transaction updates, Batch adds latency for real-time fraud detection",
            2: "DocumentDB doesn't optimize for financial transaction patterns, EventBridge adds latency for real-time transaction processing"
        },
        examStrategy: "Financial services optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Config for fraud protection and compliance."
    }
},
{
    id: 'sap_399',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national transportation authority needs to migrate their traffic management and public transit systems from legacy infrastructure to AWS, supporting smart city initiatives, real-time traffic optimization, public safety integration, and citizen services while ensuring high availability for critical transportation infrastructure and meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL smart transportation with comprehensive safety and security?",
    options: [
        "Deploy Amazon EC2 for transportation applications, Amazon RDS for traffic data, AWS Lambda for optimization processing, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Implement AWS Elastic Beanstalk for traffic applications, Amazon Aurora for transportation database, AWS Step Functions for traffic workflows, Amazon CloudFront for citizen portal, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible traffic data, AWS EventBridge for transportation coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management",
        "Deploy AWS Control Tower in GovCloud for government compliance, AWS IoT Core for traffic infrastructure monitoring, Amazon Kinesis Data Analytics for real-time traffic optimization, Amazon DynamoDB for transportation data, and AWS Lambda for public safety integration and citizen services"
    ],
    correct: 3,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for critical transportation infrastructure. IoT Core manages traffic signals, sensors, and transit systems securely. Kinesis Data Analytics enables real-time traffic optimization for smart city initiatives. DynamoDB scales for transportation data with high availability. Lambda handles public safety integration and citizen services with automatic scaling.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance needed for critical transportation infrastructure",
            1: "Elastic Beanstalk doesn't provide government security controls or smart city IoT capabilities",
            2: "EKS adds operational complexity, DocumentDB doesn't optimize for transportation data patterns and government security requirements"
        },
        examStrategy: "Smart transportation: Control Tower GovCloud + IoT Core + Kinesis Analytics + DynamoDB + Lambda for secure city infrastructure."
    }
},
{
    id: 'sap_400',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global biotechnology research platform enables 10,000+ scientists to collaborate on genetic engineering, protein design, and synthetic biology projects, requires massive computational resources for molecular simulations, manages sensitive research data, and enables secure international collaboration while protecting intellectual property and ensuring biosafety compliance.",
    question: "Which architecture provides OPTIMAL biotechnology research with comprehensive IP protection and biosafety compliance?",
    options: [
        "Amazon EC2 with GPU instances for molecular modeling, Amazon S3 for research data, AWS Lambda for genetic analysis, Amazon DynamoDB for research records, and AWS IAM for access control",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for research coordination, Amazon DocumentDB for genetic metadata, and AWS VPN for scientist access",
        "Amazon EMR for biotech analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional collaboration",
        "Amazon SageMaker for AI-driven protein design and genetic analysis, AWS ParallelCluster for massive molecular simulations, AWS Lake Formation for IP protection and biosafety compliance governance, Amazon FSx for Lustre for computational data, and AWS PrivateLink for secure international research collaboration"
    ],
    correct: 3,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for protein design and genetic analysis automation. ParallelCluster delivers massive computational resources for molecular simulations at biotechnology scale. Lake Formation ensures IP protection and biosafety compliance governance through fine-grained access controls. FSx Lustre provides high-performance storage for computational workloads. PrivateLink enables secure international research collaboration while protecting sensitive biotechnology IP.",
        whyWrong: {
            0: "Standard EC2 doesn't provide AI/ML biotechnology capabilities, basic IAM doesn't provide IP protection and biosafety compliance",
            1: "Batch alone doesn't provide AI/ML capabilities, EFS doesn't provide IP protection for sensitive biotechnology research",
            2: "EMR doesn't provide AI/ML or HPC capabilities for biotechnology research, Redshift doesn't optimize for molecular simulation workflows"
        },
        examStrategy: "Biotechnology research: SageMaker + ParallelCluster + Lake Formation + FSx Lustre + PrivateLink for AI-driven biosafety-compliant research."
    }
},

// SAP-C02 Questions 401-500 (FINAL BATCH) - 100 Questions - PROPERLY RANDOMIZED ANSWERS
// Distribution: 25 A's, 25 B's, 25 C's, 25 D's

// Questions 401-425: correct: 0 (A)
{
    id: 'sap_401',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational semiconductor manufacturer operates fabs across 12+ countries with strict export controls, requires real-time production monitoring, intellectual property protection, supply chain coordination, and compliance with technology transfer regulations while maintaining competitive advantages in chip design and manufacturing processes.",
    question: "Which governance framework ensures COMPREHENSIVE semiconductor operations with export control compliance and IP protection?",
    options: [
        "AWS Organizations with country-specific OUs for export control compliance, AWS Lake Formation for IP protection and technology transfer governance, AWS IoT Core for fab equipment monitoring, AWS PrivateLink for secure supply chain communication, and AWS CloudTrail with tamper detection for export control audit trails",
        "AWS Control Tower for semiconductor standardization, AWS Security Hub for compliance monitoring, Amazon MSK for fab communication, AWS Lambda for production processing, and AWS Direct Connect for supply chain connectivity",
        "AWS Organizations with product line OUs, AWS Directory Service for engineer authentication, Amazon ElastiCache for production caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for design collaboration",
        "AWS Landing Zone with semiconductor templates, AWS GuardDuty for fab security, Amazon Redshift for production analytics, AWS Glue for supply chain integration, and Amazon QuickSight for manufacturing dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with varying export control regulations across 12+ countries. Lake Formation provides IP protection and technology transfer governance through fine-grained access controls. IoT Core monitors fab equipment securely. PrivateLink enables secure supply chain communication without internet exposure. CloudTrail with tamper detection provides export control audit trails for semiconductor compliance.",
        whyWrong: {
            1: "Control Tower doesn't address country-specific export control regulations, MSK adds complexity for fab equipment communication",
            2: "Product line OUs don't address export control requirements, Directory Service doesn't provide IP protection for semiconductor designs",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the export control framework needed for semiconductor operations"
        },
        examStrategy: "Semiconductor governance: Organizations + country OUs + Lake Formation + IoT Core + PrivateLink + CloudTrail tamper detection for export control."
    }
},
{
    id: 'sap_402',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global logistics optimization platform coordinates 10 million shipments annually across air, sea, and land transport, optimizes multi-modal routes, manages customs clearance, tracks carbon emissions, and provides real-time visibility to customers while reducing transportation costs and environmental impact.",
    question: "Which architecture provides OPTIMAL multi-modal logistics with environmental sustainability and customer visibility?",
    options: [
        "AWS IoT Core for shipment tracking, Amazon Kinesis Data Analytics for route optimization, Amazon SageMaker for cost and carbon emission reduction, AWS Lambda for customs automation, and Amazon API Gateway for customer visibility APIs",
        "Amazon EventBridge for shipment events, AWS Batch for route calculations, Amazon DocumentDB for logistics data, Amazon API Gateway for customer integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for shipment edge processing, Amazon DynamoDB for shipment records, AWS Step Functions for logistics workflows, Amazon Redshift for analytics, and Amazon QuickSight for customer dashboards",
        "Amazon EC2 Auto Scaling for logistics processing, Amazon ElastiCache for shipment caching, Amazon RDS for logistics data, AWS Direct Connect for transport connectivity, and Amazon CloudFront for customer visibility"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure tracking for 10M annual shipments across multiple transport modes. Kinesis Data Analytics enables real-time multi-modal route optimization. SageMaker optimizes for both cost reduction and carbon emission minimization. Lambda automates customs clearance processes. API Gateway provides scalable customer visibility APIs with caching and global distribution.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time logistics optimization and customer visibility",
            2: "IoT Greengrass on every shipment adds operational complexity, Step Functions add latency for real-time route optimization",
            3: "EC2 Auto Scaling doesn't provide IoT tracking capabilities, RDS doesn't optimize for 10M annual shipments scale"
        },
        examStrategy: "Multi-modal logistics: IoT Core + Kinesis Analytics + SageMaker + Lambda + API Gateway for sustainable transportation."
    }
},
{
    id: 'sap_403',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global social media platform serves 3 billion users, experiences content delivery delays during viral events, has content moderation backlogs, faces regional content regulation challenges, and needs real-time sentiment analysis while optimizing infrastructure costs and ensuring user safety across diverse global markets.",
    question: "Which optimization strategy provides the BEST social media platform performance with comprehensive content safety and cost efficiency?",
    options: [
        "Implement Amazon CloudFront with Lambda@Edge for regional content control and viral scaling, Amazon Rekognition for automated content moderation, Amazon Comprehend for real-time sentiment analysis, Amazon DynamoDB Global Tables for user state management, and AWS Auto Scaling for cost-optimized infrastructure",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for content processing, Amazon EventBridge for user coordination, and Amazon ElastiCache for session management",
        "Use Amazon S3 Transfer Acceleration, Amazon ECS for content processing, AWS Step Functions for moderation workflows, Amazon DocumentDB for user data, and Amazon Route 53 for traffic routing",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Lambda for content processing, Amazon SQS for moderation queuing, and Amazon Connect for user support"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFront with Lambda@Edge handles viral content scaling and regional regulation compliance. Rekognition provides automated content moderation at scale for 3B users. Comprehend delivers real-time sentiment analysis for user safety. DynamoDB Global Tables manages user state globally. Auto Scaling optimizes infrastructure costs automatically during traffic variations.",
        whyWrong: {
            1: "Global Accelerator doesn't provide content moderation, EFS doesn't optimize for social media content delivery at 3B user scale",
            2: "Transfer Acceleration doesn't provide content moderation capabilities, Step Functions add latency for real-time sentiment analysis",
            3: "Direct Connect is expensive for global content delivery, Glacier has retrieval delays inappropriate for active social content"
        },
        examStrategy: "Social media optimization: CloudFront Lambda@Edge + Rekognition + Comprehend + DynamoDB Global Tables + Auto Scaling for scale and safety."
    }
},
{
    id: 'sap_404',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national postal service needs to migrate their package tracking and delivery optimization systems from legacy infrastructure to AWS, supporting 1 billion packages annually, ensuring real-time tracking, optimizing delivery routes, and maintaining high availability for critical postal services while meeting government security requirements and improving customer experience.",
    question: "Which modernization approach provides OPTIMAL postal services with comprehensive tracking and route optimization?",
    options: [
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB for package tracking, Amazon SageMaker for route optimization, AWS Lambda for delivery processing, and Amazon API Gateway for customer tracking services",
        "Implement AWS Elastic Beanstalk for postal applications, Amazon Aurora for package database, AWS Step Functions for delivery workflows, Amazon CloudFront for customer portal, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible package data, AWS EventBridge for delivery coordination, AWS Batch for route calculations, and AWS Security Hub for compliance management",
        "Deploy Amazon EC2 for postal applications, Amazon RDS for package data, AWS Lambda for tracking processing, Amazon ElastiCache for performance, and AWS CloudFormation for infrastructure automation"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for national postal services. DynamoDB scales to handle 1B packages annually with high availability for critical services. SageMaker optimizes delivery routes for efficiency and cost reduction. Lambda handles delivery processing with automatic scaling. API Gateway provides scalable customer tracking services with caching.",
        whyWrong: {
            1: "Elastic Beanstalk doesn't provide government security controls needed for national postal services",
            2: "EKS adds operational complexity, DocumentDB doesn't optimize for postal tracking patterns at 1B package scale",
            3: "Standard EC2 deployment doesn't provide government security compliance needed for national postal infrastructure"
        },
        examStrategy: "Postal modernization: Control Tower GovCloud + DynamoDB + SageMaker + Lambda + API Gateway for intelligent postal services."
    }
},
{
    id: 'sap_405',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global consulting firm manages 10,000+ client projects across 100+ countries with strict confidentiality requirements, implements information barriers between competing clients, enables secure collaboration on joint ventures, and maintains audit trails for regulatory compliance while supporting a distributed workforce and protecting client intellectual property.",
    question: "Which governance framework ensures COMPREHENSIVE client confidentiality with operational flexibility for global consulting?",
    options: [
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for information barrier enforcement and client IP protection, AWS PrivateLink for secure project collaboration, AWS CloudTrail with legal hold for audit compliance, and AWS WorkSpaces with DLP for secure distributed workforce access",
        "AWS Control Tower for consulting standardization, AWS Security Hub for compliance monitoring, Amazon S3 for client data storage, AWS Lambda for data processing, and AWS Direct Connect for office connectivity",
        "AWS Organizations with client-based OUs, AWS Config for confidentiality compliance, AWS PrivateLink for secure communication, AWS CloudTrail for audit trails, and AWS WorkSpaces for remote collaboration",
        "AWS Landing Zone with consulting templates, AWS Directory Service for consultant authentication, Amazon ElastiCache for client caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 100+ different confidentiality regulations. Lake Formation enforces information barriers and protects client IP through fine-grained access controls. PrivateLink enables secure project collaboration without internet exposure. CloudTrail with legal hold provides compliant audit trails. WorkSpaces with DLP ensures secure distributed workforce access with data loss prevention.",
        whyWrong: {
            1: "Control Tower doesn't address jurisdiction-specific confidentiality requirements, basic S3 doesn't provide information barrier controls",
            2: "Client-based OUs don't address jurisdiction-specific regulations, basic WorkSpaces doesn't provide information barrier enforcement",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the fine-grained access controls needed for client confidentiality"
        },
        examStrategy: "Global consulting: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + WorkSpaces DLP for client protection."
    }
},
{
    id: 'sap_406',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart manufacturing platform integrates 1 million+ IoT sensors across 500+ factories globally, implements predictive maintenance, optimizes production schedules, manages supply chain disruptions, and enables real-time quality control while reducing energy consumption and waste while ensuring operational safety and regulatory compliance.",
    question: "Which architecture provides OPTIMAL smart manufacturing with comprehensive sustainability and safety?",
    options: [
        "AWS IoT Core for sensor management, Amazon Kinesis Data Analytics for real-time production optimization, Amazon SageMaker for predictive maintenance and quality control, AWS Lambda for supply chain automation, and Amazon Timestream for manufacturing performance metrics and energy consumption tracking",
        "Amazon EventBridge for manufacturing events, AWS Batch for production calculations, Amazon DocumentDB for sensor data, Amazon API Gateway for factory integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for factory edge processing, Amazon DynamoDB for production records, AWS Step Functions for manufacturing workflows, Amazon Redshift for analytics, and Amazon QuickSight for factory dashboards",
        "Amazon EC2 Auto Scaling for manufacturing processing, Amazon ElastiCache for production caching, Amazon RDS for factory data, AWS Direct Connect for factory connectivity, and Amazon CloudFront for dashboard delivery"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure management for 1M+ sensors across 500+ factories with device security. Kinesis Data Analytics enables real-time production optimization and supply chain disruption management. SageMaker provides predictive maintenance and quality control automation. Lambda handles supply chain automation with automatic scaling. Timestream tracks manufacturing performance metrics and energy consumption for sustainability optimization.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time manufacturing optimization and safety systems",
            2: "IoT Greengrass on every sensor adds operational complexity, Step Functions add latency for real-time production optimization",
            3: "EC2 Auto Scaling doesn't provide IoT sensor management, RDS doesn't optimize for 1M+ sensor data at manufacturing scale"
        },
        examStrategy: "Smart manufacturing: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for intelligent sustainable production."
    }
},
{
    id: 'sap_407',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global e-learning platform serves 100 million students across universities and corporations, experiences video buffering during peak hours, has assessment processing delays, faces personalized learning recommendation challenges, and needs real-time collaboration features while optimizing content delivery costs and ensuring accessibility compliance.",
    question: "Which optimization strategy provides the BEST e-learning performance with personalization and accessibility?",
    options: [
        "Implement Amazon CloudFront with origin shield for video delivery, Amazon SageMaker for personalized learning recommendations, Amazon DynamoDB for student progress tracking, AWS Lambda for assessment processing, and Amazon Chime SDK for real-time collaboration",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for video processing, Amazon EventBridge for student coordination, and Amazon ElastiCache for session management",
        "Use Amazon S3 Transfer Acceleration, Amazon ECS for content processing, AWS Step Functions for assessment workflows, Amazon DocumentDB for student data, and Amazon Connect for student support",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Lambda for video processing, Amazon SQS for assessment queuing, and Amazon API Gateway for mobile learning apps"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFront with origin shield optimizes video delivery globally and reduces costs for 100M students. SageMaker provides personalized learning recommendations with real-time inference. DynamoDB scales for student progress tracking with high availability. Lambda handles assessment processing with automatic scaling. Chime SDK enables real-time collaboration features for interactive learning.",
        whyWrong: {
            1: "Global Accelerator doesn't provide video streaming optimization, EFS doesn't optimize for e-learning content delivery at 100M student scale",
            2: "Transfer Acceleration doesn't provide video streaming capabilities, Step Functions add latency for real-time assessments",
            3: "Direct Connect is expensive for global content delivery, Glacier has retrieval delays inappropriate for active learning content"
        },
        examStrategy: "E-learning optimization: CloudFront + origin shield + SageMaker + DynamoDB + Lambda + Chime SDK for personalized learning."
    }
},
{
    id: 'sap_408',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A central bank needs to migrate their monetary policy modeling and financial stability monitoring systems from legacy supercomputers to AWS, supporting real-time economic analysis, international coordination with other central banks, regulatory reporting, and maintaining the highest security standards for national financial data while enabling advanced economic research capabilities.",
    question: "Which modernization approach ensures MAXIMUM security for central banking operations with advanced analytical capabilities?",
    options: [
        "Deploy AWS GovCloud with isolated VPCs for financial systems, AWS ParallelCluster for economic modeling, AWS Lake Formation for financial data governance, Amazon Kinesis Data Analytics for real-time economic monitoring, and AWS CloudTrail with tamper detection for central bank audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for financial records, AWS Batch for economic calculations, Amazon S3 for policy data, and AWS Config for regulatory compliance",
        "Use AWS Outposts for secure facilities, Amazon FSx for financial data, AWS Step Functions for policy workflows, Amazon ElastiCache for performance, and AWS Direct Connect for international coordination",
        "Deploy Amazon EC2 in multiple regions, Amazon RDS Aurora for financial data, AWS Lambda for economic analysis, Amazon API Gateway for international coordination, and AWS CloudFormation for infrastructure automation"
    ],
    correct: 0,
    explanation: {
        correct: "GovCloud with isolated VPCs provides maximum security for national financial systems. ParallelCluster delivers HPC performance for complex economic modeling and monetary policy analysis. Lake Formation provides financial data governance with strict access controls. Kinesis Data Analytics enables real-time economic monitoring. CloudTrail with tamper detection provides central bank audit compliance for financial operations.",
        whyWrong: {
            1: "Control Tower doesn't provide the security isolation needed for central bank financial systems",
            2: "Outposts requires on-premises management and doesn't provide the security isolation needed for national financial data",
            3: "Standard deployment doesn't provide the security controls needed for central bank operations"
        },
        examStrategy: "Central banking: GovCloud isolated VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection for financial security."
    }
},
{
    id: 'sap_409',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational pharmaceutical company conducts clinical trials across 90+ countries with varying drug approval regulations, requires patient data protection across jurisdictions, enables secure multi-site research collaboration, ensures FDA/EMA compliance, and protects intellectual property while facilitating breakthrough drug development and international regulatory submissions.",
    question: "Which governance framework ensures COMPREHENSIVE pharmaceutical clinical trial compliance with global IP protection?",
    options: [
        "AWS Organizations with regulatory jurisdiction OUs, AWS Lake Formation for patient data governance and IP protection, AWS PrivateLink for secure multi-site collaboration, AWS CloudTrail with tamper detection for regulatory audit trails, and AWS Config for continuous FDA/EMA compliance monitoring",
        "AWS Control Tower for clinical trial standardization, AWS Security Hub for compliance monitoring, Amazon S3 for trial data storage, AWS Lambda for data processing, and AWS Direct Connect for site connectivity",
        "AWS Organizations with therapeutic area OUs, AWS Directory Service for researcher authentication, Amazon ElastiCache for trial caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration",
        "AWS Landing Zone with pharmaceutical templates, AWS GuardDuty for trial security, Amazon Redshift for trial analytics, AWS Glue for data integration, and Amazon QuickSight for trial dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with regulatory jurisdiction OUs ensure compliance with 90+ different drug approval regulations. Lake Formation provides patient data governance and IP protection through fine-grained access controls. PrivateLink enables secure multi-site collaboration without internet exposure. CloudTrail with tamper detection provides FDA/EMA-compliant audit trails. Config ensures continuous regulatory compliance monitoring across all jurisdictions.",
        whyWrong: {
            1: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide patient data protection",
            2: "Therapeutic area OUs don't address regulatory jurisdiction requirements, Directory Service doesn't provide IP protection",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the regulatory compliance framework needed for clinical trials"
        },
        examStrategy: "Clinical trials: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail tamper detection + Config for global compliance."
    }
},
{
    id: 'sap_410',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global carbon tracking and offset platform monitors emissions from 1 million+ sources worldwide, verifies carbon credits, manages offset trading, provides regulatory reporting, and enables corporate sustainability programs while ensuring data integrity and environmental compliance across multiple jurisdictions with real-time emissions monitoring.",
    question: "Which architecture provides OPTIMAL carbon tracking with comprehensive environmental compliance and trading capabilities?",
    options: [
        "AWS IoT Core for emissions monitoring, Amazon Kinesis Data Analytics for real-time carbon tracking, Amazon SageMaker for offset verification and trading optimization, AWS Lambda for regulatory reporting automation, and Amazon QLDB for immutable carbon credit records",
        "Amazon EventBridge for emissions events, AWS Batch for carbon calculations, Amazon DocumentDB for emissions data, Amazon API Gateway for trading integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for emissions edge processing, Amazon DynamoDB for carbon records, AWS Step Functions for trading workflows, Amazon Redshift for analytics, and Amazon QuickSight for sustainability dashboards",
        "Amazon EC2 Auto Scaling for carbon processing, Amazon ElastiCache for emissions caching, Amazon RDS for carbon data, AWS Direct Connect for regulatory connectivity, and Amazon CloudFront for reporting delivery"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure monitoring for 1M+ emission sources with device management. Kinesis Data Analytics enables real-time carbon tracking and environmental compliance monitoring. SageMaker provides offset verification and trading optimization algorithms. Lambda automates regulatory reporting across multiple jurisdictions. QLDB ensures immutable carbon credit records for trading integrity and audit compliance.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time emissions monitoring and trading",
            2: "IoT Greengrass on every source adds operational complexity, Step Functions add latency for real-time carbon tracking",
            3: "EC2 Auto Scaling doesn't provide IoT emissions monitoring, RDS doesn't optimize for 1M+ emission sources scale"
        },
        examStrategy: "Carbon tracking: IoT Core + Kinesis Analytics + SageMaker + Lambda + QLDB for environmental compliance and trading."
    }
},
{
    id: 'sap_411',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global telecommunications network serves 1 billion subscribers across 100+ countries, experiences network congestion during peak hours, has 5G rollout optimization challenges, faces real-time fault detection requirements, and needs predictive maintenance for network infrastructure while reducing operational costs and energy consumption across diverse markets.",
    question: "Which optimization strategy provides the BEST telecommunications network performance with sustainability and predictive maintenance?",
    options: [
        "Implement Amazon ElastiCache Redis cluster mode for real-time network state, Amazon SageMaker for predictive maintenance and 5G optimization, Amazon DynamoDB Global Tables for subscriber management, Amazon Kinesis for network telemetry, and AWS Lambda for fault detection and energy optimization",
        "Deploy Amazon Aurora Global Database for network records, Amazon MSK for real-time messaging, AWS Step Functions for network workflows, Amazon EventBridge for fault coordination, and AWS Batch for maintenance calculations",
        "Use Amazon DocumentDB for flexible network schemas, Amazon EventBridge for network events, AWS Batch for optimization calculations, Amazon SQS for fault handling, and Amazon EMR for sustainability analytics",
        "Implement Amazon RDS with read replicas, Amazon Redshift for network analytics, AWS Glue for network data integration, Amazon CloudWatch for monitoring, and AWS Auto Scaling for resource optimization"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides real-time network state access for 1B subscribers with automatic scaling. SageMaker enables predictive maintenance and 5G rollout optimization while reducing energy consumption. DynamoDB Global Tables manages subscribers across 100+ countries. Kinesis handles network telemetry efficiently. Lambda provides real-time fault detection and energy optimization automation.",
        whyWrong: {
            1: "Aurora Global Database has limitations for high-frequency network updates at 1B subscriber scale, Batch adds latency for real-time fault detection",
            2: "DocumentDB doesn't optimize for telecommunications patterns, EventBridge adds latency for real-time network management",
            3: "RDS with read replicas can't handle 1B subscriber network updates, CloudWatch alone doesn't provide predictive maintenance"
        },
        examStrategy: "Telecom optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for intelligent network management."
    }
},
{
    id: 'sap_412',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national healthcare system needs to migrate their electronic health records and medical imaging systems from legacy infrastructure to AWS, supporting 50 million patient records, ensuring HIPAA compliance, enabling provider collaboration, and maintaining high availability for emergency care while protecting patient privacy and improving care coordination.",
    question: "Which modernization approach provides OPTIMAL healthcare system performance with comprehensive privacy protection and care coordination?",
    options: [
        "Deploy AWS Control Tower for healthcare governance, Amazon HealthLake for patient records management, Amazon DynamoDB for medical data, AWS Lake Formation for HIPAA compliance and patient privacy protection, and AWS PrivateLink for secure provider collaboration",
        "Implement AWS Elastic Beanstalk for medical applications, Amazon Aurora for patient database, AWS Step Functions for healthcare workflows, Amazon S3 for medical imaging, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized healthcare services, Amazon DocumentDB for flexible patient data, AWS EventBridge for healthcare coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management",
        "Deploy Amazon EC2 for healthcare applications, Amazon RDS for patient data, AWS Lambda for medical processing, Amazon API Gateway for provider integration, and AWS CloudTrail for HIPAA audit compliance"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower provides healthcare-specific governance frameworks for medical systems. HealthLake is purpose-built for healthcare data management with FHIR support for 50M patient records. DynamoDB scales for medical data with high availability for emergency care. Lake Formation ensures HIPAA compliance and patient privacy protection. PrivateLink enables secure provider collaboration without internet exposure.",
        whyWrong: {
            1: "Elastic Beanstalk doesn't provide healthcare-specific controls and FHIR support needed for medical records",
            2: "EKS adds operational complexity, DocumentDB doesn't optimize for healthcare data patterns and HIPAA requirements",
            3: "Standard deployment doesn't provide healthcare-specific data management and HIPAA compliance frameworks needed"
        },
        examStrategy: "Healthcare modernization: Control Tower + HealthLake + DynamoDB + Lake Formation + PrivateLink for HIPAA-compliant care."
    }
},
{
    id: 'sap_413',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global investment management firm with $10 trillion assets under management operates across 80+ jurisdictions, requires real-time risk assessment, algorithmic trading capabilities, regulatory reporting per jurisdiction, client data protection, and secure communication with fund managers while maintaining competitive trading advantages and operational resilience during market volatility.",
    question: "Which governance framework ensures COMPREHENSIVE investment management with competitive protection and regulatory compliance?",
    options: [
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for client data protection and competitive intelligence governance, Amazon ElastiCache for real-time risk assessment, AWS Lambda for algorithmic trading, and AWS CloudTrail with legal hold for regulatory audit trails",
        "AWS Control Tower for investment standardization, AWS Security Hub for compliance monitoring, Amazon Kinesis for trading data, AWS Step Functions for investment workflows, and AWS Direct Connect for regulatory connectivity",
        "AWS Organizations with asset class OUs, AWS Config for investment compliance, AWS Lambda for trading algorithms, Amazon DynamoDB for portfolio data, and AWS PrivateLink for manager communication",
        "AWS Landing Zone with investment templates, AWS Directory Service for trader authentication, Amazon ElastiCache for portfolio caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for fund management"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 80+ different financial regulations. Lake Formation provides client data protection and competitive intelligence governance through fine-grained access controls. ElastiCache enables real-time risk assessment for $10T AUM with microsecond latency. Lambda scales for algorithmic trading with competitive advantages. CloudTrail with legal hold provides regulatory audit trails across all jurisdictions.",
        whyWrong: {
            1: "Control Tower doesn't address jurisdiction-specific investment regulations for 80+ jurisdictions",
            2: "Asset class OUs don't address jurisdiction-specific regulatory requirements for global operations",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the data governance needed for competitive protection"
        },
        examStrategy: "Investment management: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for global compliance."
    }
},
{
    id: 'sap_414',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A smart city infrastructure platform manages 2 million+ IoT devices across transportation, energy, water, and waste systems, optimizes city operations, predicts infrastructure maintenance needs, enables citizen services, and provides emergency response coordination while ensuring public safety and environmental sustainability across a major metropolitan area.",
    question: "Which architecture provides OPTIMAL smart city management with comprehensive public safety and sustainability?",
    options: [
        "AWS IoT Core for city-wide device management, Amazon Kinesis Data Analytics for real-time city optimization, Amazon SageMaker for predictive maintenance and emergency response, AWS Lambda for citizen services automation, and Amazon Timestream for city performance metrics and sustainability tracking",
        "Amazon EventBridge for city events, AWS Batch for infrastructure calculations, Amazon DocumentDB for device data, Amazon API Gateway for citizen services, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for city edge processing, Amazon DynamoDB for infrastructure records, AWS Step Functions for city workflows, Amazon Redshift for analytics, and Amazon QuickSight for city dashboards",
        "Amazon EC2 Auto Scaling for city processing, Amazon ElastiCache for infrastructure caching, Amazon RDS for city data, AWS Direct Connect for system connectivity, and Amazon CloudFront for citizen portal delivery"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure management for 2M+ city devices with comprehensive security controls. Kinesis Data Analytics enables real-time city optimization across transportation, energy, water, and waste systems. SageMaker provides predictive maintenance and emergency response coordination. Lambda automates citizen services with automatic scaling. Timestream tracks city performance metrics and sustainability goals efficiently.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time city operations and emergency response",
            2: "IoT Greengrass on every city device adds operational complexity, Step Functions add latency for real-time city optimization",
            3: "EC2 Auto Scaling doesn't provide IoT city device management, RDS doesn't optimize for 2M+ device data scale"
        },
        examStrategy: "Smart city: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for intelligent sustainable city operations."
    }
},
{
    id: 'sap_415',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global financial trading platform processes 100 million transactions daily across equities, derivatives, and cryptocurrency markets, experiences latency spikes during market volatility, has cross-market arbitrage opportunities requiring microsecond execution, and needs real-time risk management while maintaining regulatory compliance across 50+ financial jurisdictions.",
    question: "Which optimization strategy provides the BEST ultra-high frequency trading performance with comprehensive risk management and compliance?",
    options: [
        "Implement Amazon ElastiCache Redis cluster mode for order books, AWS Lambda with provisioned concurrency for microsecond execution, Amazon Kinesis Data Streams for market data, Amazon SageMaker for real-time risk assessment, and AWS Config for multi-jurisdiction regulatory compliance monitoring",
        "Deploy Amazon MemoryDB for trading state persistence, AWS Batch for risk processing, Amazon Aurora Global for trade records, Amazon EventBridge for market events, and AWS Security Hub for compliance dashboards",
        "Use AWS Local Zones for ultra-low latency, Amazon DocumentDB for flexible trading schemas, AWS Step Functions for trade workflows, Amazon MSK for market messaging, and AWS CloudTrail for regulatory auditing",
        "Implement AWS Outposts for on-premises performance, Amazon RDS with read replicas, AWS Glue for regulatory reporting, Amazon EMR for market analysis, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 100M daily transactions with automatic sharding. Lambda with provisioned concurrency eliminates cold starts for microsecond execution across multiple markets. Kinesis Data Streams handles high-volume market data efficiently. SageMaker provides real-time risk assessment and arbitrage detection. Config monitors regulatory compliance across 50+ jurisdictions continuously.",
        whyWrong: {
            1: "MemoryDB has higher latency than ElastiCache cluster mode for microsecond trading requirements, Batch adds latency for real-time risk",
            2: "Local Zones alone don't provide the comprehensive trading optimization needed, Step Functions add latency for microsecond execution",
            3: "Outposts requires infrastructure management, RDS with read replicas can't handle 100M daily transaction writes efficiently"
        },
        examStrategy: "Ultra-high frequency trading: ElastiCache cluster + Lambda provisioned + Kinesis + SageMaker + Config for microsecond trading compliance."
    }
},
{
    id: 'sap_416',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national space agency needs to migrate their mission control and satellite operations systems from legacy infrastructure to AWS, supporting deep space missions, real-time telemetry from multiple spacecraft, mission-critical decision making, and international space collaboration while maintaining zero-downtime requirements and the highest security standards for space operations.",
    question: "Which modernization strategy ensures MAXIMUM reliability and security for mission-critical space operations?",
    options: [
        "Deploy AWS Ground Station for spacecraft communication, Amazon Kinesis Data Streams for real-time telemetry, AWS Lambda for mission-critical processing with provisioned concurrency, Amazon DynamoDB Global Tables for mission state, and AWS Auto Scaling with multi-region failover for zero-downtime requirements",
        "Implement Amazon EC2 in multiple regions, Amazon RDS Aurora for mission data, AWS Step Functions for mission workflows, Amazon API Gateway for ground station integration, and AWS CloudFormation for infrastructure automation",
        "Use AWS Outposts for mission control rooms, Amazon MSK for telemetry streams, AWS Batch for mission calculations, Amazon ElastiCache for performance, and AWS Direct Connect for ground station connectivity",
        "Deploy AWS Wavelength for low-latency processing, Amazon EventBridge for mission coordination, Amazon DocumentDB for spacecraft data, Amazon Redshift for mission analytics, and AWS Transit Gateway for ground station networking"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Ground Station provides managed spacecraft communication infrastructure for deep space missions. Kinesis Data Streams handles real-time telemetry from multiple spacecraft with automatic scaling. Lambda with provisioned concurrency ensures mission-critical processing without cold starts. DynamoDB Global Tables provides mission state availability across regions for international collaboration. Auto Scaling with multi-region failover ensures zero-downtime requirements for space operations.",
        whyWrong: {
            1: "Standard EC2 deployment doesn't provide spacecraft communication capabilities or zero-downtime guarantees",
            2: "Outposts requires on-premises infrastructure management inappropriate for mission-critical space operations",
            3: "Wavelength is for mobile edge computing not space missions, EventBridge doesn't provide mission-critical reliability needed"
        },
        examStrategy: "Space operations: Ground Station + Kinesis + Lambda provisioned + DynamoDB Global Tables + Auto Scaling for mission-critical reliability."
    }
},
{
    id: 'sap_417',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multinational defense contractor manages classified weapons systems across 30+ military programs, requires strict security clearance access controls, maintains air-gapped development environments, enables secure multi-national collaboration, and ensures ITAR compliance while supporting rapid prototyping and testing with advanced simulation capabilities for next-generation defense systems.",
    question: "Which governance framework ensures MAXIMUM security for classified defense systems development with international collaboration?",
    options: [
        "AWS GovCloud with program-isolated VPCs per classification level, AWS Organizations with ITAR-compliant SCPs, AWS Lake Formation for classified data governance, AWS PrivateLink for secure multi-national collaboration, and AWS CloudTrail with tamper detection for defense audit compliance",
        "AWS Control Tower in commercial regions, AWS Config for ITAR monitoring, AWS PrivateLink for program communication, AWS Backup for data protection, and AWS Client VPN for contractor access",
        "AWS Organizations with contractor-based accounts, AWS Security Hub for clearance monitoring, AWS Direct Connect for government connectivity, AWS KMS for encryption, and Amazon WorkSpaces for secure development",
        "AWS Landing Zone with defense templates, AWS Directory Service for clearance authentication, AWS GuardDuty for threat detection, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 0,
    explanation: {
        correct: "GovCloud with program-isolated VPCs ensures proper classification separation for 30+ programs. Organizations with ITAR-compliant SCPs enforce export control restrictions automatically. Lake Formation provides classified data governance with fine-grained access controls for weapons systems. PrivateLink enables secure multi-national collaboration without internet exposure. CloudTrail with tamper detection provides defense-compliant audit trails for classified development.",
        whyWrong: {
            1: "Commercial regions don't meet ITAR requirements for classified weapons systems development",
            2: "Contractor-based accounts don't provide classification-level security isolation needed for defense systems",
            3: "Landing Zone is deprecated, Directory Service alone doesn't provide the security controls needed for classified weapons development"
        },
        examStrategy: "Defense systems: GovCloud + program-isolated VPCs + ITAR SCPs + Lake Formation + PrivateLink + CloudTrail tamper detection for classified development."
    }
},
{
    id: 'sap_418',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global precision agriculture platform manages 500,000+ farms worldwide using satellite imagery, IoT sensors, and drones, optimizes crop yields, predicts weather impacts, manages resource usage, and provides market insights while ensuring environmental sustainability and enabling small-scale farmer access to advanced agricultural technology.",
    question: "Which architecture provides OPTIMAL precision agriculture with sustainability and global farmer accessibility?",
    options: [
        "AWS IoT Core for farm sensor management, Amazon Kinesis Data Analytics for real-time crop optimization, Amazon SageMaker for yield prediction and weather analysis, AWS Lambda for resource management automation, and Amazon Timestream for agricultural metrics and sustainability tracking",
        "Amazon EventBridge for farm events, AWS Batch for crop calculations, Amazon DocumentDB for farm data, Amazon API Gateway for farmer integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for farm edge processing, Amazon DynamoDB for crop records, AWS Step Functions for agricultural workflows, Amazon Redshift for analytics, and Amazon QuickSight for farm dashboards",
        "Amazon EC2 Auto Scaling for agricultural processing, Amazon ElastiCache for farm caching, Amazon RDS for crop data, AWS Direct Connect for farm connectivity, and Amazon CloudFront for farmer portal delivery"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure management for 500,000+ farms with device management and global scalability. Kinesis Data Analytics enables real-time crop optimization using satellite and sensor data. SageMaker provides yield prediction and weather impact analysis for farmers. Lambda automates resource management for sustainability. Timestream tracks agricultural metrics and environmental impact efficiently for global farming operations.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time agricultural optimization and weather response",
            2: "IoT Greengrass on every farm adds operational complexity, Step Functions add latency for real-time crop optimization",
            3: "EC2 Auto Scaling doesn't provide IoT agricultural device management, RDS doesn't optimize for 500,000+ farm scale"
        },
        examStrategy: "Precision agriculture: IoT Core + Kinesis Analytics + SageMaker + Lambda + Timestream for sustainable global farming."
    }
},
{
    id: 'sap_419',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global video streaming service serves 1 billion subscribers across all continents, experiences buffering during major live events, has content personalization algorithm delays, faces regional content licensing complexities, and needs real-time viewer analytics while optimizing bandwidth costs and content delivery efficiency for diverse global markets with varying network conditions.",
    question: "Which optimization strategy provides the BEST global streaming performance with advanced personalization and cost efficiency?",
    options: [
        "Implement Amazon CloudFront with Lambda@Edge for adaptive streaming and regional content control, Amazon SageMaker for real-time personalized recommendations, Amazon DynamoDB Global Tables for viewer state management, AWS Elemental MediaServices for content optimization, and Amazon Kinesis for real-time viewer analytics",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for video processing, Amazon EventBridge for viewer coordination, and Amazon ElastiCache for session management",
        "Use Amazon S3 Transfer Acceleration, Amazon ECS for content processing, AWS Step Functions for personalization workflows, Amazon DocumentDB for viewer data, and Amazon Route 53 for traffic routing",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Lambda for video processing, Amazon SQS for viewer queuing, and Amazon API Gateway for mobile streaming apps"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFront with Lambda@Edge enables adaptive streaming based on network conditions and regional content licensing controls for 1B subscribers. SageMaker provides real-time personalized recommendations with low latency. DynamoDB Global Tables manages viewer state globally across all continents. Elemental MediaServices optimizes content delivery efficiency. Kinesis provides real-time viewer analytics for content optimization and bandwidth cost reduction.",
        whyWrong: {
            1: "Global Accelerator doesn't provide video streaming optimization or personalization, EFS doesn't optimize for global video content delivery",
            2: "Transfer Acceleration doesn't provide video streaming capabilities or personalization, Step Functions add latency for real-time recommendations",
            3: "Direct Connect is expensive for global content delivery to 1B subscribers, Glacier has retrieval delays inappropriate for streaming"
        },
        examStrategy: "Global streaming: CloudFront Lambda@Edge + SageMaker + DynamoDB Global Tables + Elemental MediaServices + Kinesis for optimized delivery."
    }
},
{
    id: 'sap_420',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national tax administration needs to migrate their taxpayer processing systems from legacy mainframes to AWS, handling 300 million tax returns annually, ensuring data privacy, maintaining audit trails, supporting real-time fraud detection, and enabling citizen self-service while meeting government security requirements and improving processing efficiency during tax season peaks.",
    question: "Which modernization approach provides OPTIMAL taxpayer services with comprehensive security and fraud protection for high-volume tax processing?",
    options: [
        "Deploy AWS Control Tower in GovCloud, Amazon DynamoDB for taxpayer records, Amazon SageMaker for fraud detection, AWS Lambda for tax processing, and Amazon Kinesis Data Analytics for real-time fraud monitoring with AWS Config for government security compliance",
        "Implement AWS Mainframe Modernization for legacy system migration, Amazon RDS for tax data, AWS Step Functions for processing workflows, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Use AWS Elastic Beanstalk for tax applications, Amazon Aurora for taxpayer database, AWS Batch for return processing, Amazon CloudFront for citizen portal, and AWS Security Hub for compliance management",
        "Deploy Amazon EKS for containerized services, Amazon DocumentDB for flexible tax data, AWS EventBridge for processing coordination, AWS Lambda for microservices, and AWS Systems Manager for compliance automation"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for national tax systems. DynamoDB scales to handle 300M tax returns annually with high availability during tax season peaks. SageMaker provides sophisticated fraud detection capabilities with real-time inference. Lambda handles tax processing with automatic scaling for seasonal demands. Kinesis Data Analytics enables real-time fraud monitoring, and Config ensures continuous government security compliance.",
        whyWrong: {
            1: "Mainframe Modernization alone doesn't provide the scale and fraud detection needed for 300M returns",
            2: "Elastic Beanstalk doesn't provide government security controls needed for national tax systems",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for tax processing patterns at 300M return scale"
        },
        examStrategy: "Tax modernization: Control Tower GovCloud + DynamoDB + SageMaker + Lambda + Kinesis Analytics + Config for secure high-volume processing."
    }
},
{
    id: 'sap_421',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global energy trading company operates across 60+ countries with different energy regulations, requires real-time commodity trading, cross-border energy transactions, regulatory reporting per jurisdiction, and secure communication between trading desks while maintaining audit trails for compliance and competitive intelligence protection in volatile energy markets.",
    question: "Which governance framework ensures OPTIMAL energy trading with competitive protection and comprehensive regulatory compliance?",
    options: [
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for competitive intelligence protection and trade governance, Amazon ElastiCache for real-time trading data, AWS Lambda for algorithmic trading, and AWS CloudTrail with legal hold for regulatory audit trails",
        "AWS Control Tower for energy standardization, AWS Security Hub for compliance monitoring, Amazon MSK for trading messages, AWS Step Functions for trade workflows, and AWS Direct Connect for desk connectivity",
        "AWS Organizations with commodity-based OUs, AWS Directory Service for trader authentication, Amazon ElastiCache for trade caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration",
        "AWS Landing Zone with energy templates, AWS GuardDuty for trading security, Amazon Redshift for trading analytics, AWS Glue for regulatory data integration, and Amazon QuickSight for trading dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 60+ different energy regulations across volatile markets. Lake Formation provides competitive intelligence protection and trade governance through fine-grained access controls. ElastiCache enables real-time commodity trading with microsecond latency for energy markets. Lambda scales for algorithmic trading strategies. CloudTrail with legal hold provides regulatory audit trails for energy trading compliance across all jurisdictions.",
        whyWrong: {
            1: "Control Tower doesn't address jurisdiction-specific energy trading regulations for global operations",
            2: "Commodity-based OUs don't address jurisdiction-specific compliance requirements for energy markets",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the competitive intelligence protection needed for energy trading"
        },
        examStrategy: "Energy trading: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for competitive compliance."
    }
},
{
    id: 'sap_422',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous vehicle fleet management platform coordinates 100,000+ self-driving vehicles across smart cities, manages real-time traffic optimization, implements collision avoidance systems, provides predictive maintenance, and enables vehicle-to-everything (V2X) communication while ensuring passenger safety and regulatory compliance for autonomous transportation.",
    question: "Which architecture provides OPTIMAL autonomous vehicle fleet management with comprehensive safety and V2X communication?",
    options: [
        "AWS IoT Core for vehicle connectivity, Amazon Kinesis Data Streams for real-time vehicle telemetry, Amazon SageMaker for collision avoidance and route optimization, AWS Lambda for traffic coordination, and Amazon Timestream for vehicle performance and safety metrics",
        "Amazon EventBridge for vehicle events, AWS Batch for route calculations, Amazon DocumentDB for vehicle data, Amazon API Gateway for city integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for vehicle edge computing, Amazon DynamoDB for vehicle records, AWS Step Functions for fleet workflows, Amazon Redshift for analytics, and Amazon QuickSight for fleet dashboards",
        "Amazon EC2 Auto Scaling for fleet processing, Amazon ElastiCache for vehicle caching, Amazon RDS for fleet data, AWS Direct Connect for city connectivity, and Amazon CloudFront for vehicle updates"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core provides secure connectivity for 100,000+ autonomous vehicles with V2X communication capabilities. Kinesis Data Streams handles real-time vehicle telemetry for safety systems. SageMaker enables collision avoidance algorithms and route optimization for autonomous driving. Lambda coordinates traffic optimization with automatic scaling. Timestream tracks vehicle performance and safety metrics for regulatory compliance and predictive maintenance.",
        whyWrong: {
            1: "EventBridge and Batch add latency inappropriate for real-time collision avoidance and safety systems",
            2: "IoT Greengrass on every vehicle adds operational complexity, Step Functions add latency for real-time safety coordination",
            3: "EC2 Auto Scaling doesn't provide IoT vehicle connectivity for V2X communication, RDS doesn't optimize for 100,000+ vehicle scale"
        },
        examStrategy: "Autonomous vehicles: IoT Core + Kinesis + SageMaker + Lambda + Timestream for safe intelligent transportation."
    }
},
{
    id: 'sap_423',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online marketplace with 1 billion products experiences search performance degradation, product recommendation delays, inventory synchronization issues across regions, seller performance tracking problems, and payment processing bottlenecks during major sales events while serving 500 million customers worldwide.",
    question: "Which optimization strategy provides the BEST marketplace performance and seller experience for global e-commerce at scale?",
    options: [
        "Deploy Amazon OpenSearch for intelligent product search, Amazon SageMaker for real-time recommendations, Amazon DynamoDB Global Tables for inventory synchronization, AWS Lambda for seller analytics, and Amazon Kinesis for payment event processing",
        "Implement Amazon ElastiCache for product caching, AWS Lambda for search processing, Amazon DynamoDB for inventory data, Amazon API Gateway for seller APIs, and Amazon SQS for payment processing",
        "Use Amazon Aurora Global Database for product catalog, Amazon MSK for real-time updates, AWS Step Functions for seller workflows, Amazon EventBridge for marketplace coordination, and AWS Batch for payment processing",
        "Deploy Amazon DocumentDB for flexible product schemas, Amazon EventBridge for marketplace events, AWS Batch for recommendation calculations, Amazon SQS for seller communication, and Amazon EMR for payment analytics"
    ],
    correct: 0,
    explanation: {
        correct: "OpenSearch provides sophisticated product search capabilities for 1B products with real-time indexing and relevance. SageMaker enables real-time recommendation inference for 500M customers. DynamoDB Global Tables ensures inventory synchronization across regions with conflict resolution. Lambda provides seller analytics with automatic scaling. Kinesis handles payment events during sales spikes efficiently for global marketplace operations.",
        whyWrong: {
            1: "Basic Lambda search processing can't match OpenSearch capabilities for 1B products at global scale",
            2: "Aurora Global Database has write limitations for high-frequency inventory updates across regions",
            3: "DocumentDB doesn't optimize for marketplace search patterns at 1B product scale, EventBridge adds latency for real-time inventory"
        },
        examStrategy: "Global marketplace: OpenSearch + SageMaker + DynamoDB Global Tables + Lambda + Kinesis for optimal e-commerce performance."
    }
},
{
    id: 'sap_424',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national emergency management agency needs to migrate their disaster response coordination systems from legacy infrastructure to AWS, supporting real-time crisis communication, resource allocation, inter-agency coordination, public alerting, and maintaining operations during natural disasters while ensuring government security compliance and enabling rapid deployment capabilities.",
    question: "Which modernization strategy ensures MAXIMUM resilience for emergency management operations with rapid deployment capabilities?",
    options: [
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB Global Tables for crisis data resilience, AWS IoT Core for emergency sensor networks, Amazon SNS for public alerting, and AWS Outposts for field operations during disasters",
        "Implement Amazon EC2 in multiple regions, Amazon RDS for emergency data, AWS Lambda for alert processing, Amazon API Gateway for agency coordination, and AWS CloudFormation for infrastructure automation",
        "Use AWS Outposts for emergency centers, Amazon MSK for crisis communication, AWS Batch for resource calculations, Amazon ElastiCache for performance, and AWS Direct Connect for agency connectivity",
        "Deploy AWS Wavelength for emergency response, Amazon EventBridge for crisis coordination, Amazon DocumentDB for emergency data, Amazon Redshift for analytics, and AWS Transit Gateway for agency networking"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for national emergency operations. DynamoDB Global Tables ensures crisis data resilience across regions during disasters with automatic failover. IoT Core manages emergency sensor networks securely with rapid deployment capabilities. SNS delivers public alerts reliably at scale during crises. Outposts enables field operations to continue during infrastructure disruptions with portable AWS infrastructure.",
        whyWrong: {
            1: "Standard deployment doesn't provide government security compliance and disaster resilience needed for national emergency management",
            2: "Outposts alone doesn't provide the scalability and government compliance needed for national emergency coordination",
            3: "Wavelength is for mobile edge computing not emergency management, EventBridge doesn't provide disaster resilience capabilities"
        },
        examStrategy: "Emergency management: Control Tower GovCloud + DynamoDB Global Tables + IoT Core + SNS + Outposts for disaster resilience."
    }
},
{
    id: 'sap_425',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global humanitarian organization coordinates disaster relief across 150+ countries with varying political situations, requires rapid deployment capabilities, secure communication in crisis zones, resource coordination, donor transparency, and compliance with international aid regulations while maintaining operational continuity during emergencies and conflicts with limited infrastructure.",
    question: "Which governance framework ensures COMPREHENSIVE humanitarian operations with crisis resilience and transparency?",
    options: [
        "AWS Organizations with country-specific OUs for regulatory compliance, AWS Outposts for rapid crisis deployment, AWS Lake Formation for donor transparency and resource governance, Amazon Kinesis Data Analytics for real-time crisis coordination, and AWS Satellite for communication in remote disaster areas",
        "AWS Control Tower for humanitarian standardization, AWS Security Hub for compliance monitoring, Amazon MSK for crisis communication, AWS Lambda for resource coordination, and AWS Direct Connect for field connectivity",
        "AWS Organizations with program-based OUs, AWS Config for aid compliance, AWS IoT Core for field monitoring, Amazon Kinesis for crisis data, and AWS Storage Gateway for operational continuity",
        "AWS Landing Zone with humanitarian templates, AWS GuardDuty for field security, Amazon Redshift for aid analytics, AWS Glue for donor data integration, and Amazon QuickSight for transparency dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 150+ different aid regulations and political requirements. Outposts enables rapid deployment in crisis zones with local AWS infrastructure during emergencies. Lake Formation provides donor transparency and resource governance with audit trails for accountability. Kinesis Data Analytics coordinates real-time crisis response and resource allocation during disasters. AWS Satellite ensures communication in remote disaster areas without existing infrastructure.",
        whyWrong: {
            1: "Control Tower doesn't address country-specific humanitarian regulations and political complexities",
            2: "Program-based OUs don't address country-specific aid regulations and political situations in crisis zones",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the crisis resilience needed for humanitarian operations"
        },
        examStrategy: "Humanitarian operations: Organizations + country OUs + Outposts + Lake Formation + Kinesis Analytics + Satellite for crisis response."
    }
},

// Questions 426-450: correct: 1 (B)
{
    id: 'sap_426',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global renewable energy trading platform enables peer-to-peer energy transactions across smart grids, manages virtual power plants, optimizes renewable energy distribution, provides carbon credit verification, and enables community energy sharing while ensuring grid stability and regulatory compliance across multiple energy markets.",
    question: "Which architecture provides OPTIMAL renewable energy trading with grid stability and carbon verification?",
    options: [
        "AWS IoT Core for grid device connectivity, Amazon MSK for high-volume energy data, Amazon EMR for energy analytics, AWS Lambda for trading processing, and Amazon S3 for energy data storage",
        "Amazon Kinesis Data Streams for real-time grid data, Amazon SageMaker for energy optimization and carbon verification, AWS Lambda for peer-to-peer trading automation, Amazon QLDB for immutable energy transaction records, and Amazon Timestream for grid performance metrics",
        "Amazon EventBridge for energy events, AWS Batch for grid calculations, Amazon DocumentDB for energy data, Amazon API Gateway for trading integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for grid edge processing, Amazon DynamoDB for energy records, AWS Step Functions for trading workflows, Amazon Redshift for analytics, and Amazon QuickSight for energy dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams handles real-time grid data for stability management. SageMaker optimizes renewable energy distribution and provides carbon credit verification algorithms. Lambda automates peer-to-peer energy trading with automatic scaling. QLDB ensures immutable energy transaction records for regulatory compliance and audit trails. Timestream tracks grid performance metrics and renewable energy efficiency for optimization.",
        whyWrong: {
            0: "MSK adds complexity for grid device communication, EMR is for batch analytics not real-time energy trading",
            2: "EventBridge and Batch add latency inappropriate for real-time grid stability and energy trading",
            3: "IoT Greengrass on every grid device adds operational complexity, Step Functions add latency for real-time energy trading"
        },
        examStrategy: "Renewable energy trading: Kinesis + SageMaker + Lambda + QLDB + Timestream for intelligent sustainable energy markets."
    }
},
{
    id: 'sap_427',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global ride-sharing platform coordinates 10 million drivers across 3,000+ cities, experiences driver-passenger matching delays during surge periods, has dynamic pricing calculation issues, faces real-time location tracking problems at massive scale, and needs comprehensive fraud detection while optimizing operational costs and driver earnings across diverse global markets.",
    question: "Which optimization strategy provides the BEST ride-sharing platform performance with fraud prevention and driver optimization?",
    options: [
        "Implement Amazon ElastiCache for driver location caching, AWS Lambda for matching algorithms, Amazon DynamoDB for ride data, Amazon API Gateway for mobile apps, and Amazon CloudWatch for monitoring",
        "Deploy Amazon ElastiCache Redis cluster mode for real-time location data, Amazon SageMaker for driver-passenger matching optimization and fraud detection, Amazon DynamoDB Global Tables for ride state management, Amazon Kinesis for location streams, and AWS Lambda for dynamic pricing and driver earnings optimization",
        "Use Amazon Aurora Global Database for ride records, Amazon MSK for real-time messaging, AWS Step Functions for ride workflows, Amazon EventBridge for surge coordination, and AWS Batch for fraud analysis",
        "Implement Amazon DocumentDB for flexible ride schemas, Amazon EventBridge for location events, AWS Batch for matching calculations, Amazon SQS for ride requests, and Amazon EMR for fraud analytics"
    ],
    correct: 1,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond location access for 10M drivers across 3,000+ cities with automatic sharding. SageMaker optimizes driver-passenger matching and provides real-time fraud detection at scale. DynamoDB Global Tables manages ride state across global cities with conflict resolution. Kinesis handles real-time location streams efficiently for massive scale. Lambda scales automatically for dynamic pricing and driver earnings optimization.",
        whyWrong: {
            0: "Basic ElastiCache setup doesn't provide clustering needed for 10M drivers at global scale",
            2: "Aurora Global Database has write limitations for high-frequency location updates at 10M driver scale",
            3: "DocumentDB doesn't optimize for ride-sharing patterns at global scale, EventBridge adds latency for real-time location processing"
        },
        examStrategy: "Ride-sharing optimization: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda for global scale operations."
    }
},
{
    id: 'sap_428',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national weather service needs to migrate their hurricane prediction and climate modeling systems from legacy supercomputers to AWS, requiring real-time atmospheric simulations, ensemble forecasting, public safety alerting, and international meteorological data sharing while maintaining forecast accuracy for emergency management and climate research.",
    question: "Which modernization approach provides OPTIMAL weather prediction with emergency response and climate research capabilities?",
    options: [
        "Deploy Amazon EMR with Spark clusters, implement Amazon S3 for weather data, use AWS Glue for data processing, Amazon Redshift for climate analytics, and AWS Direct Connect for international collaboration",
        "Implement AWS ParallelCluster with HPC-optimized instances for atmospheric modeling, Amazon FSx for Lustre for weather data processing, AWS Batch for ensemble forecasting, Amazon SNS for emergency alerts, and Amazon S3 with Cross-Region Replication for international meteorological data sharing",
        "Use Amazon ECS with GPU instances, implement Amazon EFS for shared storage, AWS Step Functions for forecast workflows, Amazon DocumentDB for weather metadata, and AWS VPN for collaborative access",
        "Deploy AWS Fargate for containerized forecasting, Amazon DynamoDB for weather records, Amazon EventBridge for forecast coordination, AWS Lambda for alert processing, and Amazon API Gateway for international APIs"
    ],
    correct: 1,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for hurricane prediction and climate modeling. FSx Lustre handles massive atmospheric datasets with parallel processing performance. Batch schedules ensemble forecasting efficiently for accuracy. SNS delivers emergency alerts to public safety systems reliably. S3 with CRR enables international meteorological data sharing for climate research collaboration.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive atmospheric modeling and hurricane prediction computations",
            2: "ECS doesn't provide HPC optimizations needed for weather modeling, EFS doesn't match FSx Lustre performance",
            3: "Fargate has resource limitations for atmospheric modeling, Lambda has execution time limits for complex forecasting"
        },
        examStrategy: "Weather prediction: ParallelCluster + HPC + FSx Lustre + Batch + SNS + S3 CRR for emergency weather forecasting."
    }
},
{
    id: 'sap_429',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global aerospace consortium collaborates on next-generation aircraft development across 40+ companies and 200+ research institutions, requires secure technology sharing, intellectual property protection, international coordination, and compliance with aviation safety regulations while enabling rapid innovation and maintaining competitive advantages in aerospace technology.",
    question: "Which governance framework ensures OPTIMAL aerospace collaboration with IP protection and aviation safety compliance?",
    options: [
        "AWS Organizations with company-based OUs, AWS Config for aerospace compliance, AWS PrivateLink for consortium collaboration, AWS CloudTrail for technology audit trails, and AWS WorkSpaces for secure development",
        "AWS Organizations with country-specific OUs for regulatory compliance, AWS Lake Formation for technology sharing governance and IP protection, AWS PrivateLink for secure consortium collaboration, AWS CloudTrail with legal hold for aviation safety audit trails, and AWS GovCloud for sensitive aerospace technology",
        "AWS Control Tower for aerospace standardization, AWS Security Hub for compliance monitoring, Amazon S3 for aerospace data storage, AWS Lambda for data processing, and AWS Direct Connect for consortium connectivity",
        "AWS Landing Zone with aerospace templates, AWS Directory Service for engineer authentication, Amazon ElastiCache for aerospace data caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with varying aviation regulations across 40+ companies globally. Lake Formation provides technology sharing governance and IP protection through fine-grained access controls for aerospace innovations. PrivateLink enables secure consortium collaboration without internet exposure. CloudTrail with legal hold provides aviation safety audit trails for regulatory compliance. GovCloud provides additional security for sensitive aerospace technology development.",
        whyWrong: {
            0: "Company-based OUs don't address country-specific aviation regulations, basic WorkSpaces doesn't provide IP protection",
            2: "Control Tower doesn't address country-specific aviation regulations, basic S3 doesn't provide technology governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the IP protection needed for aerospace technology"
        },
        examStrategy: "Aerospace collaboration: Organizations + country OUs + Lake Formation + PrivateLink + CloudTrail legal hold + GovCloud for aviation safety."
    }
},
{
    id: 'sap_430',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart water management platform monitors 1 million+ water meters and sensors across urban water systems, detects leaks in real-time, optimizes water pressure, manages quality testing, and provides consumption analytics while ensuring water safety and reducing waste through intelligent distribution across metropolitan areas.",
    question: "Which architecture provides OPTIMAL smart water management with leak detection and quality assurance?",
    options: [
        "AWS IoT Core for water sensor connectivity, Amazon MSK for high-volume water data, Amazon EMR for water analytics, AWS Lambda for leak processing, and Amazon S3 for water data storage",
        "Amazon Kinesis Data Streams for real-time water sensor data, Amazon Kinesis Data Analytics for leak detection and pressure optimization, Amazon SageMaker for water quality prediction, AWS Lambda for automated responses, and Amazon Timestream for water system performance metrics",
        "Amazon EventBridge for water events, AWS Batch for leak calculations, Amazon DocumentDB for sensor data, Amazon API Gateway for utility integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for water edge processing, Amazon DynamoDB for water records, AWS Step Functions for water workflows, Amazon Redshift for analytics, and Amazon QuickSight for water dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles data from 1M+ water sensors for real-time monitoring. Kinesis Data Analytics provides real-time leak detection and pressure optimization algorithms. SageMaker predicts water quality issues and optimizes distribution systems. Lambda provides automated responses to leaks and quality issues. Timestream tracks water system performance metrics and waste reduction efficiently for urban water management.",
        whyWrong: {
            0: "MSK adds complexity for water sensor communication, EMR is for batch analytics not real-time leak detection",
            2: "EventBridge and Batch add latency inappropriate for real-time leak detection and emergency responses",
            3: "IoT Greengrass on every sensor adds operational complexity, Step Functions add latency for real-time water management"
        },
        examStrategy: "Smart water management: Kinesis ecosystem + SageMaker + Lambda + Timestream for intelligent water system optimization."
    }
},
{
    id: 'sap_431',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global logistics network coordinates 5 million packages daily across air, sea, and land transport, experiences package tracking delays, has route optimization inefficiencies, faces capacity planning challenges during peak seasons, and needs real-time visibility for customers while reducing operational costs and carbon footprint across complex international supply chains.",
    question: "Which optimization strategy provides the BEST logistics performance with environmental sustainability and customer visibility?",
    options: [
        "Implement Amazon ElastiCache for package caching, AWS Lambda for tracking updates, Amazon DynamoDB for logistics data, Amazon API Gateway for customer visibility, and Amazon CloudWatch for monitoring",
        "Deploy Amazon DynamoDB Global Tables for package tracking, Amazon SageMaker for route optimization and capacity planning, Amazon Kinesis Data Streams for real-time logistics events, AWS Lambda for carbon footprint calculation, and Amazon API Gateway for customer visibility APIs",
        "Use Amazon Aurora Global Database for package records, Amazon MSK for real-time messaging, AWS Step Functions for logistics workflows, Amazon EventBridge for distribution coordination, and AWS Batch for route calculations",
        "Implement Amazon DocumentDB for flexible logistics schemas, Amazon EventBridge for package events, AWS Batch for optimization calculations, Amazon SQS for distribution communication, and Amazon EMR for sustainability analytics"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB Global Tables provides conflict-free package tracking for 5M daily packages across international supply chains with automatic scaling. SageMaker optimizes routes and predicts capacity needs for peak seasons while minimizing carbon footprint through intelligent planning. Kinesis Data Streams handles real-time logistics events efficiently across multiple transport modes. Lambda calculates environmental impact automatically for sustainability reporting. API Gateway provides scalable customer visibility with global distribution.",
        whyWrong: {
            0: "Basic caching doesn't solve complex international logistics optimization at 5M package scale",
            2: "Aurora Global Database has write limitations for high-frequency package updates across global operations",
            3: "DocumentDB doesn't optimize for logistics tracking patterns at global scale, EventBridge adds latency for real-time tracking"
        },
        examStrategy: "Global logistics: DynamoDB Global Tables + SageMaker + Kinesis + Lambda + API Gateway for sustainable international logistics."
    }
},
{
    id: 'sap_432',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national intelligence agency needs to migrate their cyber threat intelligence and attribution systems from air-gapped networks to AWS, maintaining the highest classification levels, enabling real-time threat hunting, supporting multi-source intelligence fusion, and ensuring absolute security isolation while advancing analytical capabilities for national cybersecurity defense.",
    question: "Which modernization approach ensures MAXIMUM security for classified cyber intelligence operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for intelligence data, use AWS Lambda for threat analysis, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Deploy AWS GovCloud with classification-isolated VPCs and air-gapped networking, AWS ParallelCluster for advanced cyber threat analysis, AWS Lake Formation for intelligence data governance, Amazon Kinesis Data Analytics for real-time threat hunting, and AWS CloudTrail with tamper detection for audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DynamoDB for threat records, AWS Batch for analysis workflows, Amazon S3 for intelligence data, and AWS Config for classification compliance",
        "Use AWS Outposts in secure facilities, Amazon FSx for intelligence data, AWS Step Functions for analysis workflows, Amazon ElastiCache for performance, and AWS Direct Connect for inter-agency connectivity"
    ],
    correct: 1,
    explanation: {
        correct: "GovCloud with classification-isolated VPCs and air-gapped networking provides maximum security separation for classified cyber intelligence. ParallelCluster delivers advanced computing for sophisticated threat analysis and attribution capabilities. Lake Formation provides intelligence data governance with strict access controls for cyber threat data. Kinesis Data Analytics enables real-time threat hunting and multi-source intelligence fusion. CloudTrail with tamper detection ensures audit compliance for national cybersecurity operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide classification-level security isolation needed for cyber intelligence operations",
            2: "Control Tower doesn't provide classification-isolated security needed for cyber threat intelligence",
            3: "Outposts requires on-premises management, Direct Connect may compromise air-gapped security requirements for classified cyber operations"
        },
        examStrategy: "Cyber intelligence: GovCloud air-gapped VPCs + ParallelCluster + Lake Formation + Kinesis Analytics + CloudTrail tamper detection for cyber defense."
    }
},
{
    id: 'sap_433',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational mining corporation operates in 80+ countries with varying environmental regulations, needs real-time equipment monitoring, environmental compliance reporting, worker safety management, and operational coordination while maintaining independence during connectivity disruptions in remote locations and ensuring sustainable mining practices.",
    question: "Which governance architecture provides OPTIMAL mining operations with environmental compliance and remote operational resilience?",
    options: [
        "AWS Organizations with commodity-based OUs, AWS Directory Service for worker authentication, Amazon ElastiCache for operations caching, AWS Batch for environmental analysis, and AWS Certificate Manager for secure communications",
        "AWS Organizations with country-specific OUs for environmental compliance, AWS IoT Core for equipment and safety monitoring, Amazon Kinesis Data Analytics for real-time operational optimization, AWS Outposts for remote site independence, and AWS Config for continuous environmental regulatory monitoring",
        "AWS Control Tower for mining standardization, AWS Security Hub for compliance monitoring, Amazon MSK for equipment communication, AWS Lambda for safety processing, and AWS Direct Connect for site connectivity",
        "AWS Landing Zone with mining templates, AWS GuardDuty for site security, Amazon Redshift for operations analytics, AWS Glue for compliance data integration, and Amazon QuickSight for safety dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 80+ different environmental regulations for sustainable mining. IoT Core manages equipment and safety monitoring systems securely across remote mining sites. Kinesis Data Analytics provides real-time operational optimization and safety analysis for mining operations. Outposts enables remote mining sites to operate independently during connectivity disruptions in isolated locations. Config monitors environmental regulatory compliance continuously for sustainable practices.",
        whyWrong: {
            0: "Commodity-based OUs don't address country-specific environmental regulations for sustainable mining",
            2: "Control Tower doesn't address country-specific environmental regulations, MSK adds complexity for mining equipment communication",
            3: "Landing Zone is deprecated, GuardDuty alone doesn't provide the environmental compliance framework needed for mining"
        },
        examStrategy: "Mining governance: Organizations + country OUs + IoT Core + Kinesis Analytics + Outposts + Config for sustainable remote mining."
    }
},
{
    id: 'sap_434',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global digital health platform integrates 500,000+ medical devices, manages patient monitoring, enables telemedicine consultations, provides AI-driven diagnostics, and facilitates medical research collaboration while ensuring HIPAA compliance, patient privacy protection, and enabling breakthrough medical discoveries through federated learning across healthcare institutions.",
    question: "Which architecture provides OPTIMAL digital health platform with AI diagnostics and federated medical research?",
    options: [
        "Amazon EC2 with GPU instances for medical analysis, Amazon S3 for patient data, AWS Lambda for diagnostic processing, Amazon DynamoDB for medical records, and AWS IAM for access control",
        "AWS IoT Core for medical device management, Amazon SageMaker for AI diagnostics and federated learning, Amazon HealthLake for patient data management, AWS Lake Formation for HIPAA compliance and privacy protection, and Amazon Chime SDK for telemedicine consultations",
        "AWS Batch for medical workflows, Amazon EFS for shared research data, AWS Step Functions for diagnostic coordination, Amazon DocumentDB for patient metadata, and AWS VPN for healthcare access",
        "Amazon EMR for medical analytics, Amazon Redshift for health data warehousing, AWS Glue for data processing, Amazon QuickSight for medical visualization, and AWS Direct Connect for healthcare collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "IoT Core provides secure management for 500,000+ medical devices with healthcare-grade security. SageMaker enables AI-driven diagnostics and federated learning for medical research while preserving patient privacy. HealthLake provides purpose-built healthcare data management with FHIR support and medical interoperability. Lake Formation ensures HIPAA compliance and patient privacy protection across institutions. Chime SDK enables secure telemedicine consultations with healthcare compliance.",
        whyWrong: {
            0: "Standard EC2 doesn't provide healthcare-specific AI capabilities, basic IAM doesn't provide HIPAA privacy protection",
            2: "Batch alone doesn't provide AI diagnostics capabilities, EFS doesn't provide HIPAA privacy protection for medical data",
            3: "EMR doesn't provide healthcare-specific AI or federated learning capabilities, Redshift doesn't optimize for medical workflows"
        },
        examStrategy: "Digital health: IoT Core + SageMaker + HealthLake + Lake Formation + Chime SDK for AI-driven HIPAA-compliant healthcare."
    }
},
{
    id: 'sap_435',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online gaming platform serves 500 million players across multiple game titles, experiences matchmaking delays during peak hours, has in-game purchase processing issues, faces regional content regulation challenges, and needs real-time player analytics while optimizing server costs and player experience across diverse global gaming markets.",
    question: "Which optimization strategy provides the BEST gaming platform performance with cost efficiency and global player satisfaction?",
    options: [
        "Implement Amazon ElastiCache for player session caching, AWS Lambda for matchmaking processing, Amazon DynamoDB for player data, Amazon API Gateway for game services, and Amazon CloudWatch for monitoring",
        "Deploy Amazon GameLift for managed game hosting and matchmaking, Amazon ElastiCache Redis cluster mode for player state, Amazon DynamoDB Global Tables for player progress, Amazon Kinesis for real-time player analytics, and AWS Lambda for in-game purchase processing with regional content control",
        "Use Amazon Aurora Global Database for player records, Amazon MSK for real-time messaging, AWS Step Functions for game workflows, Amazon EventBridge for player coordination, and AWS Batch for analytics processing",
        "Implement Amazon DocumentDB for flexible player schemas, Amazon EventBridge for game events, AWS Batch for matchmaking calculations, Amazon SQS for player communication, and Amazon EMR for player analytics"
    ],
    correct: 1,
    explanation: {
        correct: "GameLift provides managed game hosting with automatic scaling and intelligent matchmaking for 500M players with cost optimization. ElastiCache Redis cluster mode delivers sub-millisecond player state access with automatic scaling for peak hours. DynamoDB Global Tables manages player progress across global gaming markets. Kinesis provides real-time player analytics for game optimization and monetization. Lambda handles in-game purchases with automatic scaling and regional content control for compliance.",
        whyWrong: {
            0: "Basic caching doesn't solve game-specific matchmaking optimization for 500M players at global scale",
            2: "Aurora Global Database has limitations for high-frequency player updates, Step Functions add latency for real-time gaming",
            3: "DocumentDB doesn't optimize for gaming patterns at global scale, EventBridge adds latency for real-time player interactions"
        },
        examStrategy: "Gaming optimization: GameLift + ElastiCache cluster + DynamoDB Global Tables + Kinesis + Lambda for global gaming excellence."
    }
},
{
    id: 'sap_436',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national transportation authority needs to migrate their smart city traffic management and public transit systems from legacy infrastructure to AWS, supporting real-time traffic optimization, public safety integration, citizen services, and environmental monitoring while ensuring high availability for critical transportation infrastructure and meeting government security requirements.",
    question: "Which modernization approach provides OPTIMAL smart transportation with comprehensive safety and environmental monitoring?",
    options: [
        "Deploy Amazon EC2 for transportation applications, Amazon RDS for traffic data, AWS Lambda for optimization processing, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Deploy AWS Control Tower in GovCloud for government compliance, AWS IoT Core for traffic infrastructure monitoring, Amazon Kinesis Data Analytics for real-time traffic optimization, Amazon DynamoDB for transportation data, and AWS Lambda for public safety integration and citizen services",
        "Implement AWS Elastic Beanstalk for traffic applications, Amazon Aurora for transportation database, AWS Step Functions for traffic workflows, Amazon CloudFront for citizen portal, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible traffic data, AWS EventBridge for transportation coordination, AWS Lambda for microservices, and AWS Security Hub for compliance management"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for critical transportation infrastructure. IoT Core manages traffic signals, sensors, and transit systems securely for smart city initiatives. Kinesis Data Analytics enables real-time traffic optimization and environmental monitoring for sustainable transportation. DynamoDB scales for transportation data with high availability for critical infrastructure. Lambda handles public safety integration and citizen services with automatic scaling for government operations.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance needed for critical transportation infrastructure",
            2: "Elastic Beanstalk doesn't provide government security controls or smart city IoT capabilities needed",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for transportation data patterns and government security requirements"
        },
        examStrategy: "Smart transportation: Control Tower GovCloud + IoT Core + Kinesis Analytics + DynamoDB + Lambda for secure city infrastructure."
    }
},
{
    id: 'sap_437',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global pharmaceutical consortium coordinates drug development across 60+ companies and 300+ research institutions, requires secure intellectual property sharing, regulatory compliance across jurisdictions, competitive collaboration controls, and clinical trial coordination while protecting proprietary research and maintaining fair access to breakthrough discoveries through controlled technology transfer.",
    question: "Which governance framework ensures COMPREHENSIVE pharmaceutical collaboration with IP protection and controlled technology transfer?",
    options: [
        "AWS Organizations with company-based OUs, AWS Config for pharma compliance, AWS PrivateLink for research collaboration, AWS CloudTrail for IP audit trails, and AWS WorkSpaces for secure development",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for IP protection and competitive collaboration controls, AWS PrivateLink for secure consortium collaboration, AWS CloudTrail with legal hold for pharmaceutical audit trails, and AWS Resource Access Manager for controlled breakthrough discovery sharing",
        "AWS Control Tower for pharmaceutical standardization, AWS Security Hub for compliance monitoring, Amazon S3 for research data storage, AWS Lambda for data processing, and AWS Direct Connect for institutional connectivity",
        "AWS Landing Zone with pharmaceutical templates, AWS Directory Service for researcher authentication, Amazon ElastiCache for research caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with varying pharmaceutical regulations globally for 60+ companies. Lake Formation provides IP protection and competitive collaboration controls through fine-grained access controls for proprietary research. PrivateLink enables secure consortium collaboration without internet exposure. CloudTrail with legal hold provides pharmaceutical audit trails for regulatory compliance and IP protection. RAM enables controlled sharing of breakthrough discoveries among consortium members while protecting proprietary technology.",
        whyWrong: {
            0: "Company-based OUs don't address jurisdiction-specific regulatory requirements, basic WorkSpaces doesn't provide competitive controls",
            2: "Control Tower doesn't address jurisdiction-specific pharmaceutical regulations, basic S3 doesn't provide IP protection",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the competitive collaboration controls needed for pharmaceutical research"
        },
        examStrategy: "Pharmaceutical consortium: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + RAM for controlled collaboration."
    }
},
{
    id: 'sap_438',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart agriculture platform manages 2 million+ farms globally using satellite imagery, IoT sensors, and drones, optimizes crop yields, predicts weather impacts, manages resource usage, and provides market insights while ensuring environmental sustainability and enabling small-scale farmer access to advanced agricultural technology across diverse global regions.",
    question: "Which architecture provides OPTIMAL smart agriculture with sustainability focus and global farmer accessibility?",
    options: [
        "AWS IoT Core for farm sensor connectivity, Amazon MSK for high-volume agricultural data, Amazon EMR for crop analytics, AWS Lambda for farm optimization, and Amazon S3 for agricultural data storage",
        "Amazon Kinesis Data Streams for sensor ingestion, Amazon SageMaker for yield prediction and weather analysis, AWS Lake Formation for agricultural data governance, AWS IoT Core for farm device management, and Amazon Timestream for agricultural time-series data and sustainability metrics",
        "Amazon EventBridge for farm events, AWS Batch for crop calculations, Amazon DocumentDB for farm data, Amazon API Gateway for farmer integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for farm edge processing, Amazon DynamoDB for crop records, AWS Step Functions for agricultural workflows, Amazon Redshift for analytics, and Amazon QuickSight for farm dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles sensor data from 2M+ farms globally for real-time agricultural optimization. SageMaker provides sophisticated yield prediction and weather impact analysis for farmers worldwide. Lake Formation provides agricultural data governance and farmer privacy protection across diverse global regions. IoT Core manages farm devices securely with global scalability for small-scale farmer access. Timestream optimizes agricultural time-series data and sustainability metrics tracking efficiently.",
        whyWrong: {
            0: "MSK adds complexity for farm sensor communication, EMR is for batch analytics not real-time agricultural optimization",
            2: "EventBridge and Batch add latency inappropriate for real-time agricultural monitoring and weather response",
            3: "IoT Greengrass on every farm adds operational complexity, Step Functions add latency for real-time crop optimization"
        },
        examStrategy: "Smart agriculture: Kinesis + SageMaker + Lake Formation + IoT Core + Timestream for sustainable global farming intelligence."
    }
},
{
    id: 'sap_439',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial services platform processes 200 million transactions daily across banking, insurance, and investment services, experiences latency spikes during market volatility, has cross-service data synchronization issues, faces regulatory reporting bottlenecks across 80+ jurisdictions, and needs real-time fraud detection while maintaining sub-second response times for customer transactions worldwide.",
    question: "Which optimization strategy provides the BEST financial services performance with comprehensive fraud protection and global regulatory compliance?",
    options: [
        "Implement Amazon ElastiCache for transaction caching, AWS Lambda for financial processing, Amazon DynamoDB for customer data, Amazon API Gateway for service APIs, and Amazon CloudWatch for monitoring",
        "Deploy Amazon ElastiCache Redis cluster mode for real-time transaction state, Amazon SageMaker for real-time fraud detection and risk assessment, Amazon DynamoDB Global Tables for customer data synchronization, Amazon Kinesis for transaction event streaming, and AWS Config for multi-jurisdiction regulatory compliance monitoring",
        "Use Amazon Aurora Global Database for financial records, Amazon MSK for real-time messaging, AWS Step Functions for financial workflows, Amazon EventBridge for service coordination, and AWS Batch for fraud analysis",
        "Implement Amazon DocumentDB for flexible financial schemas, Amazon EventBridge for transaction events, AWS Batch for compliance calculations, Amazon SQS for service communication, and Amazon EMR for fraud analytics"
    ],
    correct: 1,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-second transaction access for 200M daily transactions with automatic scaling across global financial services. SageMaker enables real-time fraud detection and risk assessment across banking, insurance, and investment services with low-latency inference. DynamoDB Global Tables ensures customer data synchronization across services and regions with conflict resolution. Kinesis handles transaction event streaming efficiently for global scale. Config monitors regulatory compliance across 80+ jurisdictions continuously for comprehensive compliance management.",
        whyWrong: {
            0: "Basic caching doesn't solve cross-service synchronization at 200M transaction scale, CloudWatch doesn't provide fraud detection",
            2: "Aurora Global Database has limitations for high-frequency transaction updates at global scale, Batch adds latency for real-time fraud detection",
            3: "DocumentDB doesn't optimize for financial transaction patterns at global scale, EventBridge adds latency for real-time transaction processing"
        },
        examStrategy: "Global financial services: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Config for comprehensive financial operations."
    }
},
{
    id: 'sap_440',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national defense logistics agency needs to migrate their military supply chain and equipment tracking systems from legacy infrastructure to AWS, supporting global military operations, real-time asset tracking, mission-critical supply delivery, and secure communication while maintaining the highest security classification levels and ensuring operational readiness for defense missions.",
    question: "Which modernization approach ensures MAXIMUM security for classified defense logistics operations?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for logistics data, use AWS Lambda for supply processing, Amazon API Gateway for system integration, and AWS CloudTrail for audit compliance",
        "Deploy AWS GovCloud with classification-isolated VPCs for defense systems, AWS IoT Core for military asset tracking, Amazon DynamoDB Global Tables for supply chain resilience, AWS Lambda for mission-critical logistics automation, and AWS CloudTrail with tamper detection for defense audit compliance",
        "Implement AWS Control Tower in GovCloud, Amazon DocumentDB for flexible logistics data, AWS Batch for supply calculations, Amazon S3 for equipment data, and AWS Config for security compliance",
        "Use AWS Outposts for secure facilities, Amazon FSx for logistics data, AWS Step Functions for supply workflows, Amazon ElastiCache for performance, and AWS Direct Connect for military connectivity"
    ],
    correct: 1,
    explanation: {
        correct: "GovCloud with classification-isolated VPCs provides maximum security separation for classified defense logistics. IoT Core manages military asset tracking securely with defense-grade device management for global operations. DynamoDB Global Tables ensures supply chain resilience across regions for mission-critical delivery. Lambda handles logistics automation with automatic scaling for defense operations. CloudTrail with tamper detection provides defense audit compliance for classified logistics operations and operational readiness.",
        whyWrong: {
            0: "Standard deployment doesn't provide classification-level security isolation needed for defense logistics",
            2: "Control Tower doesn't provide classification-isolated security needed for military supply chain operations",
            3: "Outposts requires on-premises management inappropriate for global military operations, Direct Connect may compromise security requirements"
        },
        examStrategy: "Defense logistics: GovCloud isolated VPCs + IoT Core + DynamoDB Global Tables + Lambda + CloudTrail tamper detection for classified operations."
    }
},
{
    id: 'sap_441',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational oil and gas company operates drilling platforms and refineries across 50+ countries with varying environmental regulations, needs real-time safety monitoring, environmental compliance reporting, operational coordination, and emergency response capabilities while maintaining operational independence during connectivity issues and ensuring sustainable energy production practices.",
    question: "Which governance architecture provides OPTIMAL oil and gas operations with environmental compliance and emergency resilience?",
    options: [
        "AWS Organizations with facility-based OUs, AWS Config for safety compliance, AWS IoT Core for equipment monitoring, Amazon Kinesis for operational data, and AWS Storage Gateway for operational independence",
        "AWS Organizations with country-specific OUs for environmental compliance, AWS IoT Core for safety and equipment monitoring, Amazon Kinesis Data Analytics for real-time operational optimization, AWS Outposts for remote facility independence, and AWS Config for continuous environmental regulatory monitoring",
        "AWS Control Tower for industry standardization, AWS Security Hub for compliance monitoring, Amazon MSK for facility communication, AWS Lambda for safety processing, and AWS Direct Connect for facility connectivity",
        "AWS Landing Zone with energy templates, AWS Directory Service for worker authentication, Amazon ElastiCache for operations caching, AWS Batch for environmental analysis, and AWS Certificate Manager for secure communications"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with country OUs ensure compliance with 50+ different environmental regulations for sustainable energy production. IoT Core manages safety and equipment monitoring systems across drilling platforms and refineries securely. Kinesis Data Analytics provides real-time operational optimization and safety analysis for oil and gas operations. Outposts enables remote facilities to operate independently during connectivity issues in isolated locations. Config monitors environmental regulatory compliance continuously for sustainable practices and regulatory adherence.",
        whyWrong: {
            0: "Facility-based OUs don't address country-specific environmental regulations for sustainable energy production",
            2: "Control Tower doesn't address country-specific environmental regulations, MSK adds complexity for facility communication",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the environmental governance needed for oil and gas operations"
        },
        examStrategy: "Oil and gas governance: Organizations + country OUs + IoT Core + Kinesis Analytics + Outposts + Config for sustainable energy compliance."
    }
},
{
    id: 'sap_442',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global climate research platform enables 5,000+ scientists to collaborate on climate modeling, atmospheric analysis, ocean current studies, and carbon cycle research, requiring massive computational resources, secure data sharing, international collaboration, and long-term data preservation while advancing climate science and supporting policy decisions for climate action.",
    question: "Which architecture provides OPTIMAL climate research collaboration with advanced computational capabilities and data preservation?",
    options: [
        "Amazon EC2 with GPU instances for climate modeling, Amazon S3 for research data, AWS Lambda for climate analysis, Amazon DynamoDB for research records, and AWS IAM for access control",
        "Amazon SageMaker for climate prediction modeling, AWS ParallelCluster for massive atmospheric simulations, AWS Lake Formation for research data governance and international collaboration, Amazon S3 Glacier Deep Archive for long-term climate data preservation, and AWS PrivateLink for secure scientific collaboration",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for research coordination, Amazon DocumentDB for climate metadata, and AWS VPN for scientist access",
        "Amazon EMR for climate analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for climate visualization, and AWS Direct Connect for institutional collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for advanced climate prediction modeling and carbon cycle analysis. ParallelCluster delivers massive computational resources for atmospheric simulations and ocean current studies at climate research scale. Lake Formation provides research data governance and enables secure international collaboration among 5,000+ scientists. S3 Glacier Deep Archive provides cost-effective long-term climate data preservation for policy decisions. PrivateLink ensures secure scientific collaboration while protecting research data integrity.",
        whyWrong: {
            0: "Standard EC2 doesn't provide climate-specific modeling capabilities, basic IAM doesn't provide research collaboration governance",
            2: "Batch alone doesn't provide climate modeling capabilities, EFS doesn't provide long-term data preservation for climate research",
            3: "EMR doesn't provide climate-specific modeling or massive computational capabilities needed for atmospheric research"
        },
        examStrategy: "Climate research: SageMaker + ParallelCluster + Lake Formation + S3 Glacier Deep Archive + PrivateLink for advanced climate science."
    }
},
{
    id: 'sap_443',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global e-commerce marketplace with 2 billion products experiences search performance degradation, product recommendation delays, inventory synchronization issues across regions, seller performance tracking problems, and payment processing bottlenecks during major sales events while serving 1 billion customers worldwide across diverse markets.",
    question: "Which optimization strategy provides the BEST marketplace performance and seller experience for massive global e-commerce scale?",
    options: [
        "Implement Amazon ElastiCache for product caching, AWS Lambda for search processing, Amazon DynamoDB for inventory data, Amazon API Gateway for seller APIs, and Amazon SQS for payment processing",
        "Deploy Amazon OpenSearch for intelligent product search, Amazon SageMaker for real-time recommendations, Amazon DynamoDB Global Tables for inventory synchronization, AWS Lambda for seller analytics, and Amazon Kinesis for payment event processing",
        "Use Amazon Aurora Global Database for product catalog, Amazon MSK for real-time updates, AWS Step Functions for seller workflows, Amazon EventBridge for marketplace coordination, and AWS Batch for payment processing",
        "Implement Amazon DocumentDB for flexible product schemas, Amazon EventBridge for marketplace events, AWS Batch for recommendation calculations, Amazon SQS for seller communication, and Amazon EMR for payment analytics"
    ],
    correct: 1,
    explanation: {
        correct: "OpenSearch provides sophisticated product search capabilities for 2B products with real-time indexing and relevance optimization for global markets. SageMaker enables real-time recommendation inference for 1B customers with personalization at scale. DynamoDB Global Tables ensures inventory synchronization across regions with conflict resolution for global operations. Lambda provides seller analytics with automatic scaling for marketplace operations. Kinesis handles payment events during sales spikes efficiently for massive e-commerce scale.",
        whyWrong: {
            0: "Basic Lambda search processing can't match OpenSearch capabilities for 2B products at global scale",
            2: "Aurora Global Database has write limitations for high-frequency inventory updates across global regions",
            3: "DocumentDB doesn't optimize for marketplace search patterns at 2B product scale, EventBridge adds latency for real-time inventory"
        },
        examStrategy: "Massive marketplace: OpenSearch + SageMaker + DynamoDB Global Tables + Lambda + Kinesis for global e-commerce excellence."
    }
},
{
    id: 'sap_444',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A national social security administration needs to migrate their benefits processing systems from legacy mainframes to AWS, handling 200 million beneficiaries, ensuring data privacy, maintaining fraud detection capabilities, supporting citizen self-service, and ensuring high availability for critical benefit payments while meeting government security requirements and improving citizen experience.",
    question: "Which modernization approach provides OPTIMAL citizen services with comprehensive security and fraud protection for massive-scale benefit processing?",
    options: [
        "Deploy Amazon EC2 in AWS GovCloud, implement Amazon RDS for beneficiary data, use AWS Lambda for benefits processing, Amazon API Gateway for citizen services, and AWS CloudTrail for audit compliance",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB for beneficiary records, Amazon SageMaker for fraud detection, AWS Lambda for benefits processing, and Amazon Kinesis Data Analytics for real-time benefit monitoring with AWS Config for government security compliance",
        "Implement AWS Elastic Beanstalk for benefits applications, Amazon Aurora for beneficiary database, AWS Step Functions for processing workflows, Amazon CloudFront for citizen portal, and AWS Config for compliance monitoring",
        "Use Amazon EKS for containerized services, Amazon DocumentDB for flexible beneficiary data, AWS EventBridge for processing coordination, AWS Batch for benefits calculations, and AWS Security Hub for compliance management"
    ],
    correct: 1,
    explanation: {
        correct: "Control Tower in GovCloud provides government-compliant governance for national social security operations. DynamoDB scales to handle 200M beneficiaries with high availability for critical benefit payments and citizen services. SageMaker provides sophisticated fraud detection capabilities with real-time inference for benefit protection. Lambda handles benefits processing with automatic scaling for citizen demands. Kinesis Data Analytics enables real-time benefit monitoring, and Config ensures continuous government security compliance for citizen data protection.",
        whyWrong: {
            0: "Standard deployment doesn't provide government security compliance and fraud detection needed for social security at 200M beneficiary scale",
            2: "Elastic Beanstalk doesn't provide government security controls needed for national social security systems",
            3: "EKS adds operational complexity, DocumentDB doesn't optimize for social security processing patterns at massive government scale"
        },
        examStrategy: "Social security modernization: Control Tower GovCloud + DynamoDB + SageMaker + Lambda + Kinesis Analytics + Config for secure citizen services."
    }
},
{
    id: 'sap_445',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global investment banking consortium coordinates complex financial transactions across 100+ institutions, requires real-time risk assessment, regulatory compliance across jurisdictions, secure transaction processing, and competitive intelligence protection while enabling collaborative deal syndication and maintaining operational resilience during market volatility for multi-billion dollar transactions.",
    question: "Which governance framework ensures COMPREHENSIVE investment banking collaboration with competitive protection and market resilience?",
    options: [
        "AWS Organizations with institution-based OUs, AWS Config for banking compliance, AWS Lambda for transaction processing, Amazon DynamoDB for deal data, and AWS PrivateLink for secure communication",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for competitive intelligence protection and deal governance, Amazon ElastiCache for real-time risk assessment, AWS Lambda for algorithmic transaction processing, and AWS CloudTrail with legal hold for regulatory audit trails with multi-region resilience",
        "AWS Control Tower for banking standardization, AWS Security Hub for compliance monitoring, Amazon Kinesis for transaction data, AWS Step Functions for deal workflows, and AWS Direct Connect for institutional connectivity",
        "AWS Landing Zone with banking templates, AWS Directory Service for trader authentication, Amazon ElastiCache for deal caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for deal collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with varying financial regulations across 100+ institutions globally. Lake Formation provides competitive intelligence protection and deal governance through fine-grained access controls for multi-billion dollar transactions. ElastiCache enables real-time risk assessment with microsecond latency for market volatility. Lambda scales for algorithmic transaction processing with competitive advantages. CloudTrail with legal hold provides regulatory audit trails with multi-region resilience for operational continuity during market stress.",
        whyWrong: {
            0: "Institution-based OUs don't address jurisdiction-specific regulatory requirements for global banking operations",
            2: "Control Tower doesn't address jurisdiction-specific banking regulations for complex international transactions",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the competitive intelligence protection needed for investment banking"
        },
        examStrategy: "Investment banking: Organizations + jurisdiction OUs + Lake Formation + ElastiCache + Lambda + CloudTrail legal hold for competitive market resilience."
    }
},
{
    id: 'sap_446',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart city environmental monitoring platform tracks air quality, noise levels, water quality, and energy consumption across 10,000+ sensors in a major metropolitan area, provides real-time environmental insights, enables pollution source identification, and supports environmental policy decisions while ensuring public health protection and sustainable urban development.",
    question: "Which architecture provides OPTIMAL environmental monitoring with policy support and public health protection?",
    options: [
        "AWS IoT Core for environmental sensor connectivity, Amazon MSK for high-volume environmental data, Amazon EMR for environmental analytics, AWS Lambda for pollution processing, and Amazon S3 for environmental data storage",
        "Amazon Kinesis Data Streams for sensor ingestion, Amazon Kinesis Data Analytics for real-time environmental analysis, Amazon SageMaker for pollution source identification and air quality prediction, AWS Lambda for public health alerts, and Amazon Timestream for environmental metrics and policy analytics",
        "Amazon EventBridge for environmental events, AWS Batch for pollution calculations, Amazon DocumentDB for sensor data, Amazon API Gateway for city integration, and Amazon CloudWatch for monitoring",
        "AWS IoT Greengrass for sensor edge processing, Amazon DynamoDB for environmental records, AWS Step Functions for monitoring workflows, Amazon Redshift for analytics, and Amazon QuickSight for environmental dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles data from 10,000+ environmental sensors for real-time metropolitan monitoring. Kinesis Data Analytics provides real-time environmental analysis for immediate pollution detection and public health protection. SageMaker enables pollution source identification and air quality prediction for policy decisions. Lambda provides public health alerts with automatic scaling for environmental emergencies. Timestream tracks environmental metrics and policy analytics efficiently for sustainable urban development and regulatory compliance.",
        whyWrong: {
            0: "MSK adds complexity for environmental sensor communication, EMR is for batch analytics not real-time environmental monitoring",
            2: "EventBridge and Batch add latency inappropriate for real-time environmental monitoring and public health alerts",
            3: "IoT Greengrass on every sensor adds operational complexity, Step Functions add latency for real-time environmental analysis"
        },
        examStrategy: "Environmental monitoring: Kinesis ecosystem + SageMaker + Lambda + Timestream for intelligent sustainable city management."
    }
},
{
    id: 'sap_447',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global telecommunications network serves 2 billion subscribers across 120+ countries, experiences network congestion during peak hours, has 5G rollout optimization challenges, faces real-time fault detection requirements, and needs predictive maintenance for network infrastructure while reducing operational costs and energy consumption across diverse global markets with varying regulations.",
    question: "Which optimization strategy provides the BEST telecommunications network performance with sustainability and global regulatory compliance?",
    options: [
        "Implement Amazon ElastiCache for network caching, AWS Lambda for fault processing, Amazon DynamoDB for network data, Amazon API Gateway for subscriber services, and Amazon CloudWatch for monitoring",
        "Deploy Amazon ElastiCache Redis cluster mode for real-time network state, Amazon SageMaker for predictive maintenance and 5G optimization, Amazon DynamoDB Global Tables for subscriber management, Amazon Kinesis for network telemetry, and AWS Lambda for fault detection and energy optimization with AWS Config for multi-country regulatory compliance",
        "Use Amazon Aurora Global Database for network records, Amazon MSK for real-time messaging, AWS Step Functions for network workflows, Amazon EventBridge for fault coordination, and AWS Batch for maintenance calculations",
        "Implement Amazon DocumentDB for flexible network schemas, Amazon EventBridge for network events, AWS Batch for optimization calculations, Amazon SQS for fault handling, and Amazon EMR for sustainability analytics"
    ],
    correct: 1,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides real-time network state access for 2B subscribers with automatic scaling across 120+ countries. SageMaker enables predictive maintenance and 5G rollout optimization while reducing energy consumption for sustainability goals. DynamoDB Global Tables manages subscribers across diverse global markets with regulatory compliance. Kinesis handles network telemetry efficiently for massive telecommunications scale. Lambda provides real-time fault detection and energy optimization, while Config monitors regulatory compliance across varying international regulations.",
        whyWrong: {
            0: "Basic caching doesn't solve network optimization at 2B subscriber scale, CloudWatch doesn't provide predictive maintenance",
            2: "Aurora Global Database has limitations for high-frequency network updates at massive telecommunications scale",
            3: "DocumentDB doesn't optimize for telecommunications patterns at global scale, EventBridge adds latency for real-time network management"
        },
        examStrategy: "Global telecom: ElastiCache cluster + SageMaker + DynamoDB Global Tables + Kinesis + Lambda + Config for intelligent global networks."
    }
},
{
    id: 'sap_448',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national meteorological service needs to migrate their global weather prediction and climate monitoring systems from legacy supercomputers to AWS, supporting worldwide weather forecasting, severe weather alerting, international data exchange, and climate research while maintaining forecast accuracy and enabling advanced atmospheric research for climate science and public safety.",
    question: "Which modernization strategy ensures OPTIMAL weather prediction with global climate research and public safety capabilities?",
    options: [
        "Deploy Amazon EMR with Spark clusters, implement Amazon S3 for weather data, use AWS Glue for data processing, Amazon Redshift for climate analytics, and AWS Direct Connect for international collaboration",
        "Deploy AWS ParallelCluster with HPC-optimized instances for atmospheric modeling, Amazon FSx for Lustre for weather data processing, AWS Batch for ensemble forecasting, Amazon S3 with Cross-Region Replication for international data exchange, and AWS Ground Station for satellite weather data ingestion with Amazon SNS for public safety alerts",
        "Implement Amazon ECS with GPU instances, use Amazon EFS for shared storage, AWS Step Functions for forecast workflows, Amazon DocumentDB for weather metadata, and AWS VPN for collaborative access",
        "Use AWS Fargate for containerized forecasting, Amazon DynamoDB for weather records, Amazon EventBridge for forecast coordination, AWS Lambda for alert processing, and Amazon API Gateway for international APIs"
    ],
    correct: 1,
    explanation: {
        correct: "ParallelCluster with HPC instances provides supercomputing performance for global weather prediction and climate modeling research. FSx Lustre handles massive atmospheric datasets with parallel processing for weather forecasting accuracy. Batch schedules ensemble forecasting efficiently for prediction reliability. S3 with CRR enables international meteorological data exchange for global climate research. Ground Station ingests satellite weather data directly for improved forecast accuracy, and SNS delivers public safety alerts reliably for severe weather protection.",
        whyWrong: {
            0: "EMR is optimized for big data analytics, not intensive atmospheric modeling and global weather prediction computations",
            2: "ECS doesn't provide HPC optimizations needed for weather modeling, EFS doesn't match FSx Lustre performance for atmospheric research",
            3: "Fargate has resource limitations for atmospheric modeling, Lambda has execution time limits for complex weather forecasting"
        },
        examStrategy: "Global weather prediction: ParallelCluster + HPC + FSx Lustre + Batch + S3 CRR + Ground Station + SNS for comprehensive meteorology."
    }
},
{
    id: 'sap_449',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A multinational aerospace manufacturer designs next-generation aircraft across 20+ international programs, requires ITAR compliance for military projects, manages proprietary design data, enables secure supplier collaboration, and maintains strict export control while supporting concurrent engineering across time zones and advancing aerospace innovation for commercial and defense applications.",
    question: "Which governance framework ensures OPTIMAL aerospace design security with international collaboration and export control compliance?",
    options: [
        "AWS Organizations with program-based OUs, AWS Config for ITAR monitoring, AWS PrivateLink for supplier access, AWS CloudTrail for design audit trails, and AWS WorkSpaces for secure engineering collaboration",
        "AWS Organizations with ITAR classification OUs, AWS Lake Formation for design data governance and export control, AWS PrivateLink for secure supplier collaboration, AWS GovCloud for military programs, and AWS CloudTrail with tamper detection for compliance audit trails",
        "AWS Control Tower for aerospace standardization, AWS Security Hub for compliance monitoring, Amazon S3 for design storage, AWS Lambda for data processing, and AWS Direct Connect for supplier connectivity",
        "AWS Landing Zone with aerospace templates, AWS Directory Service for engineer authentication, Amazon ElastiCache for design caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "Organizations with ITAR classification OUs ensure proper export control separation for military and commercial aerospace programs. Lake Formation provides design data governance and export control through fine-grained access controls for proprietary aerospace innovations. PrivateLink enables secure supplier collaboration without internet exposure for international programs. GovCloud provides ITAR-compliant infrastructure for military aerospace projects. CloudTrail with tamper detection ensures compliance audit trails for export control and aerospace innovation protection.",
        whyWrong: {
            0: "Program-based OUs don't address ITAR classification requirements for export control compliance",
            2: "Control Tower doesn't address ITAR classification requirements, basic S3 doesn't provide export control governance",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the export control governance needed for aerospace design"
        },
        examStrategy: "Aerospace governance: Organizations + ITAR OUs + Lake Formation + PrivateLink + GovCloud + CloudTrail tamper detection for export control."
    }
},
{
    id: 'sap_450',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global biotechnology research platform enables 15,000+ scientists to collaborate on genetic engineering, protein design, and synthetic biology projects, requires massive computational resources for molecular simulations, manages sensitive research data, and enables secure international collaboration while protecting intellectual property and ensuring biosafety compliance across regulatory jurisdictions for breakthrough biotechnology discoveries.",
    question: "Which architecture provides OPTIMAL biotechnology research with comprehensive IP protection and biosafety compliance for global collaboration?",
    options: [
        "Amazon EC2 with GPU instances for molecular modeling, Amazon S3 for research data, AWS Lambda for genetic analysis, Amazon DynamoDB for research records, and AWS IAM for access control",
        "Amazon SageMaker for AI-driven protein design and genetic analysis, AWS ParallelCluster for massive molecular simulations, AWS Lake Formation for IP protection and biosafety compliance governance, Amazon FSx for Lustre for computational data, and AWS PrivateLink for secure international research collaboration",
        "AWS Batch for computational workflows, Amazon EFS for shared research data, AWS Step Functions for research coordination, Amazon DocumentDB for genetic metadata, and AWS VPN for scientist access",
        "Amazon EMR for biotech analytics, Amazon Redshift for research data warehousing, AWS Glue for data processing, Amazon QuickSight for research visualization, and AWS Direct Connect for institutional collaboration"
    ],
    correct: 1,
    explanation: {
        correct: "SageMaker provides AI/ML capabilities for advanced protein design and genetic analysis automation at biotechnology research scale. ParallelCluster delivers massive computational resources for molecular simulations and synthetic biology projects for 15,000+ scientists. Lake Formation ensures IP protection and biosafety compliance governance through fine-grained access controls across regulatory jurisdictions. FSx Lustre provides high-performance storage for computational workloads and genetic data. PrivateLink enables secure international research collaboration while protecting sensitive biotechnology IP and ensuring regulatory compliance.",
        whyWrong: {
            0: "Standard EC2 doesn't provide AI/ML biotechnology capabilities, basic IAM doesn't provide IP protection and biosafety compliance",
            2: "Batch alone doesn't provide AI/ML capabilities for biotechnology research, EFS doesn't provide IP protection for sensitive genetic research",
            3: "EMR doesn't provide AI/ML or HPC capabilities needed for biotechnology research, Redshift doesn't optimize for molecular simulation workflows"
        },
        examStrategy: "Biotechnology research: SageMaker + ParallelCluster + Lake Formation + FSx Lustre + PrivateLink for AI-driven biosafety-compliant global research."
    }
},

// Questions 451-475: correct: 2 (C)
{
    id: 'sap_451',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A global online education platform serves 200 million students across universities and corporations, experiences video streaming issues during peak hours, has content delivery delays, faces assessment processing bottlenecks, and needs real-time collaboration features while optimizing content delivery costs and ensuring accessibility compliance across diverse global educational markets.",
    question: "Which optimization strategy provides the BEST educational platform performance with accessibility and cost efficiency?",
    options: [
        "Implement Amazon CloudFront for content delivery, AWS Lambda for assessment processing, Amazon DynamoDB for student data, Amazon API Gateway for mobile apps, and Amazon S3 for video storage",
        "Deploy AWS Global Accelerator for performance, Amazon EFS for content storage, AWS Batch for video processing, Amazon EventBridge for student coordination, and Amazon ElastiCache for session management",
        "Use Amazon CloudFront with origin shield for video delivery, AWS Elemental MediaPackage for adaptive streaming, Amazon SageMaker for personalized learning recommendations, Amazon DynamoDB for student progress tracking, and Amazon Chime SDK for real-time collaboration",
        "Implement AWS Direct Connect for content delivery, Amazon Glacier for content archival, AWS Lambda for video processing, Amazon SQS for assessment queuing, and Amazon API Gateway for mobile learning apps"
    ],
    correct: 2,
    explanation: {
        correct: "CloudFront with origin shield optimizes video delivery globally and reduces costs for 200M students with accessibility features. MediaPackage provides adaptive streaming for diverse global markets with varying bandwidth and accessibility needs. SageMaker enables personalized learning recommendations for educational effectiveness. DynamoDB scales for student progress tracking with high availability. Chime SDK enables real-time collaboration features for interactive learning and accessibility compliance.",
        whyWrong: {
            0: "Basic CloudFront doesn't provide adaptive streaming or accessibility features needed for global educational markets",
            1: "Global Accelerator doesn't provide video streaming optimization or educational personalization capabilities",
            3: "Direct Connect is expensive for global content delivery to 200M students, Glacier has retrieval delays inappropriate for active learning content"
        },
        examStrategy: "Educational optimization: CloudFront + origin shield + MediaPackage + SageMaker + DynamoDB + Chime SDK for accessible global learning."
    }
},
{
    id: 'sap_452',
    domain: "Domain 4: Accelerate Workload Migration and Modernization",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A national space agency needs to migrate their satellite operations and mission control systems from legacy infrastructure to AWS, supporting multiple simultaneous space missions, real-time telemetry from deep space probes, mission-critical decision making, and international space collaboration while maintaining zero-downtime requirements and the highest security standards for space exploration operations.",
    question: "Which modernization strategy ensures MAXIMUM reliability and security for mission-critical space exploration operations?",
    options: [
        "Deploy Amazon EC2 in multiple regions, implement Amazon RDS Aurora for mission data, use AWS Lambda for telemetry processing, Amazon API Gateway for ground station integration, and AWS CloudFormation for infrastructure automation",
        "Implement AWS Outposts for mission control rooms, Amazon MSK for telemetry streams, AWS Batch for mission calculations, Amazon ElastiCache for performance, and AWS Direct Connect for ground station connectivity",
        "Use AWS Ground Station for satellite communication, Amazon Kinesis Data Streams for real-time telemetry, AWS Lambda for mission-critical processing with provisioned concurrency, Amazon DynamoDB Global Tables for mission state, and AWS Auto Scaling with multi-region failover for zero-downtime requirements",
        "Deploy AWS Wavelength for low-latency processing, Amazon EventBridge for mission coordination, Amazon DocumentDB for spacecraft data, Amazon Redshift for mission analytics, and AWS Transit Gateway for ground station networking"
    ],
    correct: 2,
    explanation: {
        correct: "AWS Ground Station provides managed satellite communication infrastructure for deep space missions and international collaboration. Kinesis Data Streams handles real-time telemetry from multiple spacecraft with automatic scaling for space exploration operations. Lambda with provisioned concurrency ensures mission-critical processing without cold starts for zero-downtime requirements. DynamoDB Global Tables provides mission state availability across regions for international space collaboration. Auto Scaling with multi-region failover ensures operational continuity for space missions.",
        whyWrong: {
            0: "Standard EC2 deployment doesn't provide satellite communication capabilities needed for space operations",
            1: "Outposts requires on-premises infrastructure management inappropriate for mission-critical space operations",
            3: "Wavelength is for mobile edge computing not space missions, EventBridge doesn't provide mission-critical reliability needed for space exploration"
        },
        examStrategy: "Space exploration: Ground Station + Kinesis + Lambda provisioned + DynamoDB Global Tables + Auto Scaling for mission-critical space operations."
    }
},
{
    id: 'sap_453',
    domain: "Domain 1: Design Solutions for Organizational Complexity",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global consulting firm manages 15,000+ client projects across 120+ countries with strict confidentiality requirements, implements Chinese walls between competing clients, enables secure collaboration on joint ventures, maintains audit trails for regulatory compliance, and supports a distributed workforce while protecting client intellectual property and ensuring compliance with varying international business regulations.",
    question: "Which governance framework ensures COMPREHENSIVE client confidentiality with global regulatory compliance and operational flexibility?",
    options: [
        "AWS Organizations with client-based OUs, AWS Config for confidentiality compliance, AWS PrivateLink for secure communication, AWS CloudTrail for audit trails, and AWS WorkSpaces for remote collaboration",
        "AWS Control Tower for consulting standardization, AWS Security Hub for compliance monitoring, Amazon S3 for client data storage, AWS Lambda for data processing, and AWS Direct Connect for office connectivity",
        "AWS Organizations with jurisdiction-specific OUs for regulatory compliance, AWS Lake Formation for Chinese wall enforcement and client IP protection, AWS PrivateLink for secure project collaboration, AWS CloudTrail with legal hold for audit compliance, and AWS WorkSpaces with DLP for secure distributed workforce access",
        "AWS Landing Zone with consulting templates, AWS Directory Service for consultant authentication, Amazon ElastiCache for client caching, AWS Certificate Manager for secure communications, and Amazon WorkDocs for collaboration"
    ],
    correct: 2,
    explanation: {
        correct: "Organizations with jurisdiction OUs ensure compliance with 120+ different confidentiality and business regulations globally. Lake Formation enforces Chinese walls and protects client IP through fine-grained access controls for competing client protection. PrivateLink enables secure project collaboration without internet exposure for joint ventures. CloudTrail with legal hold provides compliant audit trails for regulatory requirements. WorkSpaces with DLP ensures secure distributed workforce access with data loss prevention for client confidentiality across 15,000+ projects.",
        whyWrong: {
            0: "Client-based OUs don't address jurisdiction-specific regulations for global consulting operations",
            1: "Control Tower doesn't address jurisdiction-specific confidentiality requirements, basic S3 doesn't provide Chinese wall controls",
            3: "Landing Zone is deprecated, Directory Service doesn't provide the fine-grained access controls needed for client confidentiality at global scale"
        },
        examStrategy: "Global consulting: Organizations + jurisdiction OUs + Lake Formation + PrivateLink + CloudTrail legal hold + WorkSpaces DLP for comprehensive client protection."
    }
},
{
    id: 'sap_454',
    domain: "Domain 2: Design for New Solutions",
    difficulty: "medium",
    timeRecommendation: 150,
    scenario: "A smart transportation system coordinates 200,000+ connected vehicles, 50,000+ traffic signals, and public transit across multiple metropolitan areas, optimizes traffic flow, manages emergency vehicle routing, provides real-time passenger information, and reduces carbon emissions through intelligent routing while ensuring transportation safety and accessibility for all citizens.",
    question: "Which architecture provides OPTIMAL smart transportation coordination with sustainability and accessibility focus?",
    options: [
        "AWS IoT Core for vehicle and infrastructure connectivity, Amazon MSK for high-volume transportation data, Amazon EMR for traffic analytics, AWS Lambda for routing optimization, and Amazon S3 for transportation data storage",
        "Amazon EventBridge for transportation events, AWS Batch for route calculations, Amazon DocumentDB for vehicle data, Amazon API Gateway for passenger information, and Amazon CloudWatch for monitoring",
        "Amazon Kinesis Data Streams for real-time vehicle and signal data, Amazon Kinesis Data Analytics for traffic optimization, Amazon SageMaker for route prediction and carbon emission reduction, AWS Lambda for emergency vehicle coordination, and Amazon Timestream for transportation metrics and accessibility analytics",
        "AWS IoT Greengrass for vehicle edge processing, Amazon DynamoDB for transportation records, AWS Step Functions for traffic coordination, Amazon Redshift for analytics, and Amazon QuickSight for transportation dashboards"
    ],
    correct: 2,
    explanation: {
        correct: "Kinesis Data Streams efficiently handles data from 200,000+ vehicles and 50,000+ signals across metropolitan areas. Kinesis Data Analytics provides real-time traffic optimization for safety and accessibility. SageMaker enables intelligent routing for carbon emission reduction and accessibility route planning for all citizens. Lambda coordinates emergency vehicle routing with automatic scaling for safety. Timestream optimizes transportation metrics and accessibility analytics for inclusive transportation systems.",
        whyWrong: {
            0: "MSK adds complexity for transportation data at massive scale, EMR is for batch analytics not real-time traffic optimization",
            1: "EventBridge and Batch add latency inappropriate for real-time traffic management and emergency routing",
            3: "IoT Greengrass on every vehicle adds operational complexity, Step Functions add latency for real-time traffic coordination at metropolitan scale"
        },
        examStrategy: "Smart transportation: Kinesis ecosystem + SageMaker + Lambda + Timestream for sustainable accessible urban mobility."
    }
},
{
    id: 'sap_455',
    domain: "Domain 3: Continuous Improvement for Existing Solutions",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global cryptocurrency exchange processes 10 million transactions per second, experiences order execution delays during market volatility, has liquidity management challenges across trading pairs, faces regulatory compliance requirements in 70+ jurisdictions, and needs real-time risk management while maintaining sub-millisecond latency for high-frequency traders across global crypto markets.",
    question: "Which optimization strategy provides the BEST cryptocurrency exchange performance with comprehensive compliance and risk management?",
    options: [
        "Implement Amazon ElastiCache for order book caching, AWS Lambda for trade processing, Amazon DynamoDB for transaction data, Amazon API Gateway for trading APIs, and Amazon CloudWatch for monitoring",
        "Deploy Amazon MemoryDB for trading state, Amazon MSK for high-throughput messaging, AWS Step Functions for trading workflows, Amazon Aurora Global for transaction history, and AWS Security Hub for compliance dashboards",
        "Deploy Amazon ElastiCache Redis cluster mode for order books, AWS Lambda with provisioned concurrency for sub-millisecond execution, Amazon DynamoDB Global Tables for transaction records, Amazon Kinesis for real-time market data, and AWS Config for multi-jurisdiction compliance monitoring",
        "Use Amazon DocumentDB for flexible trading schemas, Amazon EventBridge for market events, AWS Batch for liquidity calculations, Amazon RDS for transaction storage, and AWS CloudTrail for regulatory auditing"
    ],
    correct: 2,
    explanation: {
        correct: "ElastiCache Redis cluster mode provides sub-millisecond order book access for 10M TPS with automatic sharding across global crypto markets. Lambda with provisioned concurrency eliminates cold starts for consistent sub-millisecond execution during market volatility. DynamoDB Global Tables handles global transaction records with automatic scaling and conflict resolution. Kinesis processes real-time market data efficiently for high-frequency trading. Config monitors compliance across 70+ jurisdictions continuously for comprehensive regulatory adherence.",
        whyWrong: {
            0: "Basic caching doesn't provide clustering needed for 10M TPS at global crypto scale, CloudWatch doesn't provide compliance monitoring",
            1: "MemoryDB has higher latency than ElastiCache cluster mode for sub-millisecond crypto trading requirements",
            3: "DocumentDB doesn't optimize for crypto trading performance at global scale, EventBridge adds latency for real-time order execution"
        },
        examStrategy: "Crypto exchange optimization: ElastiCache cluster + Lambda provisioned + DynamoDB Global Tables + Kinesis + Config for global crypto trading."
    }
},


{
      id: 'sap_456',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 150,
      scenario: "A national education system needs to migrate their student information and distance learning systems from legacy infrastructure to AWS, supporting 100 million students, ensuring data privacy, enabling remote learning capabilities, and maintaining high availability for critical educational services while meeting government security requirements and improving educational outcomes through personalized learning.",
      question: "Which modernization approach provides OPTIMAL educational services with comprehensive privacy protection and personalized learning capabilities?",
      options: [
        "Deploy Amazon EC2 for education applications, Amazon RDS for student data, AWS Lambda for learning processing, Amazon API Gateway for student services, and AWS CloudTrail for privacy audit compliance",
        "Implement AWS Elastic Beanstalk for learning applications, Amazon Aurora for student database, AWS Step Functions for educational workflows, Amazon S3 for learning content, and AWS Config for compliance monitoring",
        "Deploy AWS Control Tower in GovCloud for government compliance, Amazon DynamoDB for student records, Amazon SageMaker for personalized learning analytics, Amazon Chime SDK for remote learning, and AWS Shield Advanced for DDoS protection",
        "Use Amazon EKS for containerized education applications, Amazon DocumentDB for student data, Amazon Personalize for learning recommendations, Amazon Connect for student support, and AWS Security Hub for compliance monitoring"
      ],
      correct: 2,
      explanation: {
        correct: "AWS Control Tower in GovCloud ensures government security requirements are met for 100M students. DynamoDB provides scalable NoSQL storage for massive student records. SageMaker enables AI-powered personalized learning analytics improving educational outcomes. Chime SDK provides native remote learning capabilities with high availability. Shield Advanced protects critical educational infrastructure from DDoS attacks during peak learning periods.",
        whyWrong: {
          0: "Basic EC2 and RDS don't provide government-grade security compliance, CloudTrail alone insufficient for comprehensive privacy protection",
          1: "Elastic Beanstalk doesn't meet government security requirements, lacks AI-powered personalization capabilities",
          3: "EKS adds complexity without government compliance benefits, DocumentDB not optimized for student record scale"
        },
        examStrategy: "Government education systems: Control Tower GovCloud + DynamoDB + SageMaker + Chime SDK + Shield Advanced for secure, scalable, personalized learning."
      }
    },

    {
      id: 'sap_457',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 180,
      scenario: "A multinational pharmaceutical company with drug research facilities in 25 countries needs to migrate their molecular simulation workloads, clinical trial data management, and drug discovery pipelines to AWS. The migration must support HPC workloads with up to 100,000 cores, ensure HIPAA and GxP compliance, enable real-time collaboration between research teams, and reduce drug discovery timelines from 10 years to 6 years through advanced AI/ML capabilities.",
      question: "Which comprehensive migration strategy provides OPTIMAL drug discovery acceleration with full regulatory compliance and global collaboration capabilities?",
      options: [
        "Deploy Amazon EC2 with placement groups for HPC, Amazon FSx for Lustre for high-performance storage, AWS Batch for job scheduling, Amazon S3 with Glacier for clinical data archival, and AWS CloudFormation for infrastructure automation",
        "Implement AWS ParallelCluster for HPC workloads, Amazon EFS for shared file systems, AWS Step Functions for workflow orchestration, Amazon RDS for clinical databases, and AWS Control Tower for multi-account governance",
        "Deploy AWS Batch with mixed instance types, Amazon S3 for data lakes, AWS Glue for ETL processing, Amazon SageMaker for ML model training, and AWS Config for compliance monitoring",
        "Use AWS ParallelCluster with Spot Instances for cost-optimized HPC, Amazon FSx for Lustre with data compression, AWS HealthLake for clinical data standardization, Amazon SageMaker with distributed training for drug discovery AI, AWS Transit Gateway for global connectivity, and AWS Artifact for compliance documentation"
      ],
      correct: 3,
      explanation: {
        correct: "ParallelCluster with Spot Instances provides massive 100K-core HPC capability cost-effectively for molecular simulations. FSx for Lustre offers high-performance computing storage with compression. HealthLake ensures HIPAA/GxP compliance for clinical data across 25 countries. SageMaker distributed training accelerates AI-driven drug discovery reducing timelines from 10 to 6 years. Transit Gateway enables global research collaboration. Artifact provides compliance documentation for regulatory requirements.",
        whyWrong: {
          0: "Basic EC2 placement groups don't provide HPC-optimized scaling for 100K cores, Glacier too slow for active research",
          1: "EFS doesn't provide HPC-optimized performance needed for molecular simulations, RDS not specialized for clinical compliance",
          2: "AWS Batch alone insufficient for 100K-core HPC requirements, doesn't address specialized clinical compliance needs"
        },
        examStrategy: "Pharma HPC migration: ParallelCluster Spot + FSx Lustre + HealthLake + SageMaker distributed + Transit Gateway + Artifact for compliant drug discovery."
      }
    },

    {
      id: 'sap_458',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 120,
      scenario: "A global streaming media company needs to migrate their content delivery and recommendation engine from on-premises infrastructure to AWS. They serve 500 million users worldwide, store 50 petabytes of video content, and need to reduce content delivery latency to under 10ms while improving recommendation accuracy through real-time user behavior analysis.",
      question: "Which migration approach provides OPTIMAL global content delivery with real-time personalization capabilities?",
      options: [
        "Use Amazon CloudFront with multiple origin servers, Amazon S3 for content storage, AWS Elemental MediaConvert for video processing, and Amazon Personalize for recommendations",
        "Deploy Amazon CloudFront with Lambda@Edge for content optimization, Amazon S3 with Transfer Acceleration for content upload, AWS Elemental MediaPackage for adaptive streaming, Amazon Kinesis for real-time analytics, and Amazon Personalize with real-time inference for dynamic recommendations",
        "Implement Amazon EC2 with Auto Scaling for video serving, Amazon EBS for content storage, AWS Lambda for recommendation processing, and Amazon ElastiCache for caching",
        "Use AWS Global Accelerator for content delivery, Amazon EFS for video storage, AWS Batch for video processing, and Amazon SageMaker for recommendation models"
      ],
      correct: 1,
      explanation: {
        correct: "CloudFront with Lambda@Edge provides sub-10ms global content delivery with edge optimization for 500M users. S3 Transfer Acceleration handles massive 50PB content uploads efficiently. MediaPackage enables adaptive streaming for optimal user experience. Kinesis processes real-time user behavior data for accurate recommendations. Personalize with real-time inference delivers dynamic recommendations based on current user activity.",
        whyWrong: {
          0: "Basic CloudFront without edge optimization can't achieve sub-10ms latency, lacks real-time behavior analysis",
          2: "EC2 with EBS doesn't provide global edge distribution needed for sub-10ms latency to 500M users",
          3: "Global Accelerator alone insufficient for content delivery optimization, EFS not suitable for 50PB video storage"
        },
        examStrategy: "Global streaming migration: CloudFront Lambda@Edge + S3 Transfer Acceleration + MediaPackage + Kinesis + Personalize real-time for sub-10ms delivery."
      }
    },

    {
      id: 'sap_459',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 200,
      scenario: "A multinational bank with operations in 40 countries needs to migrate their core banking system, fraud detection, and risk management platforms to AWS while maintaining 99.999% availability, ensuring PCI DSS and regional banking regulation compliance, supporting real-time transaction processing for 200 million customers, and implementing advanced AI for fraud prevention.",
      question: "Which comprehensive modernization strategy provides OPTIMAL banking operations with maximum security, compliance, and AI-driven fraud protection?",
      options: [
        "Deploy Amazon EC2 in multiple AZs, Amazon RDS with Multi-AZ for banking data, AWS Lambda for transaction processing, Amazon CloudWatch for monitoring, and AWS Config for compliance",
        "Implement Amazon EKS for microservices architecture, Amazon Aurora Global Database for distributed banking data, AWS Step Functions for transaction workflows, Amazon GuardDuty for threat detection, and AWS Security Hub for compliance management",
        "Use AWS Control Tower for multi-account governance, Amazon Aurora Serverless for elastic banking database, AWS AppSync for real-time APIs, Amazon Macie for data discovery, and AWS CloudTrail for audit logging",
        "Deploy AWS Control Tower with AWS Organizations for regulatory compliance across regions, Amazon Aurora Global Database with cross-region replication, Amazon Kinesis Data Streams for real-time transaction processing, Amazon Fraud Detector with custom ML models, AWS PrivateLink for secure connectivity, AWS CloudHSM for cryptographic operations, and AWS Artifact for compliance documentation"
      ],
      correct: 3,
      explanation: {
        correct: "Control Tower with Organizations ensures compliance across 40 countries' banking regulations. Aurora Global Database provides 99.999% availability with cross-region replication for 200M customers. Kinesis processes real-time transactions at banking scale. Fraud Detector provides AI-powered fraud prevention. PrivateLink ensures secure banking connectivity. CloudHSM provides hardware-based cryptographic security for PCI DSS compliance. Artifact manages regulatory compliance documentation.",
        whyWrong: {
          0: "Basic EC2/RDS Multi-AZ insufficient for 99.999% availability across 40 countries, lacks AI fraud detection",
          1: "EKS adds complexity without banking compliance benefits, GuardDuty alone insufficient for advanced fraud detection",
          2: "Aurora Serverless may have cold start issues for real-time banking, AppSync not optimized for banking transaction scale"
        },
        examStrategy: "Multinational banking migration: Control Tower Organizations + Aurora Global + Kinesis + Fraud Detector + PrivateLink + CloudHSM + Artifact for compliant banking."
      }
    },

    {
      id: 'sap_460',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 130,
      scenario: "A global logistics company needs to migrate their supply chain management, real-time tracking, and route optimization systems to AWS. They manage 10 million shipments daily across 60 countries, need real-time visibility into cargo location and condition, and want to optimize delivery routes using machine learning to reduce delivery times by 30%.",
      question: "Which migration approach provides OPTIMAL supply chain visibility with AI-powered route optimization?",
      options: [
        "Use Amazon EC2 for tracking applications, Amazon DynamoDB for shipment data, AWS Lambda for route calculations, and Amazon S3 for logistics data storage",
        "Deploy Amazon EKS for containerized logistics apps, Amazon Aurora for shipment database, AWS Step Functions for supply chain workflows, and Amazon SageMaker for route optimization",
        "Implement AWS IoT Core for real-time tracking, Amazon Timestream for time-series shipment data, Amazon Location Service for geospatial processing, Amazon SageMaker with built-in algorithms for route optimization, AWS Lambda for event processing, and Amazon QuickSight for supply chain analytics",
        "Use Amazon API Gateway for logistics APIs, Amazon RDS for shipment records, AWS Batch for route processing, and Amazon CloudWatch for monitoring"
      ],
      correct: 2,
      explanation: {
        correct: "IoT Core manages real-time tracking of 10M daily shipments across 60 countries. Timestream efficiently stores time-series location and condition data. Location Service provides geospatial processing for global operations. SageMaker optimization algorithms reduce delivery times by 30%. Lambda processes tracking events in real-time. QuickSight provides comprehensive supply chain visibility and analytics.",
        whyWrong: {
          0: "Basic EC2 doesn't provide IoT-optimized real-time tracking capabilities, lacks geospatial optimization",
          1: "EKS adds complexity without logistics-specific benefits, Aurora not optimized for time-series tracking data",
          3: "API Gateway alone insufficient for real-time IoT tracking, Batch not suitable for real-time route optimization"
        },
        examStrategy: "Logistics migration: IoT Core + Timestream + Location Service + SageMaker + Lambda + QuickSight for real-time optimized supply chain."
      }
    },

    {
      id: 'sap_461',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 190,
      scenario: "A multinational energy company with oil rigs, wind farms, and solar installations across 35 countries needs to migrate their industrial IoT monitoring, predictive maintenance, and energy trading systems to AWS. The solution must process sensor data from 500,000 devices, predict equipment failures 30 days in advance, ensure 99.99% uptime for trading systems, and optimize energy production through AI-driven forecasting.",
      question: "Which comprehensive modernization approach provides OPTIMAL industrial IoT operations with predictive analytics and energy trading capabilities?",
      options: [
        "Deploy Amazon EC2 for IoT processing, Amazon S3 for sensor data storage, AWS Lambda for analytics, Amazon RDS for equipment data, and Amazon CloudWatch for monitoring",
        "Use AWS IoT Core for device connectivity, Amazon Kinesis for data streaming, Amazon SageMaker for predictive models, Amazon DynamoDB for device data, and AWS Step Functions for maintenance workflows",
        "Implement Amazon EKS for containerized applications, Amazon Aurora for energy data, AWS Glue for ETL processing, Amazon Redshift for analytics, and AWS Batch for batch processing",
        "Deploy AWS IoT Greengrass for edge computing at remote sites, AWS IoT Core for centralized device management, Amazon Kinesis Data Firehose for high-throughput data ingestion, Amazon Timestream for time-series sensor data, Amazon Forecast for energy production prediction, Amazon SageMaker with AutoML for predictive maintenance, AWS Direct Connect for low-latency trading connectivity, and AWS Global Accelerator for worldwide energy trading performance"
      ],
      correct: 3,
      explanation: {
        correct: "IoT Greengrass enables edge processing at remote oil rigs and wind farms. IoT Core manages 500K devices centrally. Kinesis Firehose handles massive sensor data ingestion. Timestream stores time-series data efficiently. Forecast optimizes energy production. SageMaker AutoML predicts equipment failures 30 days ahead. Direct Connect ensures low-latency trading with 99.99% uptime. Global Accelerator optimizes energy trading performance across 35 countries.",
        whyWrong: {
          0: "Basic EC2 doesn't provide IoT-optimized edge processing for remote sites, lacks predictive analytics capabilities",
          1: "Missing edge computing for remote energy sites, Step Functions not optimized for predictive maintenance workflows",
          2: "EKS adds complexity without energy industry benefits, doesn't address IoT-specific requirements"
        },
        examStrategy: "Energy IoT migration: IoT Greengrass + IoT Core + Kinesis Firehose + Timestream + Forecast + SageMaker AutoML + Direct Connect + Global Accelerator."
      }
    },

    {
      id: 'sap_462',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 140,
      scenario: "A global e-commerce platform serving 300 million customers needs to migrate their product catalog, recommendation engine, and order processing systems to AWS. They process 50 million orders daily, need to personalize shopping experiences in real-time, and want to reduce infrastructure costs by 40% while maintaining sub-second response times.",
      question: "Which migration strategy provides OPTIMAL e-commerce performance with cost reduction and personalization capabilities?",
      options: [
        "Use Amazon EKS for microservices, Amazon Aurora Serverless for elastic database scaling, Amazon ElastiCache for sub-second caching, Amazon Personalize for real-time recommendations, AWS Lambda for serverless order processing, and Amazon CloudFront for global content delivery",
        "Deploy Amazon EC2 with Auto Scaling for web applications, Amazon RDS for product data, AWS Lambda for recommendation processing, Amazon S3 for static content, and Amazon CloudWatch for monitoring",
        "Implement AWS Elastic Beanstalk for application deployment, Amazon DynamoDB for product catalog, AWS Step Functions for order workflows, Amazon SageMaker for recommendations, and AWS CloudFormation for infrastructure automation",
        "Use Amazon API Gateway for e-commerce APIs, Amazon Aurora for order database, AWS Batch for recommendation processing, Amazon EFS for shared storage, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "EKS provides scalable microservices architecture for 300M customers. Aurora Serverless automatically scales database capacity reducing costs by 40%. ElastiCache ensures sub-second response times. Personalize delivers real-time shopping personalization. Lambda processes 50M daily orders cost-effectively with serverless architecture. CloudFront provides global performance optimization.",
        whyWrong: {
          1: "Basic EC2 Auto Scaling doesn't provide cost optimization compared to serverless, lacks real-time personalization",
          2: "Elastic Beanstalk doesn't offer same cost optimization as serverless architecture, Step Functions adds latency",
          3: "Batch processing not suitable for real-time recommendations, EFS unnecessary overhead for e-commerce"
        },
        examStrategy: "E-commerce migration: EKS + Aurora Serverless + ElastiCache + Personalize + Lambda + CloudFront for cost-effective personalized shopping."
      }
    },

    {
      id: 'sap_463',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 180,
      scenario: "A multinational healthcare network with 500 hospitals across 25 countries needs to migrate their electronic health records (EHR), medical imaging, and clinical decision support systems to AWS. The solution must ensure HIPAA compliance, support 50 petabytes of medical imaging data, enable real-time clinical analytics, and improve patient outcomes through AI-powered diagnostic assistance.",
      question: "Which comprehensive modernization approach provides OPTIMAL healthcare operations with AI-driven clinical insights and regulatory compliance?",
      options: [
        "Deploy Amazon EC2 for EHR applications, Amazon S3 for medical images, AWS Lambda for clinical processing, Amazon RDS for patient data, and AWS CloudTrail for HIPAA compliance",
        "Use Amazon EKS for containerized healthcare apps, Amazon Aurora for EHR database, AWS Step Functions for clinical workflows, Amazon SageMaker for diagnostic AI, and AWS Config for compliance monitoring",
        "Implement AWS HealthLake for clinical data standardization, Amazon S3 with Intelligent Tiering for medical imaging storage, Amazon HealthImaging for DICOM image management, Amazon Comprehend Medical for clinical text analysis, Amazon SageMaker with healthcare-specific models for diagnostic assistance, AWS PrivateLink for secure connectivity, and AWS Artifact for HIPAA compliance documentation",
        "Deploy Amazon API Gateway for healthcare APIs, Amazon DynamoDB for patient records, AWS Glue for clinical data processing, Amazon Redshift for analytics, and AWS Security Hub for compliance management"
      ],
      correct: 2,
      explanation: {
        correct: "HealthLake standardizes EHR data across 500 hospitals ensuring HIPAA compliance. S3 Intelligent Tiering cost-effectively manages 50PB of medical imaging. HealthImaging provides specialized DICOM image management. Comprehend Medical extracts insights from clinical text. SageMaker healthcare models improve diagnostic accuracy and patient outcomes. PrivateLink ensures secure HIPAA-compliant connectivity. Artifact manages regulatory compliance documentation across 25 countries.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide healthcare-specific compliance and AI capabilities, CloudTrail alone insufficient for HIPAA",
          1: "EKS adds complexity without healthcare benefits, doesn't address specialized medical imaging and compliance needs",
          3: "DynamoDB not optimal for complex clinical data relationships, doesn't provide healthcare-specific AI capabilities"
        },
        examStrategy: "Healthcare migration: HealthLake + S3 Intelligent Tiering + HealthImaging + Comprehend Medical + SageMaker + PrivateLink + Artifact for compliant healthcare."
      }
    },

    {
      id: 'sap_464',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 125,
      scenario: "A global telecommunications company needs to migrate their network monitoring, customer service, and billing systems to AWS. They serve 200 million subscribers, process 10 billion call records daily, need real-time network performance monitoring, and want to implement AI-powered customer service to reduce support costs by 50%.",
      question: "Which migration approach provides OPTIMAL telecom operations with AI-enhanced customer service and real-time monitoring?",
      options: [
        "Deploy Amazon Connect for customer service automation, Amazon Kinesis for real-time call record processing, Amazon Timestream for network performance metrics, Amazon Lex for conversational AI, AWS Lambda for billing calculations, and Amazon QuickSight for operational dashboards",
        "Use Amazon EC2 for telecom applications, Amazon RDS for subscriber data, AWS Lambda for call processing, Amazon S3 for call records, and Amazon CloudWatch for network monitoring",
        "Implement Amazon EKS for microservices, Amazon Aurora for billing database, AWS Step Functions for customer workflows, Amazon SageMaker for AI models, and AWS Batch for data processing",
        "Deploy Amazon API Gateway for telecom APIs, Amazon DynamoDB for call records, AWS Glue for ETL processing, Amazon Redshift for analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "Connect provides AI-powered customer service automation reducing support costs by 50%. Kinesis processes 10B daily call records in real-time. Timestream efficiently stores network performance time-series data. Lex enables intelligent conversational customer interactions. Lambda handles elastic billing calculations for 200M subscribers. QuickSight provides comprehensive operational visibility.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide AI-powered customer service automation, CloudWatch insufficient for telecom-scale monitoring",
          2: "EKS adds complexity without telecom-specific benefits, doesn't address AI customer service requirements",
          3: "API Gateway alone insufficient for customer service automation, doesn't provide AI conversational capabilities"
        },
        examStrategy: "Telecom migration: Connect + Kinesis + Timestream + Lex + Lambda + QuickSight for AI-powered telecom operations."
      }
    },

    {
      id: 'sap_465',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 200,
      scenario: "A multinational aerospace and defense contractor needs to migrate their satellite communication systems, radar data processing, and mission-critical command and control applications to AWS. The solution must meet FedRAMP High requirements, process petabytes of radar data daily, ensure 99.999% availability for mission operations, and enable real-time global communications with minimal latency.",
      question: "Which comprehensive modernization strategy provides OPTIMAL aerospace operations with maximum security and mission-critical reliability?",
      options: [
        "Deploy Amazon EC2 in multiple AZs, Amazon S3 for radar data, AWS Lambda for data processing, Amazon CloudWatch for monitoring, and AWS Config for FedRAMP compliance",
        "Use Amazon EKS for containerized applications, Amazon Aurora for operational data, AWS Step Functions for mission workflows, Amazon GuardDuty for threat detection, and AWS Security Hub for compliance management",
        "Implement AWS GovCloud with FedRAMP High certification, AWS Ground Station for satellite communications, Amazon S3 with Cross-Region Replication for radar data resilience, Amazon Kinesis Data Analytics for real-time radar processing, AWS Direct Connect with dedicated circuits for mission-critical connectivity, Amazon EC2 with dedicated tenancy for command systems, and AWS CloudHSM for cryptographic operations",
        "Deploy AWS Control Tower for governance, Amazon Redshift for radar analytics, AWS Batch for data processing, Amazon API Gateway for communications, and AWS Artifact for compliance documentation"
      ],
      correct: 2,
      explanation: {
        correct: "GovCloud with FedRAMP High meets defense contractor security requirements. Ground Station provides native satellite communication capabilities. S3 Cross-Region Replication ensures radar data resilience for 99.999% availability. Kinesis Analytics processes petabytes of radar data in real-time. Direct Connect with dedicated circuits ensures minimal latency for global communications. EC2 dedicated tenancy provides isolated command systems. CloudHSM ensures hardware-based cryptographic security for mission-critical operations.",
        whyWrong: {
          0: "Basic EC2 and S3 don't meet FedRAMP High requirements, Config alone insufficient for defense compliance",
          1: "EKS doesn't provide defense-grade isolation, lacks satellite communication capabilities",
          3: "Control Tower standard doesn't meet FedRAMP High requirements, Redshift not optimized for real-time radar processing"
        },
        examStrategy: "Aerospace migration: GovCloud FedRAMP High + Ground Station + S3 Cross-Region + Kinesis Analytics + Direct Connect + EC2 dedicated + CloudHSM for mission-critical aerospace."
      }
    },

    {
      id: 'sap_466',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 135,
      scenario: "A global social media platform with 2 billion users needs to migrate their content moderation, real-time messaging, and social graph systems to AWS. They process 100 million posts daily, need to detect harmful content within seconds, support real-time messaging for active users, and maintain user engagement through personalized content feeds.",
      question: "Which migration strategy provides OPTIMAL social media operations with real-time content moderation and personalization?",
      options: [
        "Use Amazon Rekognition for content moderation, Amazon ElastiCache for real-time messaging, Amazon Personalize for content recommendations, Amazon Kinesis for post processing, AWS Lambda for social graph updates, and Amazon CloudFront for global content delivery",
        "Deploy Amazon EC2 for social applications, Amazon DynamoDB for user data, AWS Lambda for content processing, Amazon S3 for media storage, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for microservices, Amazon Aurora for social database, AWS Step Functions for content workflows, Amazon SageMaker for recommendation models, and AWS Batch for batch processing",
        "Use Amazon API Gateway for social APIs, Amazon RDS for user profiles, AWS Glue for data processing, Amazon Redshift for analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "Rekognition automatically detects harmful content in 100M daily posts within seconds. ElastiCache provides sub-millisecond real-time messaging for 2B users. Personalize delivers personalized content feeds to maintain engagement. Kinesis processes massive post volumes in real-time. Lambda handles dynamic social graph updates elastically. CloudFront ensures global content delivery performance.",
        whyWrong: {
          1: "Basic EC2/DynamoDB doesn't provide AI-powered content moderation, lacks real-time messaging optimization",
          2: "EKS adds complexity without social media benefits, Step Functions adds latency for real-time operations",
          3: "RDS not optimized for social graph scale, Glue not suitable for real-time content processing"
        },
        examStrategy: "Social media migration: Rekognition + ElastiCache + Personalize + Kinesis + Lambda + CloudFront for real-time social platform."
      }
    },

    {
      id: 'sap_467',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 185,
      scenario: "A multinational automotive manufacturer with production facilities in 30 countries needs to migrate their manufacturing execution systems (MES), quality control, and supply chain coordination to AWS. The solution must support real-time production line monitoring, predict quality defects before they occur, coordinate just-in-time delivery across global suppliers, and meet automotive industry security standards (ISO 27001).",
      question: "Which comprehensive modernization approach provides OPTIMAL manufacturing operations with predictive quality control and global supply chain coordination?",
      options: [
        "Deploy Amazon EC2 for MES applications, Amazon S3 for production data, AWS Lambda for quality processing, Amazon RDS for supplier data, and AWS CloudTrail for audit compliance",
        "Use Amazon EKS for containerized manufacturing apps, Amazon Aurora for production database, AWS Step Functions for manufacturing workflows, Amazon SageMaker for quality prediction, and AWS Config for compliance monitoring",
        "Implement AWS IoT Core for production line connectivity, AWS IoT SiteWise for industrial data modeling, Amazon Timestream for time-series production metrics, Amazon Lookout for Equipment for predictive maintenance, Amazon Forecast for supply chain optimization, AWS PrivateLink for secure supplier connectivity, and AWS Control Tower for multi-account governance across regions",
        "Deploy Amazon API Gateway for manufacturing APIs, Amazon DynamoDB for production records, AWS Glue for ETL processing, Amazon Redshift for analytics, and AWS Security Hub for compliance management"
      ],
      correct: 2,
      explanation: {
        correct: "IoT Core connects manufacturing equipment across 30 countries in real-time. SiteWise models industrial production data effectively. Timestream stores time-series production metrics efficiently. Lookout for Equipment predicts quality defects before occurrence. Forecast optimizes just-in-time supply chain delivery. PrivateLink provides secure ISO 27001-compliant supplier connectivity. Control Tower ensures governance and compliance across global manufacturing operations.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide IoT-optimized manufacturing connectivity, lacks predictive quality capabilities",
          1: "EKS adds complexity without manufacturing benefits, doesn't address IoT and predictive maintenance needs",
          3: "API Gateway alone insufficient for manufacturing IoT connectivity, doesn't provide predictive quality analytics"
        },
        examStrategy: "Manufacturing migration: IoT Core + SiteWise + Timestream + Lookout Equipment + Forecast + PrivateLink + Control Tower for smart manufacturing."
      }
    },

    {
      id: 'sap_468',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 145,
      scenario: "A global insurance company needs to migrate their claims processing, risk assessment, and customer portal systems to AWS. They process 5 million claims annually, need to detect fraudulent claims within minutes, provide 24/7 customer access to policy information, and comply with insurance regulations across 40 countries.",
      question: "Which migration approach provides OPTIMAL insurance operations with fraud detection and regulatory compliance?",
      options: [
        "Deploy AWS Control Tower for regulatory compliance, Amazon Fraud Detector for claims analysis, Amazon Connect for customer service, Amazon S3 for policy documents, AWS Lambda for claims processing, and Amazon QuickSight for claims analytics",
        "Use Amazon EC2 for insurance applications, Amazon RDS for claims data, AWS Lambda for fraud detection, Amazon S3 for document storage, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for microservices, Amazon Aurora for insurance database, AWS Step Functions for claims workflows, Amazon SageMaker for risk models, and AWS Batch for batch processing",
        "Deploy Amazon API Gateway for insurance APIs, Amazon DynamoDB for policy data, AWS Glue for data processing, Amazon Redshift for analytics, and AWS Config for compliance monitoring"
      ],
      correct: 0,
      explanation: {
        correct: "Control Tower ensures compliance with insurance regulations across 40 countries. Fraud Detector uses machine learning to identify fraudulent claims within minutes from 5M annual claims. Connect provides 24/7 customer access to policy information. S3 securely stores policy documents with appropriate retention policies. Lambda processes claims efficiently and cost-effectively. QuickSight provides comprehensive claims and risk analytics for business insights.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide specialized fraud detection capabilities, lacks 24/7 customer service automation",
          2: "EKS adds complexity without insurance benefits, doesn't address fraud detection and customer service needs",
          3: "API Gateway alone insufficient for customer service automation, doesn't provide AI-powered fraud detection"
        },
        examStrategy: "Insurance migration: Control Tower + Fraud Detector + Connect + S3 + Lambda + QuickSight for compliant insurance operations."
      }
    },

    {
      id: 'sap_469',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 190,
      scenario: "A multinational media and entertainment company with content production studios in 20 countries needs to migrate their video production, content distribution, and digital rights management systems to AWS. The solution must support 8K video editing workflows, distribute content to 100 million subscribers globally, ensure content security and DRM protection, and enable real-time collaboration between production teams worldwide.",
      question: "Which comprehensive modernization strategy provides OPTIMAL media production with global distribution and content protection capabilities?",
      options: [
        "Deploy Amazon EC2 for video editing, Amazon S3 for content storage, AWS Elemental MediaConvert for video processing, Amazon CloudFront for distribution, and AWS Lambda for DRM processing",
        "Use Amazon EKS for containerized media applications, Amazon Aurora for metadata database, AWS Step Functions for production workflows, AWS Elemental MediaLive for streaming, and AWS Config for compliance monitoring",
        "Implement Amazon FSx for Lustre for high-performance 8K video editing storage, AWS Thinkbox Deadline for render farm management, Amazon S3 with Cross-Region Replication for content redundancy, AWS Elemental MediaPackage for adaptive streaming, AWS Elemental MediaConnect for secure content transport, Amazon CloudFront with signed URLs for global distribution, AWS Direct Connect for high-bandwidth studio connectivity, and AWS WorkSpaces for remote collaboration",
        "Deploy Amazon API Gateway for media APIs, Amazon DynamoDB for content metadata, AWS Glue for media processing, Amazon Redshift for analytics, and AWS Security Hub for content protection"
      ],
      correct: 2,
      explanation: {
        correct: "FSx for Lustre provides high-performance storage required for 8K video editing workflows. Thinkbox Deadline manages render farms across 20 studios efficiently. S3 Cross-Region Replication ensures content redundancy and availability. MediaPackage provides adaptive streaming for 100M subscribers. MediaConnect securely transports content between studios with DRM protection. CloudFront with signed URLs provides secure global distribution with content protection. Direct Connect ensures high-bandwidth connectivity for 8K content transfer. WorkSpaces enables real-time collaboration between global production teams.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide high-performance 8K editing capabilities, lacks comprehensive content protection",
          1: "EKS adds complexity without media benefits, doesn't address high-performance 8K video editing requirements",
          3: "API Gateway/DynamoDB doesn't provide video-optimized storage and processing, lacks content protection capabilities"
        },
        examStrategy: "Media migration: FSx Lustre + Thinkbox Deadline + S3 Cross-Region + MediaPackage + MediaConnect + CloudFront signed URLs + Direct Connect + WorkSpaces."
      }
    },

    {
      id: 'sap_470',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 140,
      scenario: "A global food delivery platform serving 50 million customers needs to migrate their order management, delivery tracking, and restaurant partner systems to AWS. They process 10 million orders daily, need real-time delivery tracking with 1-minute location updates, dynamic pricing based on demand, and want to optimize delivery routes to reduce delivery times by 25%.",
      question: "Which migration strategy provides OPTIMAL food delivery operations with real-time tracking and route optimization?",
      options: [
        "Use AWS IoT Core for delivery tracking, Amazon Location Service for geospatial processing, Amazon Forecast for demand prediction, AWS Lambda for dynamic pricing calculations, Amazon DynamoDB for order data, and Amazon API Gateway for mobile app integration",
        "Deploy Amazon EC2 for delivery applications, Amazon RDS for order data, AWS Lambda for tracking processing, Amazon S3 for delivery data, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for microservices, Amazon Aurora for order database, AWS Step Functions for delivery workflows, Amazon SageMaker for route optimization, and AWS Batch for batch processing",
        "Use Amazon API Gateway for delivery APIs, Amazon ElastiCache for real-time tracking, AWS Glue for data processing, Amazon Redshift for analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "IoT Core manages real-time tracking with 1-minute location updates for delivery drivers. Location Service optimizes delivery routes reducing delivery times by 25%. Forecast predicts demand patterns for dynamic pricing. Lambda calculates dynamic pricing efficiently for 10M daily orders. DynamoDB handles high-volume order data for 50M customers. API Gateway provides seamless mobile app integration for real-time tracking and ordering.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide IoT-optimized real-time tracking, lacks geospatial route optimization",
          2: "EKS adds complexity without delivery benefits, Step Functions adds latency for real-time operations",
          3: "ElastiCache alone insufficient for delivery tracking, doesn't provide geospatial optimization capabilities"
        },
        examStrategy: "Food delivery migration: IoT Core + Location Service + Forecast + Lambda + DynamoDB + API Gateway for optimized delivery operations."
      }
    },

    {
      id: 'sap_471',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 195,
      scenario: "A multinational smart city initiative spanning 25 metropolitan areas needs to migrate their traffic management, environmental monitoring, and citizen services systems to AWS. The solution must process data from 2 million IoT sensors, predict traffic congestion 2 hours in advance, monitor air quality in real-time, optimize energy consumption across smart buildings, and provide citizens with real-time city services through mobile applications.",
      question: "Which comprehensive modernization approach provides OPTIMAL smart city operations with predictive analytics and citizen engagement capabilities?",
      options: [
        "Deploy Amazon EC2 for city applications, Amazon S3 for sensor data, AWS Lambda for analytics processing, Amazon RDS for city data, and Amazon CloudWatch for monitoring",
        "Use Amazon EKS for containerized city services, Amazon Aurora for city database, AWS Step Functions for smart city workflows, Amazon SageMaker for predictive models, and AWS Batch for data processing",
        "Implement AWS IoT Core for sensor connectivity, AWS IoT Device Management for 2M sensor fleet management, Amazon Timestream for time-series sensor data, Amazon Forecast for traffic prediction, Amazon Lookout for Metrics for anomaly detection in air quality, AWS Lambda for real-time processing, Amazon API Gateway for citizen mobile apps, Amazon Location Service for geospatial analytics, and Amazon QuickSight for city operations dashboards",
        "Deploy Amazon API Gateway for city APIs, Amazon DynamoDB for sensor records, AWS Glue for ETL processing, Amazon Redshift for analytics, and AWS Security Hub for security management"
      ],
      correct: 2,
      explanation: {
        correct: "IoT Core connects 2M sensors across 25 cities. IoT Device Management handles massive sensor fleet operations. Timestream efficiently stores time-series environmental and traffic data. Forecast predicts traffic congestion 2 hours in advance. Lookout for Metrics detects air quality anomalies for real-time monitoring. Lambda processes sensor data in real-time for immediate responses. API Gateway provides citizen mobile app integration for city services. Location Service enables geospatial analytics for traffic and energy optimization. QuickSight provides comprehensive smart city operations dashboards.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide IoT-optimized sensor connectivity, lacks predictive analytics capabilities",
          1: "EKS adds complexity without smart city benefits, doesn't address specialized IoT and predictive requirements",
          3: "API Gateway alone insufficient for IoT sensor connectivity, doesn't provide predictive analytics for traffic and air quality"
        },
        examStrategy: "Smart city migration: IoT Core + IoT Device Management + Timestream + Forecast + Lookout Metrics + Lambda + API Gateway + Location Service + QuickSight."
      }
    },

    {
      id: 'sap_472',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 130,
      scenario: "A global gaming company needs to migrate their multiplayer game servers, player analytics, and in-game purchase systems to AWS. They serve 100 million players worldwide, need sub-50ms latency for competitive gaming, process 1 billion in-game events daily, and want to implement AI-powered anti-cheat detection to maintain fair gameplay.",
      question: "Which migration approach provides OPTIMAL gaming performance with AI-powered cheat detection and global low-latency?",
      options: [
        "Deploy Amazon GameLift for managed game servers, Amazon ElastiCache for sub-50ms player session caching, Amazon Kinesis for real-time event processing, Amazon SageMaker for anti-cheat ML models, AWS Lambda for in-game purchase processing, and Amazon CloudFront for global game asset delivery",
        "Use Amazon EC2 for game servers, Amazon DynamoDB for player data, AWS Lambda for event processing, Amazon S3 for game assets, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized game services, Amazon Aurora for gaming database, AWS Step Functions for game workflows, Amazon Redshift for player analytics, and AWS Batch for batch processing",
        "Deploy Amazon API Gateway for gaming APIs, Amazon RDS for player records, AWS Glue for data processing, Amazon QuickSight for analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "GameLift provides managed, auto-scaling game servers with global deployment for sub-50ms latency to 100M players. ElastiCache ensures ultra-low latency for competitive gaming sessions. Kinesis processes 1B daily in-game events in real-time. SageMaker provides AI-powered anti-cheat detection maintaining fair gameplay. Lambda handles elastic in-game purchase processing. CloudFront delivers game assets globally with minimal latency.",
        whyWrong: {
          1: "Basic EC2 doesn't provide gaming-optimized server management, lacks AI-powered anti-cheat capabilities",
          2: "EKS adds complexity without gaming benefits, Step Functions adds latency for real-time gaming",
          3: "API Gateway/RDS doesn't provide gaming-optimized latency, lacks AI anti-cheat detection"
        },
        examStrategy: "Gaming migration: GameLift + ElastiCache + Kinesis + SageMaker + Lambda + CloudFront for global low-latency gaming with AI anti-cheat."
      }
    },

    {
      id: 'sap_473',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 180,
      scenario: "A multinational research institution with laboratories in 30 countries needs to migrate their scientific computing, data sharing, and collaboration platforms to AWS. The solution must support high-performance computing workloads requiring up to 50,000 cores, enable secure data sharing between researchers globally, ensure research data integrity and compliance with international research standards, and accelerate scientific discovery through AI/ML capabilities.",
      question: "Which comprehensive modernization strategy provides OPTIMAL scientific research operations with global collaboration and AI-accelerated discovery?",
      options: [
        "Deploy Amazon EC2 with cluster placement groups, Amazon S3 for research data, AWS Lambda for data processing, Amazon RDS for research metadata, and AWS CloudTrail for audit compliance",
        "Use Amazon EKS for containerized research applications, Amazon Aurora for research database, AWS Step Functions for research workflows, Amazon SageMaker for ML models, and AWS Config for compliance monitoring",
        "Implement AWS ParallelCluster for HPC workloads, Amazon S3 with Cross-Region Replication for global data sharing, AWS DataSync for secure data transfer, Amazon FSx for Lustre for high-performance shared storage, Amazon SageMaker with Jupyter notebooks for collaborative analysis, AWS PrivateLink for secure inter-institutional connectivity, AWS Control Tower for multi-account research governance, and AWS Artifact for compliance documentation",
        "Deploy Amazon API Gateway for research APIs, Amazon DynamoDB for experiment data, AWS Glue for research data processing, Amazon Redshift for research analytics, and AWS Security Hub for security management"
      ],
      correct: 2,
      explanation: {
        correct: "ParallelCluster provides scalable HPC resources up to 50,000 cores for scientific computing. S3 Cross-Region Replication enables secure global research data sharing across 30 countries. DataSync ensures secure and efficient data transfer between institutions. FSx for Lustre provides high-performance shared storage for collaborative research. SageMaker Jupyter notebooks enable AI-accelerated scientific discovery and global collaboration. PrivateLink ensures secure connectivity maintaining research data integrity. Control Tower provides governance across multi-institutional accounts. Artifact manages compliance with international research standards.",
        whyWrong: {
          0: "Basic EC2 placement groups don't provide HPC-optimized scaling for 50K cores, lacks secure international collaboration",
          1: "EKS adds complexity without HPC benefits, doesn't address specialized scientific computing and collaboration needs",
          3: "API Gateway/DynamoDB doesn't provide HPC capabilities, lacks secure international research collaboration features"
        },
        examStrategy: "Scientific research migration: ParallelCluster + S3 Cross-Region + DataSync + FSx Lustre + SageMaker Jupyter + PrivateLink + Control Tower + Artifact."
      }
    },

    {
      id: 'sap_474',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 150,
      scenario: "A global weather forecasting service needs to migrate their meteorological data processing, forecast modeling, and weather alert systems to AWS. They collect data from 100,000 weather stations worldwide, need to generate accurate forecasts up to 14 days in advance, provide real-time severe weather alerts to 500 million users, and improve forecast accuracy by 20% through machine learning.",
      question: "Which migration strategy provides OPTIMAL weather forecasting with improved accuracy and real-time alerting capabilities?",
      options: [
        "Use AWS IoT Core for weather station connectivity, Amazon Kinesis for real-time data ingestion, Amazon Forecast for weather prediction modeling, Amazon SNS for alert distribution, AWS Lambda for data processing, and Amazon S3 for historical weather data storage",
        "Deploy Amazon EC2 for weather applications, Amazon RDS for meteorological data, AWS Lambda for forecast processing, Amazon S3 for weather data, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized weather services, Amazon Aurora for weather database, AWS Step Functions for forecasting workflows, Amazon SageMaker for weather models, and AWS Batch for batch processing",
        "Use Amazon API Gateway for weather APIs, Amazon DynamoDB for station data, AWS Glue for data processing, Amazon Redshift for analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "IoT Core connects 100,000 weather stations globally for real-time data collection. Kinesis ingests massive volumes of meteorological data in real-time. Forecast provides machine learning-powered weather modeling improving accuracy by 20% for 14-day forecasts. SNS delivers real-time severe weather alerts to 500M users instantly. Lambda processes weather data efficiently and elastically. S3 stores historical weather data for model training and analysis.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide IoT-optimized weather station connectivity, lacks ML-powered forecast accuracy",
          2: "EKS adds complexity without weather benefits, doesn't address specialized meteorological data processing needs",
          3: "API Gateway alone insufficient for weather station connectivity, doesn't provide ML-powered forecasting capabilities"
        },
        examStrategy: "Weather forecasting migration: IoT Core + Kinesis + Forecast + SNS + Lambda + S3 for accurate real-time weather services."
      }
    },

    {
      id: 'sap_475',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 200,
      scenario: "A multinational space agency with missions across 15 countries needs to migrate their satellite control, deep space communications, and mission planning systems to AWS. The solution must ensure 99.999% availability for active missions, process petabytes of space science data, coordinate international space missions, support real-time satellite telemetry, and meet strict space industry security and reliability standards.",
      question: "Which comprehensive modernization approach provides OPTIMAL space mission operations with maximum reliability and international coordination capabilities?",
      options: [
        "Deploy Amazon EC2 in multiple AZs, Amazon S3 for space data, AWS Lambda for telemetry processing, Amazon RDS for mission data, and AWS CloudWatch for monitoring",
        "Use Amazon EKS for containerized space applications, Amazon Aurora for mission database, AWS Step Functions for mission workflows, Amazon SageMaker for space analytics, and AWS Config for compliance monitoring",
        "Implement AWS Control Tower for multi-national governance, AWS Ground Station for satellite communications, Amazon S3 with Cross-Region Replication for mission-critical data redundancy, Amazon Kinesis Data Streams for real-time telemetry processing, AWS Direct Connect with redundant circuits for mission control connectivity, Amazon EC2 with dedicated tenancy for mission-critical systems, AWS CloudHSM for cryptographic operations, and AWS Artifact for regulatory compliance across 15 countries",
        "Deploy Amazon API Gateway for space APIs, Amazon DynamoDB for satellite data, AWS Glue for space data processing, Amazon Redshift for space analytics, and AWS Security Hub for security management"
      ],
      correct: 2,
      explanation: {
        correct: "Control Tower provides governance for international space missions across 15 countries. Ground Station offers native satellite communication capabilities for deep space missions. S3 Cross-Region Replication ensures 99.999% availability for mission-critical data. Kinesis processes real-time satellite telemetry at petabyte scale. Direct Connect with redundancy ensures reliable mission control connectivity. EC2 dedicated tenancy provides isolated systems meeting space industry security standards. CloudHSM ensures hardware-based cryptographic security for sensitive space operations. Artifact manages regulatory compliance across international space agencies.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't meet space industry security and reliability standards, lacks satellite communication capabilities",
          1: "EKS doesn't provide space-grade isolation and security, lacks satellite communication and deep space capabilities",
          3: "API Gateway/DynamoDB doesn't provide space mission-critical reliability, lacks satellite communication capabilities"
        },
        examStrategy: "Space agency migration: Control Tower + Ground Station + S3 Cross-Region + Kinesis + Direct Connect + EC2 dedicated + CloudHSM + Artifact for mission-critical space operations."
      }
    },

    {
      id: 'sap_476',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 135,
      scenario: "A global humanitarian organization operating in 50 countries needs to migrate their disaster response coordination, resource distribution, and volunteer management systems to AWS. They need to rapidly deploy emergency response systems, coordinate relief efforts across multiple organizations, ensure reliable communications in disaster zones, and manage donations and resource allocation transparently.",
      question: "Which migration approach provides OPTIMAL disaster response coordination with rapid deployment and transparent resource management?",
      options: [
        "Deploy AWS Control Tower for multi-country governance, Amazon Connect for emergency communications, AWS Step Functions for disaster response workflows, Amazon S3 for resource tracking, AWS Lambda for donation processing, and Amazon QuickSight for transparency reporting",
        "Use Amazon EC2 for disaster applications, Amazon RDS for resource data, AWS Lambda for coordination processing, Amazon S3 for emergency data, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized relief services, Amazon Aurora for humanitarian database, AWS Batch for resource processing, Amazon SageMaker for resource optimization, and AWS Config for compliance monitoring",
        "Deploy Amazon API Gateway for humanitarian APIs, Amazon DynamoDB for volunteer data, AWS Glue for data processing, Amazon Redshift for analytics, and AWS Security Hub for security management"
      ],
      correct: 0,
      explanation: {
        correct: "Control Tower provides governance and rapid deployment capabilities across 50 countries for emergency response. Connect ensures reliable communications even in disaster zones. Step Functions orchestrates complex multi-organizational disaster response workflows. S3 provides durable resource tracking and coordination data storage. Lambda processes donations and resource allocation efficiently during crisis periods. QuickSight provides transparent reporting on resource distribution and donation usage for accountability.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide rapid deployment capabilities for emergency response, lacks transparent reporting",
          2: "EKS adds complexity without disaster response benefits, doesn't address emergency communication and transparency needs",
          3: "API Gateway alone insufficient for emergency communications, doesn't provide transparent resource allocation reporting"
        },
        examStrategy: "Disaster response migration: Control Tower + Connect + Step Functions + S3 + Lambda + QuickSight for rapid transparent humanitarian operations."
      }
    },

    {
      id: 'sap_477',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 185,
      scenario: "A multinational cryptocurrency exchange serving 50 million users across 100 countries needs to migrate their trading engine, wallet services, and compliance systems to AWS. The solution must process 1 million transactions per second, ensure sub-millisecond trading latency, implement advanced fraud detection, meet financial regulations in each country, and maintain 99.999% uptime for trading operations.",
      question: "Which comprehensive modernization strategy provides OPTIMAL cryptocurrency trading with maximum performance, security, and regulatory compliance?",
      options: [
        "Deploy Amazon EC2 for trading applications, Amazon DynamoDB for transaction data, AWS Lambda for trade processing, Amazon S3 for trading data, and AWS CloudTrail for audit compliance",
        "Use Amazon EKS for containerized trading services, Amazon Aurora for trading database, AWS Step Functions for trading workflows, Amazon GuardDuty for fraud detection, and AWS Config for compliance monitoring",
        "Implement AWS Control Tower for multi-country regulatory compliance, Amazon ElastiCache for sub-millisecond trading latency, Amazon DynamoDB with DAX for ultra-fast transaction processing, Amazon Kinesis Data Streams for real-time order matching, Amazon Fraud Detector with custom ML models for advanced fraud detection, AWS PrivateLink for secure connectivity, AWS CloudHSM for cryptographic wallet operations, and AWS Direct Connect for institutional trading connectivity",
        "Deploy Amazon API Gateway for trading APIs, Amazon RDS for user accounts, AWS Batch for transaction processing, Amazon Redshift for trading analytics, and AWS Security Hub for security management"
      ],
      correct: 2,
      explanation: {
        correct: "Control Tower ensures compliance with financial regulations across 100 countries. ElastiCache provides sub-millisecond trading latency for competitive cryptocurrency trading. DynamoDB with DAX processes 1M transactions per second with ultra-low latency. Kinesis handles real-time order matching for 50M users. Fraud Detector provides advanced ML-powered fraud detection for cryptocurrency security. PrivateLink ensures secure trading connectivity. CloudHSM provides hardware-based cryptographic security for wallet operations. Direct Connect ensures reliable institutional trading connectivity with 99.999% uptime.",
        whyWrong: {
          0: "Basic EC2/DynamoDB doesn't provide sub-millisecond latency needed for crypto trading, lacks advanced fraud detection",
          1: "EKS adds complexity without crypto trading benefits, GuardDuty alone insufficient for advanced crypto fraud detection",
          3: "API Gateway/RDS doesn't provide crypto trading-optimized performance, Batch not suitable for real-time trading"
        },
        examStrategy: "Cryptocurrency exchange migration: Control Tower + ElastiCache + DynamoDB DAX + Kinesis + Fraud Detector + PrivateLink + CloudHSM + Direct Connect."
      }
    },

    {
      id: 'sap_478',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 140,
      scenario: "A global renewable energy company with wind and solar farms across 40 countries needs to migrate their energy production monitoring, grid integration, and energy trading systems to AWS. They manage 10,000 renewable energy installations, need real-time production optimization, predictive maintenance to reduce downtime by 30%, and automated energy trading to maximize revenue.",
      question: "Which migration strategy provides OPTIMAL renewable energy operations with predictive maintenance and automated trading capabilities?",
      options: [
        "Use AWS IoT Core for energy system connectivity, Amazon Timestream for time-series energy data, Amazon Forecast for production prediction, Amazon Lookout for Equipment for predictive maintenance, AWS Lambda for automated trading algorithms, and Amazon QuickSight for energy operations dashboards",
        "Deploy Amazon EC2 for energy applications, Amazon RDS for production data, AWS Lambda for trading processing, Amazon S3 for energy data, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized energy services, Amazon Aurora for energy database, AWS Step Functions for energy workflows, Amazon SageMaker for optimization models, and AWS Batch for batch processing",
        "Use Amazon API Gateway for energy APIs, Amazon DynamoDB for installation data, AWS Glue for data processing, Amazon Redshift for analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "IoT Core connects 10,000 renewable energy installations for real-time monitoring across 40 countries. Timestream efficiently stores time-series energy production and grid data. Forecast optimizes energy production through weather and demand prediction. Lookout for Equipment provides predictive maintenance reducing downtime by 30%. Lambda executes automated energy trading algorithms to maximize revenue. QuickSight provides comprehensive energy operations and trading performance dashboards.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide IoT-optimized energy monitoring, lacks predictive maintenance capabilities",
          2: "EKS adds complexity without energy benefits, doesn't address specialized IoT and predictive maintenance needs",
          3: "API Gateway alone insufficient for energy system connectivity, doesn't provide predictive maintenance and trading automation"
        },
        examStrategy: "Renewable energy migration: IoT Core + Timestream + Forecast + Lookout Equipment + Lambda + QuickSight for optimized energy operations."
      }
    },

    {
      id: 'sap_479',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 190,
      scenario: "A multinational news organization with bureaus in 80 countries needs to migrate their content management, real-time news distribution, and audience analytics systems to AWS. The solution must support real-time news publishing to 200 million readers globally, ensure content authenticity and combat misinformation, enable live video streaming for breaking news, and provide personalized news experiences while maintaining editorial independence across regions.",
      question: "Which comprehensive modernization approach provides OPTIMAL news operations with global distribution, content authenticity, and personalized experiences?",
      options: [
        "Deploy Amazon EC2 for news applications, Amazon S3 for content storage, AWS Lambda for content processing, Amazon RDS for news database, and Amazon CloudFront for content delivery",
        "Use Amazon EKS for containerized news services, Amazon Aurora for news database, AWS Step Functions for publishing workflows, AWS Elemental MediaLive for streaming, and AWS Config for compliance monitoring",
        "Implement Amazon S3 with Cross-Region Replication for global content distribution, AWS Elemental MediaLive for real-time video streaming, Amazon CloudFront with Lambda@Edge for personalized content delivery, Amazon Textract for content digitization, Amazon Comprehend for content analysis and misinformation detection, Amazon Personalize for audience recommendations, AWS Blockchain for content authenticity verification, and Amazon Kinesis for real-time audience analytics",
        "Deploy Amazon API Gateway for news APIs, Amazon DynamoDB for article metadata, AWS Glue for content processing, Amazon Redshift for audience analytics, and AWS Security Hub for content security"
      ],
      correct: 2,
      explanation: {
        correct: "S3 Cross-Region Replication provides global content distribution to 200M readers across 80 countries. MediaLive enables real-time breaking news video streaming. CloudFront with Lambda@Edge delivers personalized news experiences while maintaining global performance. Textract digitizes content from various sources. Comprehend analyzes content for misinformation detection maintaining news authenticity. Personalize provides audience-specific news recommendations while preserving editorial independence. Blockchain ensures content authenticity and combats misinformation. Kinesis provides real-time audience analytics for editorial insights.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide personalized content delivery, lacks content authenticity and misinformation detection",
          1: "EKS adds complexity without news benefits, doesn't address content authenticity and personalization needs",
          3: "API Gateway/DynamoDB doesn't provide news-optimized distribution, lacks content authenticity verification capabilities"
        },
        examStrategy: "News organization migration: S3 Cross-Region + MediaLive + CloudFront Lambda@Edge + Textract + Comprehend + Personalize + Blockchain + Kinesis for authentic global news."
      }
    },

    {
      id: 'sap_480',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 145,
      scenario: "A global online learning platform serving 100 million students needs to migrate their course delivery, student assessment, and progress tracking systems to AWS. They offer courses in 50 languages, need to support real-time interactive classes with up to 10,000 concurrent students, implement AI-powered personalized learning paths, and ensure accessibility compliance across different regions.",
      question: "Which migration approach provides OPTIMAL online learning with personalized experiences and global accessibility?",
      options: [
        "Deploy Amazon Chime SDK for interactive classes, Amazon Personalize for learning path recommendations, Amazon Translate for multi-language support, Amazon Polly for text-to-speech accessibility, AWS Lambda for assessment processing, and Amazon CloudFront for global content delivery",
        "Use Amazon EC2 for learning applications, Amazon RDS for student data, AWS Lambda for course processing, Amazon S3 for course content, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized learning services, Amazon Aurora for education database, AWS Step Functions for learning workflows, Amazon SageMaker for learning analytics, and AWS Batch for batch processing",
        "Deploy Amazon API Gateway for learning APIs, Amazon DynamoDB for course data, AWS Glue for data processing, Amazon Redshift for student analytics, and AWS Config for compliance monitoring"
      ],
      correct: 0,
      explanation: {
        correct: "Chime SDK supports real-time interactive classes with up to 10,000 concurrent students. Personalize provides AI-powered personalized learning paths for 100M students. Translate enables course delivery in 50 languages. Polly ensures accessibility compliance with text-to-speech capabilities. Lambda processes student assessments efficiently and elastically. CloudFront provides global content delivery ensuring consistent learning experiences worldwide.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide interactive classroom capabilities, lacks AI-powered personalization and accessibility features",
          2: "EKS adds complexity without learning benefits, doesn't address real-time interactive classes and accessibility needs",
          3: "API Gateway alone insufficient for interactive classrooms, doesn't provide AI personalization and accessibility compliance"
        },
        examStrategy: "Online learning migration: Chime SDK + Personalize + Translate + Polly + Lambda + CloudFront for personalized accessible global education."
      }
    },

    {
      id: 'sap_481',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 185,
      scenario: "A multinational maritime shipping company with vessels operating across all major shipping routes needs to migrate their fleet management, cargo tracking, and port operations systems to AWS. The solution must track 5,000 vessels in real-time, optimize shipping routes to reduce fuel consumption by 20%, predict maintenance needs for maritime equipment, ensure compliance with international maritime regulations, and coordinate with port authorities globally.",
      question: "Which comprehensive modernization strategy provides OPTIMAL maritime operations with route optimization and regulatory compliance?",
      options: [
        "Deploy Amazon EC2 for maritime applications, Amazon S3 for shipping data, AWS Lambda for tracking processing, Amazon RDS for vessel data, and AWS CloudTrail for compliance auditing",
        "Use Amazon EKS for containerized maritime services, Amazon Aurora for shipping database, AWS Step Functions for shipping workflows, Amazon SageMaker for route optimization, and AWS Config for compliance monitoring",
        "Implement AWS IoT Core for vessel connectivity, Amazon Location Service for global route optimization, Amazon Timestream for maritime time-series data, Amazon Forecast for maintenance prediction, AWS PrivateLink for secure port authority connectivity, AWS Ground Station for satellite vessel communications, Amazon Lookout for Equipment for predictive maintenance, and AWS Control Tower for international maritime compliance",
        "Deploy Amazon API Gateway for shipping APIs, Amazon DynamoDB for cargo data, AWS Glue for maritime data processing, Amazon Redshift for shipping analytics, and AWS Security Hub for security management"
      ],
      correct: 2,
      explanation: {
        correct: "IoT Core provides real-time connectivity for 5,000 vessels across global shipping routes. Location Service optimizes shipping routes reducing fuel consumption by 20%. Timestream stores maritime time-series data for vessel performance analysis. Forecast predicts maintenance needs for maritime equipment. PrivateLink ensures secure connectivity with port authorities globally. Ground Station provides satellite communications for vessels in remote waters. Lookout for Equipment predicts maritime equipment maintenance needs. Control Tower ensures compliance with international maritime regulations across all shipping routes.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide IoT-optimized vessel connectivity, lacks route optimization and predictive maintenance",
          1: "EKS adds complexity without maritime benefits, doesn't address satellite communications and port authority coordination",
          3: "API Gateway alone insufficient for vessel connectivity, doesn't provide satellite communications and route optimization"
        },
        examStrategy: "Maritime migration: IoT Core + Location Service + Timestream + Forecast + PrivateLink + Ground Station + Lookout Equipment + Control Tower for optimized shipping."
      }
    },

    {
      id: 'sap_482',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 125,
      scenario: "A global sports organization managing multiple international tournaments needs to migrate their event management, live broadcasting, and fan engagement systems to AWS. They broadcast events to 500 million viewers worldwide, need real-time sports analytics, fan interaction capabilities during live events, and comprehensive athlete performance tracking across different sports.",
      question: "Which migration strategy provides OPTIMAL sports broadcasting with real-time analytics and fan engagement?",
      options: [
        "Use AWS Elemental MediaLive for live sports broadcasting, Amazon Kinesis for real-time sports analytics, Amazon Pinpoint for fan engagement, AWS Lambda for performance calculations, Amazon S3 for sports content storage, and Amazon CloudFront for global sports content delivery",
        "Deploy Amazon EC2 for sports applications, Amazon RDS for event data, AWS Lambda for analytics processing, Amazon S3 for broadcast content, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized sports services, Amazon Aurora for sports database, AWS Step Functions for event workflows, Amazon SageMaker for performance analytics, and AWS Batch for batch processing",
        "Use Amazon API Gateway for sports APIs, Amazon DynamoDB for fan data, AWS Glue for sports data processing, Amazon Redshift for sports analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "Elemental MediaLive provides scalable live sports broadcasting to 500M viewers globally. Kinesis processes real-time sports analytics and performance data during live events. Pinpoint enables interactive fan engagement during tournaments. Lambda calculates athlete performance metrics in real-time. S3 stores sports content and historical performance data. CloudFront ensures global delivery of sports content with minimal latency.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide live broadcasting capabilities, lacks real-time fan engagement features",
          2: "EKS adds complexity without sports broadcasting benefits, doesn't address live streaming and fan interaction needs",
          3: "API Gateway alone insufficient for live sports broadcasting, doesn't provide real-time fan engagement capabilities"
        },
        examStrategy: "Sports broadcasting migration: MediaLive + Kinesis + Pinpoint + Lambda + S3 + CloudFront for global sports engagement."
      }
    },

    {
      id: 'sap_483',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 200,
      scenario: "A multinational space exploration company with missions to Mars, Moon, and asteroid mining operations needs to migrate their mission control, deep space data processing, and interplanetary communication systems to AWS. The solution must handle communication delays up to 22 minutes with Mars, process exabytes of space exploration data, coordinate multi-planetary missions, ensure 99.9999% reliability for life-support systems, and enable real-time collaboration between Earth-based and space-based teams.",
      question: "Which comprehensive modernization approach provides OPTIMAL interplanetary mission operations with maximum reliability and deep space capabilities?",
      options: [
        "Deploy Amazon EC2 for space applications, Amazon S3 for space data, AWS Lambda for mission processing, Amazon RDS for mission data, and AWS CloudWatch for monitoring",
        "Use Amazon EKS for containerized space services, Amazon Aurora for space database, AWS Step Functions for mission workflows, Amazon SageMaker for space analytics, and AWS Config for compliance monitoring",
        "Implement AWS Control Tower for multi-planetary governance, AWS Ground Station for deep space communications, Amazon S3 with Intelligent Tiering for exabyte-scale space data storage, AWS Snow Family for data transfer from space missions, Amazon Kinesis Data Streams for mission telemetry, AWS Step Functions for mission-critical workflows with retry logic, Amazon WorkSpaces for Earth-space collaboration, AWS Outposts for space-based computing, and AWS CloudHSM for mission-critical security",
        "Deploy Amazon API Gateway for space APIs, Amazon DynamoDB for mission data, AWS Glue for space data processing, Amazon Redshift for space analytics, and AWS Security Hub for security management"
      ],
      correct: 2,
      explanation: {
        correct: "Control Tower provides governance for multi-planetary mission operations. Ground Station handles deep space communications with 22-minute Mars delays. S3 Intelligent Tiering manages exabytes of space exploration data cost-effectively. Snow Family enables data transfer from remote space missions. Kinesis processes mission telemetry in real-time. Step Functions provides mission-critical workflows with automatic retry for 99.9999% reliability. WorkSpaces enables Earth-space team collaboration. Outposts provides space-based computing capabilities. CloudHSM ensures hardware-based security for life-support and mission-critical systems.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide space mission-critical reliability, lacks deep space communication capabilities",
          1: "EKS doesn't provide space-grade reliability for life-support systems, lacks deep space and multi-planetary capabilities",
          3: "API Gateway/DynamoDB doesn't provide space mission-critical reliability, lacks deep space communication and space-based computing"
        },
        examStrategy: "Interplanetary migration: Control Tower + Ground Station + S3 Intelligent + Snow Family + Kinesis + Step Functions + WorkSpaces + Outposts + CloudHSM for space exploration."
      }
    },

    {
      id: 'sap_484',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 150,
      scenario: "A global wildlife conservation organization monitoring endangered species across 60 countries needs to migrate their animal tracking, habitat monitoring, and conservation analytics systems to AWS. They track 100,000 animals with GPS collars and sensors, need real-time alerts for poaching activities, analyze environmental data to predict habitat changes, and coordinate international conservation efforts.",
      question: "Which migration approach provides OPTIMAL wildlife conservation with real-time monitoring and predictive analytics?",
      options: [
        "Deploy AWS IoT Core for animal tracking devices, Amazon Location Service for geofencing and movement analysis, Amazon Lookout for Metrics for anomaly detection in animal behavior, Amazon Forecast for habitat prediction, AWS Lambda for real-time poaching alerts, and Amazon QuickSight for conservation analytics dashboards",
        "Use Amazon EC2 for conservation applications, Amazon RDS for tracking data, AWS Lambda for alert processing, Amazon S3 for environmental data, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized conservation services, Amazon Aurora for wildlife database, AWS Step Functions for conservation workflows, Amazon SageMaker for habitat models, and AWS Batch for batch processing",
        "Deploy Amazon API Gateway for conservation APIs, Amazon DynamoDB for animal data, AWS Glue for environmental data processing, Amazon Redshift for conservation analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "IoT Core connects 100,000 GPS collars and sensors for real-time animal tracking across 60 countries. Location Service provides geofencing and movement pattern analysis for wildlife protection. Lookout for Metrics detects anomalies in animal behavior indicating potential poaching activities. Forecast predicts habitat changes using environmental data. Lambda triggers real-time poaching alerts for immediate conservation response. QuickSight provides comprehensive conservation analytics for international coordination efforts.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide IoT-optimized animal tracking, lacks geofencing and anomaly detection for poaching",
          2: "EKS adds complexity without conservation benefits, doesn't address real-time tracking and poaching alert needs",
          3: "API Gateway alone insufficient for animal tracking devices, doesn't provide geofencing and predictive habitat analytics"
        },
        examStrategy: "Wildlife conservation migration: IoT Core + Location Service + Lookout Metrics + Forecast + Lambda + QuickSight for conservation protection."
      }
    },

    {
      id: 'sap_485',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 195,
      scenario: "A multinational quantum computing research consortium with facilities in 20 countries needs to migrate their quantum simulation, algorithm development, and collaborative research systems to AWS. The solution must support quantum-classical hybrid computing workloads, enable secure quantum algorithm sharing between research institutions, process quantum measurement data in real-time, and coordinate international quantum research projects while maintaining quantum coherence in distributed systems.",
      question: "Which comprehensive modernization strategy provides OPTIMAL quantum computing research with international collaboration and quantum-classical integration?",
      options: [
        "Deploy Amazon EC2 for quantum applications, Amazon S3 for quantum data, AWS Lambda for algorithm processing, Amazon RDS for research data, and AWS CloudTrail for research auditing",
        "Use Amazon EKS for containerized quantum services, Amazon Aurora for quantum database, AWS Step Functions for quantum workflows, Amazon SageMaker for quantum analytics, and AWS Config for compliance monitoring",
        "Implement Amazon Braket for quantum computing services, AWS ParallelCluster for classical HPC support, Amazon S3 with Cross-Region Replication for quantum algorithm sharing, AWS PrivateLink for secure quantum research connectivity, Amazon Kinesis for real-time quantum measurement data, AWS Direct Connect with ultra-low latency for quantum coherence, Amazon FSx for Lustre for high-performance quantum data storage, and AWS Control Tower for multi-institutional quantum research governance",
        "Deploy Amazon API Gateway for quantum APIs, Amazon DynamoDB for quantum measurements, AWS Glue for quantum data processing, Amazon Redshift for quantum analytics, and AWS Security Hub for quantum security"
      ],
      correct: 2,
      explanation: {
        correct: "Braket provides native quantum computing services for quantum-classical hybrid workloads. ParallelCluster supports classical HPC requirements for quantum simulation. S3 Cross-Region Replication enables secure quantum algorithm sharing between 20 countries. PrivateLink ensures secure connectivity maintaining quantum research confidentiality. Kinesis processes quantum measurement data in real-time. Direct Connect with ultra-low latency preserves quantum coherence in distributed systems. FSx for Lustre provides high-performance storage for quantum data processing. Control Tower provides governance for international quantum research collaboration.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide quantum computing capabilities, lacks quantum coherence and specialized quantum services",
          1: "EKS doesn't provide quantum computing services, lacks quantum-classical integration and coherence requirements",
          3: "API Gateway/DynamoDB doesn't provide quantum computing capabilities, lacks quantum coherence and specialized quantum research features"
        },
        examStrategy: "Quantum research migration: Braket + ParallelCluster + S3 Cross-Region + PrivateLink + Kinesis + Direct Connect + FSx Lustre + Control Tower for quantum computing."
      }
    },

    {
      id: 'sap_486',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 140,
      scenario: "A global fashion retail company with stores in 70 countries needs to migrate their inventory management, demand forecasting, and omnichannel customer experience systems to AWS. They manage 10 million products across online and physical stores, need real-time inventory visibility, AI-powered trend prediction, and personalized shopping experiences that adapt to local fashion preferences in different regions.",
      question: "Which migration strategy provides OPTIMAL fashion retail operations with trend prediction and localized personalization?",
      options: [
        "Use Amazon Forecast for demand prediction, Amazon Personalize for personalized recommendations, AWS Lambda for inventory updates, Amazon DynamoDB for product catalog, Amazon Translate for localization, and Amazon CloudFront for global e-commerce delivery",
        "Deploy Amazon EC2 for retail applications, Amazon RDS for inventory data, AWS Lambda for trend processing, Amazon S3 for product images, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized retail services, Amazon Aurora for retail database, AWS Step Functions for retail workflows, Amazon SageMaker for trend analytics, and AWS Batch for batch processing",
        "Use Amazon API Gateway for retail APIs, Amazon ElastiCache for real-time inventory, AWS Glue for data processing, Amazon Redshift for retail analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "Forecast provides AI-powered demand prediction and trend forecasting for fashion retail. Personalize delivers localized personalized shopping experiences adapted to regional fashion preferences. Lambda handles real-time inventory updates across 10M products and 70 countries. DynamoDB manages the global product catalog with high performance. Translate enables localization for different regional markets. CloudFront provides global e-commerce delivery with optimal performance for omnichannel experiences.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide AI-powered trend prediction, lacks personalized shopping and localization capabilities",
          2: "EKS adds complexity without fashion retail benefits, doesn't address trend prediction and localization needs",
          3: "API Gateway alone insufficient for trend prediction, doesn't provide AI-powered personalization and localization"
        },
        examStrategy: "Fashion retail migration: Forecast + Personalize + Lambda + DynamoDB + Translate + CloudFront for global personalized fashion retail."
      }
    },

    {
      id: 'sap_487',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 180,
      scenario: "A multinational precision agriculture company serving farms across 40 countries needs to migrate their crop monitoring, irrigation optimization, and yield prediction systems to AWS. The solution must process data from 1 million IoT sensors, provide real-time crop health monitoring, optimize water usage to reduce consumption by 40%, predict crop yields 6 months in advance, and ensure food security through AI-driven agricultural insights.",
      question: "Which comprehensive modernization approach provides OPTIMAL precision agriculture with water optimization and yield prediction capabilities?",
      options: [
        "Deploy Amazon EC2 for agriculture applications, Amazon S3 for sensor data, AWS Lambda for crop processing, Amazon RDS for farm data, and Amazon CloudWatch for monitoring",
        "Use Amazon EKS for containerized agriculture services, Amazon Aurora for agriculture database, AWS Step Functions for farming workflows, Amazon SageMaker for crop analytics, and AWS Config for compliance monitoring",
        "Implement AWS IoT Core for sensor connectivity, AWS IoT Device Management for 1M sensor fleet, Amazon Timestream for time-series agricultural data, Amazon SageMaker with built-in algorithms for crop health analysis, Amazon Forecast for yield prediction, AWS Lambda for irrigation automation, Amazon Location Service for field mapping, and Amazon QuickSight for agricultural operations dashboards",
        "Deploy Amazon API Gateway for agriculture APIs, Amazon DynamoDB for crop data, AWS Glue for agricultural data processing, Amazon Redshift for farm analytics, and AWS Security Hub for farm security"
      ],
      correct: 2,
      explanation: {
        correct: "IoT Core connects 1M agricultural sensors across 40 countries for real-time monitoring. IoT Device Management handles massive sensor fleet operations. Timestream stores time-series agricultural data efficiently. SageMaker provides AI-driven crop health analysis and real-time monitoring. Forecast predicts crop yields 6 months in advance for food security planning. Lambda automates irrigation systems reducing water consumption by 40%. Location Service provides precise field mapping and monitoring. QuickSight delivers comprehensive agricultural operations dashboards for farm management insights.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide IoT-optimized agricultural sensor connectivity, lacks AI-driven crop health analysis",
          1: "EKS adds complexity without agriculture benefits, doesn't address specialized IoT and precision agriculture needs",
          3: "API Gateway alone insufficient for agricultural sensor connectivity, doesn't provide AI crop health analysis and yield prediction"
        },
        examStrategy: "Precision agriculture migration: IoT Core + IoT Device Management + Timestream + SageMaker + Forecast + Lambda + Location Service + QuickSight for smart farming."
      }
    },

    {
      id: 'sap_488',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 135,
      scenario: "A global nonprofit organization coordinating disaster relief efforts across 80 countries needs to migrate their emergency response, resource coordination, and volunteer management systems to AWS. They need to rapidly mobilize resources during disasters, coordinate with local governments and NGOs, ensure secure communications in crisis zones, and provide real-time disaster impact assessment.",
      question: "Which migration approach provides OPTIMAL disaster relief coordination with rapid mobilization and secure communications?",
      options: [
        "Deploy AWS Control Tower for multi-country governance, Amazon Connect for emergency communications, AWS Step Functions for disaster response workflows, Amazon S3 for resource tracking, AWS Lambda for rapid deployment automation, and Amazon QuickSight for impact assessment dashboards",
        "Use Amazon EC2 for relief applications, Amazon RDS for resource data, AWS Lambda for coordination processing, Amazon S3 for emergency data, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized relief services, Amazon Aurora for humanitarian database, AWS Batch for resource processing, Amazon SageMaker for impact analytics, and AWS Config for compliance monitoring",
        "Deploy Amazon API Gateway for relief APIs, Amazon DynamoDB for volunteer data, AWS Glue for disaster data processing, Amazon Redshift for relief analytics, and AWS Security Hub for security management"
      ],
      correct: 0,
      explanation: {
        correct: "Control Tower provides governance and rapid resource mobilization across 80 countries during disasters. Connect ensures secure emergency communications even in crisis zones. Step Functions orchestrates complex disaster response workflows with multiple organizations. S3 provides durable resource tracking and coordination data. Lambda enables rapid deployment automation for emergency response systems. QuickSight provides real-time disaster impact assessment and resource allocation dashboards for effective relief coordination.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide rapid disaster response deployment, lacks secure emergency communications capabilities",
          2: "EKS adds complexity without disaster relief benefits, doesn't address emergency communications and rapid deployment needs",
          3: "API Gateway alone insufficient for emergency communications, doesn't provide rapid deployment automation for disaster response"
        },
        examStrategy: "Disaster relief migration: Control Tower + Connect + Step Functions + S3 + Lambda + QuickSight for rapid secure disaster coordination."
      }
    },

    {
      id: 'sap_489',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "hard",
      timeRecommendation: 190,
      scenario: "A multinational biotech company developing personalized medicine solutions across 30 countries needs to migrate their genomic analysis, drug discovery, and clinical trial management systems to AWS. The solution must process whole genome sequences for 10 million patients, ensure HIPAA and GDPR compliance, accelerate drug discovery through AI, support international clinical trials, and deliver personalized treatment recommendations.",
      question: "Which comprehensive modernization strategy provides OPTIMAL personalized medicine with genomic analysis and regulatory compliance?",
      options: [
        "Deploy Amazon EC2 for biotech applications, Amazon S3 for genomic data, AWS Lambda for analysis processing, Amazon RDS for patient data, and AWS CloudTrail for compliance auditing",
        "Use Amazon EKS for containerized biotech services, Amazon Aurora for genomic database, AWS Step Functions for research workflows, Amazon SageMaker for drug discovery, and AWS Config for compliance monitoring",
        "Implement AWS HealthLake for clinical data standardization, Amazon Omics for genomic analysis workflows, Amazon S3 with Cross-Region Replication for genomic data storage, Amazon SageMaker with distributed training for drug discovery AI, AWS PrivateLink for secure clinical trial connectivity, Amazon Comprehend Medical for clinical text analysis, AWS Control Tower for multi-country compliance, and AWS Artifact for regulatory documentation",
        "Deploy Amazon API Gateway for biotech APIs, Amazon DynamoDB for genomic markers, AWS Glue for genomic data processing, Amazon Redshift for biotech analytics, and AWS Security Hub for biotech security"
      ],
      correct: 2,
      explanation: {
        correct: "HealthLake standardizes clinical data ensuring HIPAA compliance. Omics provides specialized genomic analysis workflows for 10M patient sequences. S3 Cross-Region Replication ensures genomic data availability across 30 countries. SageMaker distributed training accelerates AI-driven drug discovery. PrivateLink provides secure connectivity for international clinical trials. Comprehend Medical extracts insights from clinical texts for personalized medicine. Control Tower ensures GDPR and healthcare compliance across countries. Artifact manages regulatory documentation for biotech operations.",
        whyWrong: {
          0: "Basic EC2/S3 doesn't provide specialized genomic analysis capabilities, lacks HIPAA/GDPR compliance features",
          1: "EKS adds complexity without biotech benefits, doesn't address specialized genomic analysis and clinical compliance needs",
          3: "API Gateway/DynamoDB doesn't provide genomic analysis capabilities, lacks healthcare compliance and clinical trial features"
        },
        examStrategy: "Personalized medicine migration: HealthLake + Omics + S3 Cross-Region + SageMaker distributed + PrivateLink + Comprehend Medical + Control Tower + Artifact for compliant genomics."
      }
    },

    {
      id: 'sap_490',
      domain: "Domain 4: Accelerate Workload Migration and Modernization",
      difficulty: "medium",
      timeRecommendation: 125,
      scenario: "A global music streaming platform serving 300 million users needs to migrate their music recommendation, content delivery, and artist analytics systems to AWS. They stream 10 billion songs monthly, need personalized music recommendations in real-time, support high-quality audio streaming globally, and provide artists with detailed performance analytics.",
      question: "Which migration strategy provides OPTIMAL music streaming with personalized recommendations and global performance?",
      options: [
        "Use Amazon Personalize for music recommendations, Amazon CloudFront for global audio streaming, Amazon Kinesis for real-time listening analytics, AWS Lambda for recommendation processing, Amazon S3 for music content storage, and Amazon QuickSight for artist analytics dashboards",
        "Deploy Amazon EC2 for streaming applications, Amazon RDS for user data, AWS Lambda for recommendation processing, Amazon S3 for music files, and Amazon CloudWatch for monitoring",
        "Implement Amazon EKS for containerized streaming services, Amazon Aurora for music database, AWS Step Functions for streaming workflows, Amazon SageMaker for recommendation models, and AWS Batch for batch processing",
        "Use Amazon API Gateway for streaming APIs, Amazon DynamoDB for listening history, AWS Glue for music data processing, Amazon Redshift for music analytics, and AWS Config for configuration management"
      ],
      correct: 0,
      explanation: {
        correct: "Personalize delivers real-time personalized music recommendations for 300M users. CloudFront provides global high-quality audio streaming with minimal latency. Kinesis processes real-time listening analytics from 10B monthly streams. Lambda handles dynamic recommendation processing efficiently. S3 provides scalable and durable music content storage. QuickSight delivers detailed artist performance analytics for music industry insights.",
        whyWrong: {
          1: "Basic EC2/RDS doesn't provide AI-powered music recommendations, lacks global high-quality audio streaming optimization",
          2: "EKS adds complexity without music streaming benefits, doesn't address real-time recommendation and global streaming needs",
          3: "API Gateway alone insufficient for music streaming optimization, doesn't provide AI-powered personalized recommendations"
        },
        examStrategy: "Music streaming migration: Personalize + CloudFront + Kinesis + Lambda + S3 + QuickSight for global personalized music platform."
      }
    },

  {
  id: 'sap_491',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "hard",
  timeRecommendation: 185,
  scenario: "A multinational smart manufacturing consortium with factories across 25 countries needs to migrate their industrial automation, predictive maintenance, and supply chain optimization systems to AWS. The solution must coordinate production across 500 factories, predict equipment failures 60 days in advance, optimize just-in-time delivery reducing inventory by 50%, and ensure cybersecurity for industrial control systems.",
  question: "Which comprehensive modernization approach provides OPTIMAL smart manufacturing with predictive analytics and supply chain optimization?",
  options: [
    "Deploy Amazon EC2 for manufacturing applications, Amazon S3 for production data, AWS Lambda for automation processing, Amazon RDS for factory data, and AWS CloudTrail for security auditing",
    "Use Amazon EKS for containerized manufacturing services, Amazon Aurora for production database, AWS Step Functions for manufacturing workflows, Amazon SageMaker for predictive models, and AWS Config for compliance monitoring",
    "Implement AWS IoT Greengrass for edge computing in factories, AWS IoT Core for industrial device management, Amazon Timestream for time-series production data, Amazon Lookout for Equipment for predictive maintenance, Amazon Forecast for supply chain optimization, AWS PrivateLink for secure factory connectivity, AWS IoT Device Defender for cybersecurity, and AWS Control Tower for multi-country industrial governance",
    "Deploy Amazon API Gateway for manufacturing APIs, Amazon DynamoDB for production records, AWS Glue for industrial data processing, Amazon Redshift for manufacturing analytics, and AWS Security Hub for industrial security"
  ],
  correct: 2,
  explanation: {
    correct: "IoT Greengrass enables edge processing at 500 factories for real-time automation with local decision-making capabilities. Lookout for Equipment predicts equipment failures 60 days in advance using ML-powered anomaly detection on industrial sensor data. Forecast optimizes just-in-time delivery reducing inventory by 50% through demand prediction algorithms. IoT Device Defender provides cybersecurity for industrial control systems with continuous threat monitoring. Control Tower ensures governance across 25 countries for manufacturing compliance and regulatory requirements.",
    whyWrong: {
      0: "Basic EC2 doesn't provide IoT-optimized industrial connectivity for 500 factories, lacks specialized predictive maintenance capabilities for 60-day advance predictions",
      1: "EKS adds unnecessary complexity for manufacturing IoT workloads, doesn't address edge computing needs for distributed factory operations",
      3: "API Gateway insufficient for industrial device connectivity at scale, lacks specialized predictive maintenance and cybersecurity features for manufacturing"
    },
    examStrategy: "Smart manufacturing migration: IoT Greengrass edge + Lookout Equipment predictive + Forecast optimization + Device Defender security + Control Tower governance for industrial transformation."
  }
},

{
  id: 'sap_492',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "medium",
  timeRecommendation: 130,
  scenario: "A global virtual reality platform for remote collaboration needs to migrate their 3D rendering, multi-user synchronization, and immersive content delivery systems to AWS. They serve 50 million VR users, need sub-20ms latency for VR interactions, support simultaneous collaboration sessions with up to 1,000 participants, and deliver high-resolution 3D content globally.",
  question: "Which migration approach provides OPTIMAL VR collaboration with low-latency interactions and global content delivery?",
  options: [
    "Deploy Amazon GameLift for VR session management, Amazon ElastiCache for sub-20ms latency, Amazon CloudFront for global 3D content delivery, AWS Lambda for VR event processing, Amazon S3 for immersive content storage, and Amazon API Gateway for VR application integration",
    "Use Amazon EC2 for VR applications, Amazon RDS for user data, AWS Lambda for VR processing, Amazon S3 for VR content, and Amazon CloudWatch for monitoring",
    "Implement Amazon EKS for containerized VR services, Amazon Aurora for VR database, AWS Step Functions for VR workflows, Amazon SageMaker for VR analytics, and AWS Batch for batch processing",
    "Deploy Amazon API Gateway for VR APIs, Amazon DynamoDB for session data, AWS Glue for VR data processing, Amazon Redshift for VR analytics, and AWS Config for configuration management"
  ],
  correct: 0,
  explanation: {
    correct: "GameLift provides managed VR session orchestration supporting 1,000 concurrent participants with automatic scaling and low-latency networking optimized for real-time multiplayer experiences. ElastiCache delivers sub-20ms response times essential for smooth VR interactions across 50M users with in-memory caching. CloudFront ensures global delivery of high-resolution 3D content with edge optimization and geographic distribution. Lambda processes VR events in real-time with millisecond-level responsiveness for immersive collaborative experiences.",
    whyWrong: {
      1: "Basic EC2 lacks gaming-optimized networking for VR latency requirements, insufficient for managing 1,000 concurrent participant sessions",
      2: "EKS introduces unnecessary complexity without VR-specific optimizations, Step Functions adds latency unsuitable for real-time VR interactions",
      3: "Standard APIs don't provide sub-20ms latency optimization needed for VR, lacks specialized session management for large-scale collaboration"
    },
    examStrategy: "VR collaboration migration: GameLift sessions + ElastiCache ultra-low latency + CloudFront global delivery + Lambda real-time processing for immersive VR."
  }
},

{
  id: 'sap_493',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "hard",
  timeRecommendation: 200,
  scenario: "A multinational climate research organization with observation stations across all continents needs to migrate their climate modeling, weather prediction, and environmental impact assessment systems to AWS. The solution must process data from 50,000 climate sensors, run complex climate simulations requiring 100,000 CPU cores, predict climate changes 100 years into the future, coordinate international climate research, and ensure data integrity for critical climate science.",
  question: "Which comprehensive modernization strategy provides OPTIMAL climate research with long-term prediction and international coordination capabilities?",
  options: [
    "Deploy Amazon EC2 for climate applications, Amazon S3 for climate data, AWS Lambda for sensor processing, Amazon RDS for research data, and AWS CloudWatch for monitoring",
    "Use Amazon EKS for containerized climate services, Amazon Aurora for climate database, AWS Step Functions for research workflows, Amazon SageMaker for climate models, and AWS Config for compliance monitoring",
    "Implement AWS IoT Core for climate sensor connectivity, AWS ParallelCluster for massive climate simulation workloads, Amazon S3 with Cross-Region Replication for climate data preservation, Amazon Timestream for time-series environmental data, Amazon Forecast for climate prediction modeling, AWS Ground Station for satellite climate data, AWS DataSync for international research data sharing, and AWS Control Tower for global climate research governance",
    "Deploy Amazon API Gateway for climate APIs, Amazon DynamoDB for sensor measurements, AWS Glue for climate data processing, Amazon Redshift for climate analytics, and AWS Security Hub for research security"
  ],
  correct: 2,
  explanation: {
    correct: "ParallelCluster provides 100,000 CPU cores for complex climate simulations with HPC-optimized networking and job scheduling for massive computational workloads. S3 Cross-Region Replication ensures climate data preservation across continents with 99.999999999% durability for long-term research integrity. Forecast enables ML-powered climate predictions up to 100 years using historical patterns and environmental variables. Ground Station ingests satellite climate data for comprehensive global monitoring across all continents.",
    whyWrong: {
      0: "Standard EC2 lacks HPC optimization for 100K CPU climate simulations, insufficient for complex long-term modeling requirements",
      1: "EKS doesn't provide HPC capabilities needed for massive climate simulations, lacks specialized climate data preservation features",
      3: "Standard database solutions insufficient for 50K sensor scale, lacks HPC processing capabilities for 100-year climate predictions"
    },
    examStrategy: "Climate research migration: ParallelCluster HPC + S3 Cross-Region preservation + Forecast long-term prediction + Ground Station satellite data for global climate science."
  }
},

{
  id: 'sap_494',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "medium",
  timeRecommendation: 145,
  scenario: "A global autonomous vehicle testing company operating across 30 countries needs to migrate their vehicle simulation, sensor data processing, and safety validation systems to AWS. They collect terabytes of sensor data daily from 10,000 test vehicles, need real-time processing for safety-critical decisions, run millions of simulation scenarios, and ensure regulatory compliance for autonomous vehicle certification.",
  question: "Which migration strategy provides OPTIMAL autonomous vehicle development with real-time processing and safety validation?",
  options: [
    "Use AWS IoT Core for vehicle sensor connectivity, Amazon Kinesis for real-time sensor data processing, AWS Batch for simulation workloads, Amazon S3 for sensor data storage, AWS Lambda for safety-critical processing, and AWS Control Tower for regulatory compliance across countries",
    "Deploy Amazon EC2 for vehicle applications, Amazon RDS for vehicle data, AWS Lambda for sensor processing, Amazon S3 for simulation data, and Amazon CloudWatch for monitoring",
    "Implement Amazon EKS for containerized vehicle services, Amazon Aurora for vehicle database, AWS Step Functions for testing workflows, Amazon SageMaker for vehicle analytics, and AWS Config for compliance monitoring",
    "Use Amazon API Gateway for vehicle APIs, Amazon DynamoDB for sensor readings, AWS Glue for vehicle data processing, Amazon Redshift for vehicle analytics, and AWS Security Hub for vehicle security"
  ],
  correct: 0,
  explanation: {
    correct: "Kinesis processes terabytes of sensor data daily from 10,000 vehicles in real-time for safety-critical decision making with sub-second latency requirements. Batch efficiently runs millions of simulation scenarios with cost-optimized compute scaling for comprehensive autonomous vehicle testing coverage. Lambda handles safety-critical processing with automatic scaling and millisecond response times for immediate safety decisions. Control Tower ensures regulatory compliance for autonomous vehicle certification across 30 countries with automated governance frameworks.",
    whyWrong: {
      1: "Standard EC2 lacks IoT optimization for 10K vehicle sensor connectivity, insufficient real-time processing capabilities for safety-critical decisions",
      2: "EKS adds complexity without autonomous vehicle benefits, Step Functions introduces latency unsuitable for safety-critical processing",
      3: "Standard APIs insufficient for real-time vehicle sensor data at scale, lacks simulation workload optimization for millions of scenarios"
    },
    examStrategy: "Autonomous vehicle migration: IoT Core connectivity + Kinesis real-time + Batch simulation + Lambda safety-critical + Control Tower compliance for AV development."
  }
},

{
  id: 'sap_495',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "hard",
  timeRecommendation: 190,
  scenario: "A multinational digital twin consortium for smart cities across 50 metropolitan areas needs to migrate their real-time city modeling, infrastructure optimization, and citizen services systems to AWS. The solution must create digital twins of entire cities, process real-time data from 10 million urban sensors, predict infrastructure maintenance needs, optimize traffic flow reducing congestion by 35%, and enable collaborative urban planning across international boundaries.",
  question: "Which comprehensive modernization approach provides OPTIMAL digital twin cities with infrastructure optimization and international collaboration?",
  options: [
    "Deploy Amazon EC2 for city modeling applications, Amazon S3 for city data, AWS Lambda for sensor processing, Amazon RDS for infrastructure data, and AWS CloudTrail for audit compliance",
    "Use Amazon EKS for containerized city services, Amazon Aurora for city database, AWS Step Functions for city workflows, Amazon SageMaker for urban analytics, and AWS Config for compliance monitoring",
    "Implement AWS IoT Core for urban sensor connectivity, AWS IoT TwinMaker for digital twin modeling, Amazon Timestream for time-series urban data, Amazon Location Service for traffic optimization, Amazon Lookout for Equipment for infrastructure maintenance prediction, AWS Lambda for real-time city processing, Amazon QuickSight for urban planning dashboards, and AWS Control Tower for international smart city governance",
    "Deploy Amazon API Gateway for city APIs, Amazon DynamoDB for sensor data, AWS Glue for urban data processing, Amazon Redshift for city analytics, and AWS Security Hub for urban security"
  ],
  correct: 2,
  explanation: {
    correct: "TwinMaker creates comprehensive digital twins of 50 metropolitan areas with 3D visualization and real-time data integration from 10M sensors. Location Service optimizes traffic flow reducing congestion by 35% through ML-powered route analysis and predictive traffic modeling. Lookout for Equipment predicts infrastructure maintenance needs using anomaly detection on urban systems data. Control Tower enables international collaboration across metropolitan boundaries with standardized governance frameworks for smart city initiatives.",
    whyWrong: {
      0: "Standard EC2 lacks digital twin modeling capabilities, insufficient for 10M sensor scale across 50 metropolitan areas",
      1: "EKS doesn't provide digital twin services, lacks traffic optimization algorithms for 35% congestion reduction targets",
      3: "Standard database insufficient for 10M sensor real-time processing, lacks digital twin visualization and traffic optimization capabilities"
    },
    examStrategy: "Digital twin cities migration: TwinMaker modeling + Location Service traffic optimization + Lookout Equipment predictive + Control Tower governance for smart city collaboration."
  }
},

{
  id: 'sap_496',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "medium",
  timeRecommendation: 135,
  scenario: "A global language learning platform serving 200 million users needs to migrate their adaptive learning, speech recognition, and progress tracking systems to AWS. They offer courses in 100 languages, need real-time pronunciation assessment, AI-powered personalized learning paths, and want to reduce learning time by 40% through advanced adaptive algorithms.",
  question: "Which migration approach provides OPTIMAL language learning with speech recognition and adaptive personalization?",
  options: [
    "Deploy Amazon Transcribe for speech recognition, Amazon Polly for pronunciation examples, Amazon Personalize for adaptive learning paths, AWS Lambda for progress processing, Amazon Translate for multi-language support, and Amazon CloudFront for global content delivery",
    "Use Amazon EC2 for learning applications, Amazon RDS for user data, AWS Lambda for speech processing, Amazon S3 for course content, and Amazon CloudWatch for monitoring",
    "Implement Amazon EKS for containerized learning services, Amazon Aurora for education database, AWS Step Functions for learning workflows, Amazon SageMaker for learning analytics, and AWS Batch for batch processing",
    "Deploy Amazon API Gateway for learning APIs, Amazon DynamoDB for progress tracking, AWS Glue for learning data processing, Amazon Redshift for education analytics, and AWS Config for configuration management"
  ],
  correct: 0,
  explanation: {
    correct: "Transcribe provides real-time pronunciation assessment with phoneme-level accuracy for immediate feedback across 100 languages. Personalize creates AI-powered adaptive learning paths reducing learning time by 40% through ML-driven content recommendation algorithms. Polly generates natural pronunciation examples supporting 100 languages with neural voice synthesis. Translate enables cross-language learning features and localization for 200M global users with high-quality language processing.",
    whyWrong: {
      1: "Standard EC2 lacks AI speech recognition capabilities, insufficient for real-time pronunciation assessment across 100 languages",
      2: "EKS adds complexity without language learning benefits, lacks speech recognition and adaptive personalization features",
      3: "Standard APIs don't provide speech recognition capabilities, lacks AI-powered adaptive learning algorithms for 40% time reduction"
    },
    examStrategy: "Language learning migration: Transcribe speech recognition + Personalize adaptive paths + Polly pronunciation + Translate multi-language for global education platform."
  }
},

{
  id: 'sap_497',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "hard",
  timeRecommendation: 185,
  scenario: "A multinational underwater exploration company with submersibles operating in all major oceans needs to migrate their deep-sea data collection, marine life monitoring, and ocean research coordination systems to AWS. The solution must handle communication delays with deep-sea vehicles, process oceanographic data from 1,000 underwater sensors, predict marine ecosystem changes, coordinate international ocean research, and ensure environmental compliance for marine operations.",
  question: "Which comprehensive modernization strategy provides OPTIMAL underwater exploration with marine ecosystem monitoring and international research coordination?",
  options: [
    "Deploy Amazon EC2 for ocean applications, Amazon S3 for oceanographic data, AWS Lambda for sensor processing, Amazon RDS for marine data, and AWS CloudWatch for monitoring",
    "Use Amazon EKS for containerized ocean services, Amazon Aurora for ocean database, AWS Step Functions for exploration workflows, Amazon SageMaker for marine analytics, and AWS Config for compliance monitoring",
    "Implement AWS IoT Core for underwater sensor connectivity, Amazon Timestream for time-series oceanographic data, Amazon Forecast for marine ecosystem prediction, AWS Step Functions for delay-tolerant underwater workflows, Amazon Location Service for underwater navigation, AWS PrivateLink for secure research vessel connectivity, Amazon S3 with Cross-Region Replication for marine research data sharing, and AWS Control Tower for international ocean research governance",
    "Deploy Amazon API Gateway for ocean APIs, Amazon DynamoDB for sensor measurements, AWS Glue for oceanographic data processing, Amazon Redshift for marine analytics, and AWS Security Hub for ocean security"
  ],
  correct: 2,
  explanation: {
    correct: "Step Functions handles delay-tolerant workflows accommodating deep-sea communication delays with automatic retry and state management for intermittent connectivity. Forecast predicts marine ecosystem changes using oceanographic data patterns and environmental variables for conservation planning. Location Service provides underwater navigation for submersibles across all major oceans with precise positioning. Control Tower ensures international ocean research governance with environmental compliance across maritime boundaries and regulations.",
    whyWrong: {
      0: "Standard EC2 lacks underwater communication optimization, insufficient for deep-sea delay tolerance and marine ecosystem prediction requirements",
      1: "EKS doesn't address underwater communication delays, lacks marine-specific ecosystem prediction and delay-tolerant workflow capabilities",
      3: "Standard APIs insufficient for underwater connectivity challenges, lacks delay-tolerant communication and specialized marine ecosystem modeling"
    },
    examStrategy: "Underwater exploration migration: Step Functions delay-tolerant + Forecast ecosystem prediction + Location Service navigation + Control Tower governance for international marine research."
  }
},

{
  id: 'sap_498',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "medium",
  timeRecommendation: 140,
  scenario: "A global virtual events platform needs to migrate their live streaming, interactive networking, and analytics systems to AWS. They host events for up to 1 million concurrent attendees, need real-time audience interaction, support multiple breakout sessions simultaneously, and provide detailed engagement analytics to event organizers.",
  question: "Which migration strategy provides OPTIMAL virtual events with massive scalability and real-time interaction capabilities?",
  options: [
    "Use AWS Elemental MediaLive for live streaming, Amazon IVS for interactive video, Amazon Chime SDK for breakout sessions, Amazon Kinesis for real-time audience analytics, AWS Lambda for event processing, and Amazon QuickSight for engagement dashboards",
    "Deploy Amazon EC2 for event applications, Amazon RDS for attendee data, AWS Lambda for streaming processing, Amazon S3 for event content, and Amazon CloudWatch for monitoring",
    "Implement Amazon EKS for containerized event services, Amazon Aurora for event database, AWS Step Functions for event workflows, Amazon SageMaker for engagement analytics, and AWS Batch for batch processing",
    "Use Amazon API Gateway for event APIs, Amazon DynamoDB for session data, AWS Glue for event data processing, Amazon Redshift for event analytics, and AWS Config for configuration management"
  ],
  correct: 0,
  explanation: {
    correct: "MediaLive provides scalable live streaming supporting 1M concurrent attendees with adaptive bitrate and global distribution capabilities. IVS enables real-time audience interaction with sub-second latency for immediate engagement feedback and live polling. Chime SDK supports multiple simultaneous breakout sessions with integrated video, audio, and screen sharing capabilities. Kinesis processes real-time audience analytics for immediate engagement insights during live events with high-throughput data streaming.",
    whyWrong: {
      1: "Standard EC2 lacks live streaming optimization for 1M concurrent users, insufficient for real-time interactive video requirements at scale",
      2: "EKS adds complexity without live streaming benefits, lacks real-time interaction capabilities for massive audience scale",
      3: "Standard APIs don't provide live streaming infrastructure, insufficient for 1M concurrent attendees and real-time interaction requirements"
    },
    examStrategy: "Virtual events migration: MediaLive streaming + IVS interaction + Chime SDK breakouts + Kinesis analytics for scalable interactive event platform."
  }
},

{
  id: 'sap_499',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "hard",
  timeRecommendation: 195,
  scenario: "A multinational archaeological research consortium with excavation sites across 40 countries needs to migrate their artifact cataloging, 3D site reconstruction, and historical analysis systems to AWS. The solution must create detailed 3D models of archaeological sites, enable collaborative analysis between international research teams, preserve cultural heritage data for future generations, predict optimal excavation strategies, and ensure compliance with cultural heritage protection laws.",
  question: "Which comprehensive modernization approach provides OPTIMAL archaeological research with 3D reconstruction and international collaboration capabilities?",
  options: [
    "Deploy Amazon EC2 for archaeology applications, Amazon S3 for artifact data, AWS Lambda for analysis processing, Amazon RDS for site data, and AWS CloudTrail for heritage compliance",
    "Use Amazon EKS for containerized archaeology services, Amazon Aurora for archaeological database, AWS Step Functions for research workflows, Amazon SageMaker for site analytics, and AWS Config for compliance monitoring",
    "Implement Amazon S3 with Cross-Region Replication for cultural heritage preservation, AWS ParallelCluster for 3D reconstruction processing, Amazon Rekognition for artifact analysis, Amazon Textract for historical document digitization, AWS WorkSpaces for collaborative research environments, Amazon Forecast for excavation optimization, AWS PrivateLink for secure international research connectivity, and AWS Control Tower for cultural heritage compliance across 40 countries",
    "Deploy Amazon API Gateway for archaeology APIs, Amazon DynamoDB for artifact records, AWS Glue for archaeological data processing, Amazon Redshift for heritage analytics, and AWS Security Hub for heritage security"
  ],
  correct: 2,
  explanation: {
    correct: "ParallelCluster provides HPC processing for detailed 3D site reconstruction with photogrammetry and LiDAR data processing capabilities. S3 Cross-Region Replication preserves cultural heritage data across multiple regions with 99.999999999% durability for future generations. Rekognition automatically catalogs and analyzes artifacts using computer vision for pattern recognition and classification. Control Tower ensures compliance with cultural heritage protection laws across 40 countries with automated governance frameworks and regulatory monitoring.",
    whyWrong: {
      0: "Standard EC2 lacks HPC optimization for 3D reconstruction processing, insufficient for detailed archaeological site modeling requirements",
      1: "EKS doesn't provide HPC capabilities needed for 3D reconstruction, lacks specialized cultural heritage preservation features",
      3: "Standard database insufficient for 3D reconstruction data requirements, lacks cultural heritage compliance and preservation capabilities"
    },
    examStrategy: "Archaeological research migration: ParallelCluster 3D reconstruction + S3 Cross-Region preservation + Rekognition artifact analysis + Control Tower heritage compliance for cultural preservation."
  }
},

{
  id: 'sap_500',
  domain: "Domain 4: Accelerate Workload Migration and Modernization",
  difficulty: "hard",
  timeRecommendation: 200,
  scenario: "A multinational interstellar communication research consortium with radio telescopes across 6 continents needs to migrate their signal processing, extraterrestrial intelligence search, and astronomical data analysis systems to AWS. The solution must process exabytes of radio telescope data, detect potential extraterrestrial signals in real-time, coordinate global telescope arrays, enable international collaboration on SETI research, and maintain the highest levels of data integrity for scientific discovery.",
  question: "Which comprehensive modernization approach provides OPTIMAL interstellar communication research with global telescope coordination and advanced signal detection capabilities?",
  options: [
    "Deploy Amazon EC2 for signal processing applications, Amazon S3 for telescope data, AWS Lambda for signal analysis, Amazon RDS for research data, and AWS CloudWatch for monitoring",
    "Use Amazon EKS for containerized astronomy services, Amazon Aurora for telescope database, AWS Step Functions for research workflows, Amazon SageMaker for signal analytics, and AWS Config for compliance monitoring",
    "Implement AWS ParallelCluster for massive signal processing workloads, Amazon S3 with Intelligent Tiering for exabyte-scale telescope data, AWS Ground Station for telescope coordination, Amazon Kinesis Analytics for real-time signal detection, Amazon SageMaker with distributed training for pattern recognition, AWS Direct Connect for high-bandwidth telescope connectivity, Amazon FSx for Lustre for high-performance shared analysis storage, and AWS Control Tower for international SETI research governance",
    "Deploy Amazon API Gateway for astronomy APIs, Amazon DynamoDB for signal data, AWS Glue for telescope data processing, Amazon Redshift for astronomical analytics, and AWS Security Hub for research security"
  ],
  correct: 2,
  explanation: {
    correct: "ParallelCluster provides massive computational power for processing exabytes of radio telescope data with HPC-optimized networking and job scheduling across 6 continents. Kinesis Analytics detects potential extraterrestrial signals in real-time using advanced stream processing and ML algorithms for pattern recognition. Ground Station coordinates global telescope arrays with satellite communication capabilities for synchronized observations. SageMaker distributed training enables advanced pattern recognition for SETI signal analysis with scientific-grade accuracy requirements and collaborative research capabilities.",
    whyWrong: {
      0: "Standard EC2 insufficient for exabyte-scale signal processing requirements, lacks real-time detection capabilities for extraterrestrial signals",
      1: "EKS doesn't provide HPC capabilities needed for massive signal processing, lacks telescope coordination and real-time signal detection features",
      3: "Standard database insufficient for exabyte telescope data scale, lacks real-time signal detection and global telescope coordination capabilities"
    },
    examStrategy: "SETI research migration: ParallelCluster exabyte processing + Kinesis Analytics real-time detection + Ground Station coordination + SageMaker pattern recognition for interstellar communication research."
  }
},
	
];  // <-- This closes the questionBank array

// Randomize answer positions to fix the problem where correct answer is always position 0
function randomizeQuestionBankAnswers(questions) {
    return questions.map(question => {
        // Skip if already randomized or has multiple correct answers
        if (question.correct !== 0 || Array.isArray(question.correct)) {
            return question;
        }
        
        // Create array of indices [0, 1, 2, 3]
        const indices = Array.from({length: question.options.length}, (_, i) => i);
        
        // Shuffle indices
        for (let i = indices.length - 1; i > 0; i--) {
            const j = Math.floor(Math.random() * (i + 1));
            [indices[i], indices[j]] = [indices[j], indices[i]];
        }
        
        // Reorder options based on shuffled indices
        const newOptions = indices.map(i => question.options[i]);
        
        // Find where the correct answer moved to
        const newCorrectIndex = indices.indexOf(0);
        
        // Update explanation's whyWrong indices if it exists
        let newExplanation = {...question.explanation};
        if (newExplanation.whyWrong) {
            const newWhyWrong = {};
            Object.entries(newExplanation.whyWrong).forEach(([key, value]) => {
                const oldIndex = parseInt(key);
                const newIndex = indices.indexOf(oldIndex);
                if (newIndex !== newCorrectIndex) {
                    newWhyWrong[newIndex] = value;
                }
            });
            newExplanation.whyWrong = newWhyWrong;
        }
        
        return {
            ...question,
            options: newOptions,
            correct: newCorrectIndex,
            explanation: newExplanation
        };
    });
}

// Apply the randomization to fix all questions
window.questionBank = randomizeQuestionBankAnswers(window.questionBank);

console.log('SAP Question Bank loaded successfully with', window.questionBank.length, 'questions');
